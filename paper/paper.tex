\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage[UKenglish]{babel}
\usepackage[UKenglish]{isodate}
\usepackage{fullpage}
\usepackage{amsthm}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{mathtools}
\usepackage{hyperref}
\usepackage[ruled,vlined]{algorithm2e}
\usepackage[capitalise]{cleveref}
\usepackage{bm}
\usepackage{booktabs}
\usepackage[table]{xcolor}
\usepackage{tikz}
\usepackage[backgroundcolor=lightgray]{todonotes}
\usepackage{complexity}
\usepackage{subcaption}
\usepackage{multirow}
\usepackage[binary-units]{siunitx}
\usepackage[inline]{enumitem}

\usetikzlibrary{cd}
\usetikzlibrary{bayesnet}
\usetikzlibrary{calc}

\newtheorem{theorem}{Theorem}
\newtheorem{observation}{Observation}
\newtheorem{lemma}{Lemma}
\newtheorem{proposition}{Proposition}
\newtheorem{corollary}{Corollary}
\theoremstyle{definition}
\newtheorem{definition}{Definition}
\newtheorem{example}{Example}
\theoremstyle{remark}
\newtheorem*{remark}{Remark}

\DeclareMathOperator{\im}{im}

\title{Weighted Model Counting with Conditional Weights}
\author{Paulius Dilkas}

\begin{document}
\maketitle

\section{Introduction}

\begin{itemize}
\item The Main Narrative
  \begin{enumerate}
  \item When weights are defined on literals, the measure on the free BA is
    fully independent.
  \item This means that the BA itself must be larger (i.e., have additional
    `meaningless' literals) to turn any probability distribution into an
    independent one.
  \item We show how we can define conditional weights on literals, allowing us
    to encode any probability distribution into a Boolean algebra that's not
    necessarily independent and thus can be smaller.
  \item We demonstrate a specific example of this by presenting a new way to
    encode Bayesian networks into instances of WMC and adapting a WMC algorithm
    (ADDMC) to run on the new format.
  \item We show that this results in significantly faster inference.
  \item We show that our encoding results in asymptotically fewer literals and
    fewer ADDs.
  \end{enumerate}
\item Introduce
  \begin{itemize}
  \item WMC \cite{DBLP:conf/aaai/SangBK05}
  \item ADDMC \cite{DBLP:conf/aaai/DudekPV20} and ADDs
    \cite{DBLP:journals/fmsd/BaharFGHMPS97}
  \item A Bayesian network is a directed acyclic graph with random variables as
    vertices that defines a probability distribution over them. The full
    probability distribution can be expressed as the product of the conditional
    probability distributions of all random variables. For discrete Bayesian
    networks (and we only consider discrete networks in this paper), each such
    distribution can be represented by a table known as a conditional
    probability table (CPT).
  \end{itemize}
\item[F] What are the main claims, what are the main takeaways, intuitive [???]
  of theorems to follow.
\item Our work is, in spirit, similar to... using ROBDDs for Inference in
  Bayesian Networks with Troubleshooting as an Example
  \cite{DBLP:conf/uai/NielsenWJK00}: this is an interesting approach where
  deterministic parts of a BN are expressed as Boolean functions, i.e.,
  extracting the logical from the probabilistic. Maybe I can make a case that my
  encoding (the textual version) kind of does the same, even though the
  algorithm doesn't.
\end{itemize}

\section{Related Work}

\paragraph{Using WMC to perform inference on Bayesian networks.} Hitherto, four
techniques have been proposed for encoding Bayesian networks into instances of
WMC. We will identify them based on the initials of authors as well as
publications years: \texttt{d02} \cite{DBLP:conf/kr/Darwiche02}, \texttt{sbk05}
\cite{DBLP:conf/aaai/SangBK05}, \texttt{cd05} \cite{DBLP:conf/ijcai/ChaviraD05},
and \texttt{cd06} \cite{DBLP:conf/sat/ChaviraD06}. Below we summarise the
observed performance differences among them. Sang et al.
\cite{DBLP:conf/aaai/SangBK05} claim that \texttt{sbk05} is a smaller encoding
than \texttt{d02} with respect to both the number of clauses and the number of
variables but provide no experimental comparison. Chavira and Darwiche
\cite{DBLP:conf/ijcai/ChaviraD05} compare \texttt{cd05} with \texttt{d02} by
measuring the time it takes to compile either encoding into an arithmetic
circuit (but do not measure inference time). The more recent encoding
\texttt{cd05} always compiles faster and results in a smaller arithmetic circuit
(as measured by the number of edges). In their subsequent paper, the same
authors perform two sets of experiments (that are relevant to this summary)
\cite{DBLP:conf/sat/ChaviraD06}. First, they compile \texttt{cd05} and
\texttt{cd06} encodings into d-DNNF (i.e., deterministic decomposable negation
normal form \cite{DBLP:journals/jancl/Darwiche01}), measuring both compilation
time and numbers of edges in the d-DNNF diagram. The results are mostly in
favour of \texttt{cd06}. Second, they compare the inference time of
\texttt{sbk05} run with Cachet \cite{DBLP:conf/sat/SangBBKP04} with the compile
times of \texttt{cd05} and \texttt{cd06}, but only on five (types of) instances.
In these experiments, \texttt{cd06} is always faster than \texttt{cd05}, while
the comparison with \texttt{sbk05} is mixed. Sometimes \texttt{cd06} is orders
of magnitude faster than \texttt{sbk05}, sometimes slightly slower. The
performance difference between \texttt{sbk05} and \texttt{cd05} is even harder
to judge: \texttt{sbk05} is better on three out of five instances and worse on
the remaining two. Based on this description, one would expect \texttt{cd06} to
be faster than both \texttt{cd05} and \texttt{sbk05}, both of which should be
faster than \texttt{d02}. The experiments in \cref{sec:experiments}, however,
strongly disagree with this prediction, showing that the quality of an encoding
depends strongly on the underlying search algorithm or compilation technique.

\paragraph{ADDs and their use in probabilistic inference.} ADDs provide an
efficient way to manipulate functions from a Boolean algebra to any algebraic
structure (most commonly the real numbers)
\cite{DBLP:journals/fmsd/BaharFGHMPS97}. They have been used to represent value
functions in Markov decision processes \cite{DBLP:conf/uai/HoeySHB99} and, for
Bayesian network inference, to represent each CPT as an ADD
\cite{DBLP:conf/icml/ZhaoMP15} as well as by combining ADDs and arithmetic
circuits into a  single representation \cite{DBLP:conf/ijcai/ChaviraD07}. ADDs
are particularly advantageous in this situation because of their ability to
fully exploit context-specific independence, i.e., observable structure within a
CPT that is not inherited from the structure of the Bayesian network
\cite{DBLP:conf/uai/BoutilierFGK96}.

\section{Boolean Algebras, Power Sets, and Propositional
  Logic} \label{sec:prelims}

\begin{table}
  \centering
  \caption{A comparison of Boolean-algebraic (BA) and set-theoretic (ST)
    concepts for $2^X$ for some set $X$}
  \label{tbl:notation}
  \begin{tabular}{lccl}
    \toprule
    BA name & BA symbol & ST symbol & ST name \\
    \midrule
    bottom & $\bot$ & $\emptyset$ & empty set \\
    top & $\top$ & $X$ & \\
    meet, and & $\land$ & $\cap$ & intersection \\
    join, or & $\lor$ & $\cup$ & union \\
    complement, not & $\neg$ & $^c$ & complement \\
            & $\le$ & $\subseteq$ & subset relation, set inclusion \\
    atom & & & singleton, unit set \\
    \bottomrule
  \end{tabular}
\end{table}

\begin{table}
  \caption{Notation for a logic with two atoms. The elements in both columns are
    listed in the same order.}
  \label{tbl:notation_example}
  \centering
  \begin{tabular}{lcc}
    \toprule
    Name in logic & Boolean-algebraic notation & Set-theoretic notation \\
    \midrule
    Atoms (elements of $U$) & $a, b$ & $a, b$ \\
    \rowcolor{gray!10} Models (elements of $2^U$) & $\neg a \land \neg b, a \land \neg b, \neg a \land b, a \land b$ & $\emptyset, \{a\}, \{b\}, \{a, b\}$ \\
    & $\top$ & $\{ \emptyset, \{a\}, \{b\}, \{a, b\} \}$ \\
    & $\neg a \lor \neg b, a \to b$ & $\{ \emptyset, \{a\}, \{b\} \}, \{ \emptyset, \{a\}, \{a, b\} \}$ \\
    & $b \to a, a \lor b$ & $\{ \emptyset, \{b\}, \{a, b\} \}, \{ \{a\}, \{b\}, \{a, b\} \}$ \\
    & $\neg b, \neg a, a \leftrightarrow b$ & $\{\emptyset, \{a\}\}, \{\emptyset, \{b\}\}, \{\emptyset, \{a, b\}\}$ \\
    & $(a \land \neg b) \lor (b \land \neg a), a, b$ & $\{\{a\}, \{b\}\}, \{\{a\}, \{a, b\}\}, \{\{b\}, \{a, b\}\}$ \\
    & $\neg a \land \neg b, a \land \neg b, \neg a \land b, a \land b$ & $\{\emptyset\}, \{\{a\}\}, \{\{b\}\}, \{\{a, b\}\}$ \\
    \multirow{-7}{*}{Formulas (elements of $2^{2^U}$)} & $\bot$ & $\emptyset$ \\
    \bottomrule
  \end{tabular}
\end{table}

\todo[inline]{Completely rewrite this section. It needs to be very clear and
  introduce all relevant terminology.}
\todo[inline]{Would it be more accurate to replace the word `model' with
  `interpretation' in most situations?}

For any set $X$, we can interpret $2^X$ as either a power set or a Boolean
algebra---see \cref{tbl:notation} for a summary of the differences in
terminology and notation.

Let $\mathcal{L}$ be a propositional logic with atoms $U = \{ a, b \}$. See
\cref{tbl:notation_example} for an overview of the models and formulas in
$\mathcal{L}$ as well as their set-theoretic and Boolean-algebraic
representations. Note the differences in the meaning of the word `atom' between
the logical and the Boolean-theoretic interpretations. In the Boolean algebra
$2^{2^U}$, an atom is a set with a single element which corresponds to a model
of $\mathcal{L}$ (also known as an element of $2^U$); whereas an atom of
$\mathcal{L}$ is an atomic formula, i.e., an element of $U$. We will primarily
use the term `atom' to mean the former.

In the rest of the paper, for any set $U$, we will use set-theoretic notation
for $2^U$ and Boolean-algebraic notation for $2^{2^U}$, except for (Boolean)
atoms in $2^{2^U}$ that are denoted as $\{x\}$ for some model $x \in 2^U$.

\subsection{The Space of Functions on Boolean Algebras}

We build on the definitions of multiplication and projection in the ADDMC paper
\cite{DBLP:conf/aaai/DudekPV20} and define more operations that can be used to
manipulate functions from Boolean algebras to non-negative reals. All of these
operations have efficient implementations in the CUDD \cite{somenzi1998cudd}
package for manipulating ADDs (among other things) that is used by ADDMC
{\cite{DBLP:conf/aaai/DudekPV20}.

\begin{definition}[Operations on functions]
  Let $\alpha\colon 2^X \to \mathbb{R}_{\ge 0}$ and $\beta\colon 2^Y \to
  \mathbb{R}_{\ge 0}$ be functions, $p \in \mathbb{R}_{\ge 0}$, and $x \in X$.
  We define the following operations:
  \begin{description}
  \item[Addition:] $\alpha + \beta\colon 2^{X \cup Y} \to \mathbb{R}_{\ge 0}$ is
    such that $(\alpha + \beta)(T) = \alpha(T \cap X) + \beta(T \cap Y)$ for all
    $T \in 2^{X \cup Y}$.
  \item[Multiplication:] $\alpha \cdot \beta\colon 2^{X \cup Y} \to
    \mathbb{R}_{\ge 0}$ is such that $(\alpha \cdot \beta)(T) = \alpha(T \cap X)
    \cdot \beta(T \cap Y)$ for all $T \in 2^{X \cup Y}$.
  \item[Scalar multiplication:] $p\alpha\colon 2^X \to \mathbb{R}_{\ge 0}$ is
    such that $(p\alpha)(T) = p \cdot \alpha(T)$ for all $T \in 2^X$.
  \item[Complement:] $\overline{\alpha}\colon 2^X \to \mathbb{R}_{\ge 0}$ is
    such that $\overline{\alpha}(T) = 1 - \alpha(T)$ for all $T \in 2^X$.
  \item[Projection:] $\exists_x\alpha\colon 2^{X \setminus \{ x \}} \to
    \mathbb{R}_{\ge 0}$ is such that $(\exists_x\alpha)(T) = \alpha(T) +
    \alpha(T \cup \{ x \})$ for all $T \in 2^{X \setminus \{x \}}$.
  \end{description}
\end{definition}

\begin{observation}
  Let $U$ be a set, and $\mathcal{V} = \{ \alpha\colon 2^X \to \mathbb{R}_{\ge 0}
  \mid X \subseteq U \}$. Then $\mathcal{V}$ is a semi-vector space with three
  additional operations: (non-scalar) multiplication, complement, and projection.
  Specifically, note that both addition and multiplication are both associative
  and commutative.
\end{observation}

\begin{definition}[Special functions]
  We define several special functions:
  \begin{itemize}
  \item unit $1\colon 2^\emptyset \to \mathbb{R}_{\ge 0}$, $1(\emptyset) = 1$;
  \item zero $0\colon 2^\emptyset \to \mathbb{R}_{\ge 0}$, $0(\emptyset) = 0$;
  \item and $[a]\colon 2^{\{a\}} \to \mathbb{R}_{\ge 0}$, $[a](\emptyset) = 0$,
    $[a](\{a\}) = 1$ for any $a$.
  \end{itemize}
\end{definition}

Henceforth, for any function $\alpha\colon 2^X \to \mathbb{R}_{\ge 0}$ and any
set $T$, we will write $\alpha(T)$ to mean $\alpha(T \cap X)$.

\section{WMC as a Measure}

Let $U$ be a set. A \emph{measure} is a function $\mu\colon 2^{2^U} \to
\mathbb{R}_{\ge 0}$ such that $\mu(\bot) = 0$, and $\mu(a \lor b) = \mu(a) +
\mu(b)$ for all $a, b \in 2^{2^U}$ whenever $a \land b = \bot$. A \emph{weight
  function} is a function $\nu\colon 2^U \to \mathbb{R}_{\ge 0}$. A weight
function is \emph{factored} if $\nu = \prod_{x \in U} \nu_x$ for some functions
$\nu_x\colon 2^{\{x\}} \to \mathbb{R}_{\ge 0}$, $x \in U$. We say that a weight
function $\nu\colon 2^U \to \mathbb{R}_{\ge 0}$ \emph{induces} a measure
$\mu_\nu\colon 2^{2^U} \to \mathbb{R}_{\ge 0}$ if
\begin{equation} \label{eq:induced_measure}
  \mu_\nu(x) = \sum_{\{u\} \le x} \nu(u).
\end{equation}
Finally, a measure $\mu\colon 2^{2^U} \to \mathbb{R}_{\ge 0}$ is
\emph{factorable} if there exists a factored weight function $\nu\colon 2^U \to
\mathbb{R}_{\ge 0}$ that induces $\mu$.

\begin{lemma} \label{prop:measure}
  The function $\mu_\nu$, as defined by \cref{eq:induced_measure}, is a measure.
\end{lemma}
\begin{proof}
  Note that $\mu_\nu(\bot) = 0$ since there are no atoms below $\bot$. Let $a, b
  \in 2^{2^{U}}$ be such that $a \land b = \bot$. By elementary properties of
  Boolean algebras, all atoms below $a \lor b$ are either below $a$ or below
  $b$. Moreover, none of them can be below both $a$ and $b$ because then they
  would have to be below $a \land b = \bot$. Thus
  \[
    \mu_\nu(a \lor b) = \sum_{\{u\} \le a \lor b} \nu(u) = \sum_{\{u\} \le a}
    \nu(u) + \sum_{\{u\} \le b} \nu(u) = \mu_\nu(a) + \mu_\nu(b)
  \]
  as required.
\end{proof}

In this formulation, the process of calculating the value of $\mu_\nu(x)$ for
some $x \in 2^{2^U}$ with a given definition of $\nu$ is known as WMC.

\paragraph{Relation to the classical (logic-based) view of WMC.} Let
$\mathcal{L}$ be a propositional logic with two atoms $a$ and $b$ as in
\cref{sec:prelims} and $w\colon \{ a, b, \neg a, \neg b \} \to \mathbb{R}_{\ge
  0}$ a \emph{weight function} defined as $w(a) = 0.3$, $w(\neg a) = 0.7$, $w(b)
= 0.2$, $w(\neg b) = 0.8$. Furthermore, let $\Delta$ be a theory in
$\mathcal{L}$ with a sole axiom $a$. Then $\Delta$ has two models: $\{ a, b \}$
and $\{ a, \neg b \}$ and its WMC \cite{DBLP:journals/ai/ChaviraD08} is
\begin{equation} \label{eq:wmc_example}
  \mathrm{WMC}(\Delta) = \sum_{\omega \models \Delta} \prod_{\omega \models l} w(l) = w(a)w(b) + w(a)w(\neg b) = 0.3.
\end{equation}
Alternatively, we can define $\nu_a\colon 2^{\{a\}} \to \mathbb{R}_{\ge 0}$ as
$\nu_a(\{ a \}) = 0.3$, $\nu_a(\emptyset) = 0.7$ and $\nu_b\colon 2^{\{b\}} \to
\mathbb{R}_{\ge 0}$ as $\nu_b(\{ b \}) = 0.2$, $\nu_b(\emptyset) = 0.8$. Let
$\mu$ be the measure on $2^{2^U}$ induced by $\nu = \nu_a \cdot \nu_b$. Then,
equivalently to \cref{eq:wmc_example}, we can write
\[
  \mu(a) = \nu(\{ a, b \}) + \nu(\{ a \}) = \nu_a(\{a\})\nu_b(\{b\}) +
  \nu_a(\{a\})\nu_b(\emptyset) = 0.3.
\]

\subsection{Limitations of Factorable Measures}

\begin{example}
  Let $U = \{a, b\}$ be a set of atoms and $\mu\colon 2^{2^U} \to
  \mathbb{R}_{\ge 0}$ a measure defined as:\footnote{The value of $\mu$ on any
    other element of the Boolean algebra can be deduced using the definition.}
  \begin{align*}
    \mu(a \land b) &= 0.72, \\
    \mu(a \land \neg b) &= 0.18, \\
    \mu(\neg a \land b) &= 0.07, \\
    \mu(\neg a \land \neg b) &= 0.03.
  \end{align*}
  If $\mu$ could be represented using traditional (factored) WMC, we would have
  to find two weight functions $\nu_a\colon 2^{\{a\}} \to \mathbb{R}_{\ge 0}$
  and $\nu_b\colon 2^{\{b\}} \to \mathbb{R}_{\ge 0}$ such that $\nu = \nu_a
  \cdot \nu_b$ induces $\mu$, i.e., $\nu_a$ and $\nu_b$ would have to satisfy
  this system of equations:
  \begin{align*}
    \nu_a(\{a\}) \cdot \nu_b(\{b\}) &= 0.72 \\
    \nu_a(\{a\}) \cdot \nu_b(\emptyset) &= 0.18 \\
    \nu_a(\emptyset) \cdot \nu_b(\{b\}) &= 0.07 \\
    \nu_a(\emptyset) \cdot \nu_b(\emptyset) &= 0.03,
  \end{align*}
  which has no solutions.

  Alternatively, we can let $b$ depend on $a$ and consider weight functions
  $\nu_a\colon 2^{\{a\}} \to \mathbb{R}_{\ge 0}$ and $\nu_b\colon 2^{\{a, b\}}
  \to \mathbb{R}_{\ge 0}$ defined as $\nu_a(\{a\}) = 0.9$, $\nu_a(\emptyset) =
  0.1$, and $\nu_b(\{a, b\}) = 0.8$, $\nu_b(\{a\}) = 0.2$, $\nu_b(\{b\}) = 0.7$,
  $\nu_b(\emptyset) = 0.3$. One can easily check that with these definitions
  $\nu$ indeed induces $\mu$.
\end{example}

Note that in this case we chose to interpret $\nu_b$ as $\Pr(b \mid a)$
while---with a different definition of $\nu_b$ that represents the joint
probability distribution $\Pr(a, b)$---$\nu_b$ by itself could induce $\mu$. In
general, however, factorising the full weight function into several smaller
functions often results in weight functions with smaller domains which leads to
increased efficiency and decreased memory usage \cite{DBLP:conf/aaai/DudekPV20}.

\begin{lemma} \label{lemma:before_theorem}
  For any measure $\mu\colon 2^{2^U} \to \mathbb{R}_{\ge 0}$ and elements $a, b
  \in 2^{2^U}$,
  \begin{equation} \label{eq:to_prove}
    \mu(a \land b) = \mu(a)\mu(b)
  \end{equation}
  if and only if
  \begin{equation} \label{eq:to_prove2}
    \mu(a \land b) \cdot \mu(\neg a \land \neg b) = \mu(a \land \neg b)
    \cdot \mu(\neg a \land b).
  \end{equation}
\end{lemma}
\begin{proof}
  First, note that $a = (a \land b) \lor (a \land \neg b)$ and $(a \land b)
  \land (a \land \neg b) = 0$, so, by properties of a measure,
  \begin{equation} \label{eq:temp}
    \mu(a) = \mu(a \land b) + \mu(a \land \neg b).
  \end{equation}
  Applying \cref{eq:temp} and the equivalent expression for $\mu(b)$ allows us
  to rewrite \cref{eq:to_prove} as
  \[
    \mu(a \land b) = [\mu(a \land b) + \mu(a \land \neg b)][\mu(a \land b) +
    \mu(\neg a \land b)]
  \]
  which is equivalent to
  \begin{equation} \label{eq:temp6}
    \mu(a \land b)[1 - \mu(a \land b) - \mu(a \land \neg b) - \mu(\neg a \land b)] = \mu(a \land \neg b)\mu(\neg a \land b).
  \end{equation}
  Since $a \land b$, $a \land \neg b$, $\neg a \land b$, $\neg a \land \neg b$
  are pairwise disjoint and their supremum is $\top$,
  \[
    \mu(a \land b) + \mu(a \land \neg b) + \mu(\neg a \land b) + \mu(\neg a
    \land \neg b) = 1,
  \]
  and this allows us to rewrite \cref{eq:temp6} into \cref{eq:to_prove2}. As all
  transformations are invertible, the two expressions are equivalent.
\end{proof}

\todo[inline]{The proof is only valid for probability measures!}

\begin{theorem}
  A measure $\mu\colon 2^{2^U} \to \mathbb{R}_{\ge 0}$ is factorable if and only
  if
  \begin{equation} \label{eq:wmccondition}
  \mu(u \land v) = \mu(u)\mu(v)
  \end{equation}
  for all distinct $u, v \in U \cup \{ \neg w \mid w \in U \}$ such that $u \ne
  \neg v$.
\end{theorem}
\begin{proof}
  ($\Leftarrow$) For each $x \in U$, let $\nu_x\colon 2^{\{x\}} \to
  \mathbb{R}_{\ge 0}$ be defined by $\nu_x(\{ x \}) = \mu(x)$ and
  $\nu_x(\emptyset) = \mu(\neg x)$. Let $\mu_\nu$ be the measure induced by $\nu
  = \prod_{x \in U} \nu_x$. We will show that $\mu = \mu_\nu$. First, note that
  $\mu_\nu(\bot) = 0 = \mu(\bot)$ by \cref{prop:measure} and the definition of a
  measure. Second, let $a = \bigwedge_{u \in U} a_u$ be an atom in $2^{2^U}$
  such that $a_u \in \{ u, \neg u \}$ for all $u \in U$. Then
  \[
    \mu_\nu(a) = \nu(\{ a_u \mid u \in U \}) = \prod_{u \in U} \nu_u(\{a_u\}) =
    \prod_{u \in U} \mu(a_u) = \mu\left(\bigwedge_{u \in U} a_u \right) = \mu(a)
  \]

  Finally, note that if $\mu_\nu$ and $\mu$ agree on all atoms, then they must
  also agree on all other non-zero elements of the Boolean algebra.

  ($\Rightarrow$) For the other direction, we are given a factored weight
  function $\nu = \prod_{x \in U} \nu_x$, and we want to show that its induced
  measure $\mu_\nu$ satisfies \cref{eq:wmccondition}. Let $k_u, k_v \in U \cup
  \{\neg w \mid w \in U \}$ be such that $k_u \in \{ u, \neg u \}$, $k_v \in \{
  v, \neg v \}$, and $u \ne v$. We then want to show that
  \begin{equation} \label{eq:to_prove3}
    \mu_\nu(k_u \land k_v) = \mu_\nu(k_u)\mu_\nu(k_v)
  \end{equation}
  which is equivalent to
  \begin{equation} \label{eq:to_prove4}
    \mu_\nu(k_u \land k_v) \cdot \mu_\nu(\neg k_u \land \neg k_v) = \mu_\nu(k_u \land \neg k_v) \cdot \mu_\nu(\neg k_u \land k_v)
  \end{equation}
  by \cref{lemma:before_theorem}. Then
  \begin{align*}
    \mu_\nu(k_u \land k_v) &= \sum_{\{a\} \le k_u \land k_v} \nu(a) = \sum_{\{a\} \le k_u \land k_v} \prod_{x \in U} \nu_x(a) \\
                        &= \sum_{\{a\} \le k_u \land k_v} \nu_u(a_u)\nu_v(a_v) \prod_{x \in U \setminus \{ u, v \}} \nu_x(a) = \sum_{\{a\} \le k_u \land k_v} \nu_u(k_u)\nu_v(k_v) \prod_{x \in U \setminus \{ u, v \}} \nu_x(a) \\
    &= \nu_u(k_u)\nu_v(k_v) \sum_{\{a\} \le k_u \land k_v} \prod_{x \in U \setminus \{ u, v \}} \nu_x(a) = \nu_u(k_u)\nu_v(k_v)c,
  \end{align*}
  where $c = \sum_{\{a\} \le k_u \land k_v} \prod_{x \in U \setminus \{ u, v \}}
  \nu_x(a)$, and is the same for $\mu_\nu(\neg k_u \land k_v)$, $\mu_\nu(k_u
  \land \neg k_v)$, and $\mu_\nu(\neg k_u \land \neg k_v)$ as well. But then
  \cref{eq:to_prove4} becomes
  \[
    \nu_u(k_u)\nu_v(k_v)\nu_u(\neg k_u)\nu_v(\neg k_v)c^2 = \nu_u(k_u)\nu_v(\neg
    k_v)\nu_u(\neg k_u)\nu_v(k_v)c^2
  \]
  which is trivially true.
\end{proof}

\section{Encoding Bayesian Networks Using Conditional Weights}

\begin{figure}
  \centering
  \begin{subfigure}{0.2\textwidth}
    \centering
    \begin{tikzpicture}[edge from parent/.style={draw,-latex}]
      \node[draw,circle] {$W$}
      child {node[draw,circle] {$F$}}
      child {node[draw,circle] {$T$}};
    \end{tikzpicture}
  \end{subfigure}%
  \begin{subfigure}{0.8\textwidth}
    \centering
    \begin{tabular}[t]{cc}
      \toprule
      $w$ & $\Pr(W = w)$ \\
      \midrule
      1 & 0.5 \\
      0 & 0.5 \\
      \bottomrule
    \end{tabular}
    \begin{tabular}[t]{ccc}
      \toprule
      $w$ & $f$ & $\Pr(F = f \mid W = w)$ \\
      \midrule
      1 & 1 & 0.6 \\
      1 & 0 & 0.4 \\
      0 & 1 & 0.1 \\
      0 & 0 & 0.9 \\
      \bottomrule
    \end{tabular}
    \begin{tabular}[t]{ccc}
      \toprule
      $w$ & $t$ & $\Pr(T = t \mid W = w)$ \\
      \midrule
      1 & $l$ & 0.2 \\
      1 & $m$ & 0.4 \\
      1 & $h$ & 0.4 \\
      0 & $l$ & 0.6 \\
      0 & $m$ & 0.3 \\
      0 & $h$ & 0.1 \\
      \bottomrule
    \end{tabular}
  \end{subfigure}
  \caption{An example Bayesian network with its CPTs}
  \label{fig:example_bn}
\end{figure}

In this section, we describe a way to encode Bayesian networks into WMC without
restricting oneself to factorable measures and thus having to add extra
variables to maintain literal independence. In line with the names of previous
encodings, we shall call this encoding \texttt{db20}.

Recall that a Bayesian network is a directed acyclic graph with random variables
as vertices. Let $\mathcal{V}$ denote the set of random variables. For any
random variable $X \in \mathcal{V}$, let $\im X$ denote its set of possible
values and $\mathrm{pa}(X)$ the set of its parents. The full probability
distribution is then equal to $\prod_{X \in \mathcal{V}} \Pr(X \mid
\mathrm{pa}(X))$. See \cref{fig:example_bn} for an example Bayesian network
which we will refer to throughout this section. For this network, $\mathcal{V} =
\{ W, F, T \}$, $\mathrm{pa}(W) = \emptyset$, $\mathrm{pa}(F) = \mathrm{pa}(T) =
\{ W \}$, $\im W = \im F = \{0, 1 \}$, and $\im T = \{ l, m, h \}$.

\begin{definition}[Indicator variables]
  Let $X \in \mathcal{V}$ be a random variable. If $X$ is binary (i.e., $|\im X|
  = 2$), we can arbitrary identify one of the values as $1$ and the other one as
  $0$ (i.e, $\im X \cong \{ 0, 1 \}$). Then $X$ can be represented by a single
  \emph{indicator variable} $\lambda_{X=1}$. For notational simplicity, for any
  set $S$, we write $\lambda_{X=0} \in S$ or $S = \{ \lambda_{X=0}, \dots \}$ to
  mean $\lambda_{X=1} \not\in S$.

  On the other hand, if $X$ is not binary, we represent $X$ with $|\im X|$
  indicator variables, one for each value. We let
  \[
    \mathcal{E}(X) = \begin{cases}
      \{ \lambda_{X=1} \} & \text{if } |\im X| = 2 \\
      \{ \lambda_{X=x} \mid x \in \im X \} & \text{otherwise.}
    \end{cases}
  \]
  denote the set of indicator variables for $X$ and $\mathcal{E}^*(X) =
  \mathcal{E}(X) \cup \bigcup_{Y \in \mathrm{pa}(X)} \mathcal{E}(Y)$ denote the
  set of indicator variables for $X$ and its parents in the Bayesian network.
  Finally, let $U = \bigcup_{X \in \mathcal{V}} \mathcal{E}(X)$ denote the set
  of all indicator variables for all random variables in the Bayesian network.
\end{definition}

For the Bayesian network in \cref{fig:example_bn}, this gives us:
\begin{align*}
  \mathcal{E}(W) &= \{ \lambda_{W=1} \}, \\
  \mathcal{E}(F) &= \{ \lambda_{F=1} \}, \\
  \mathcal{E}(T) &= \{ \lambda_{T=l}, \lambda_{T=m}, \lambda_{T=h} \}, \\
  \mathcal{E}^*(W) &= \{ \lambda_{W=1} \}, \\
  \mathcal{E}^*(F) &= \{ \lambda_{F=1}, \lambda_{W=1} \}, \\
  \mathcal{E}^*(T) &= \{ \lambda_{T=l}, \lambda_{T=m}, \lambda_{T=h}, \lambda_{W=1} \}.
\end{align*}

\begin{algorithm}
  \caption{Encoding a Bayesian network as a function $2^U \to \mathbb{R}_{\ge
      0}$}
  \label{alg:encoding}
  \KwData{a Bayesian network with vertices $\mathcal{V}$ and probability
    distribution $\Pr$}
  \KwResult{a function $\phi\colon 2^U \to \mathbb{R}_{\ge 0}$}
  $\phi \gets 1$\;
  \For{$X \in \mathcal{V}$}{
    \textit{let} $\mathrm{pa}(X) = \{ Y_1, \dots, Y_n \}$\;
    $\mathrm{CPT}_X \gets 0$\;
    \eIf{$|\im X| = 2$}{
      \For{$(y_1, \dots, y_n) \in \prod_{i = 1}^n \im Y_i$}{
        $p_1 \gets \Pr(X = 1 \mid Y_1 = y_1, \dots, Y_n = y_n)$\;
        $p_0 \gets \Pr(X \ne 1 \mid Y_1 = y_1, \dots, Y_n = y_n)$\;
        $\mathrm{CPT}_X \gets \mathrm{CPT}_X + p_1[\lambda_{X=1}] \cdot
        \prod_{i=1}^n [\lambda_{Y_i=y_i}] + p_0 \overline{[\lambda_{X=1}]} \cdot
        \prod_{i=1}^n [\lambda_{Y_i=y_i}]$\;
      }
    }{
      \textit{let} $\im X = \{ x_1, \dots, x_m \}$\;
      \For{$x \in \im X$ {\rm \textbf{and}} $(y_1, \dots, y_n) \in \prod_{i =
          1}^n \im Y_i$}{
        $p_x \gets \Pr(X = x \mid Y_1 = y_1, \dots, Y_n = y_n)$\;
        $\mathrm{CPT}_X \gets \mathrm{CPT}_X + p_x[\lambda_{X=x}] \cdot
        \prod_{i=1}^n [\lambda_{Y_i=y_i}] + \overline{[\lambda_{X=1}]} \cdot
        \prod_{i=1}^n [\lambda_{Y_i=y_i}]$\;
      }
      $\mathrm{CPT}_X \gets \mathrm{CPT}_X \cdot \left( \sum_{i=1}^m [\lambda_{X
          = x_i}] \right) \cdot \prod_{i=1}^m \prod_{j=i+1}^m
      (\overline{[\lambda_{X = x_i}]} + \overline{[\lambda_{X = x_j}]})$\;
    }
    $\phi \gets \phi \cdot \mathrm{CPT}_X$\;
  }
  \Return{$\phi$}\;
\end{algorithm}

\Cref{alg:encoding} shows how a Bayesian network with vertices $\mathcal{V}$
can be represented as a weight function $\phi\colon 2^U \to \mathbb{R}_{\ge 0}$.
The algorithm begins with the unit function and multiplies it by
$\mathrm{CPT}_X\colon 2^{\mathcal{E}^*(X)} \to \mathbb{R}_{\ge 0}$ for each
random variable $X \in \mathcal{V}$. We call each such function a
\emph{conditional weight function} as it represents a conditional probability
distribution. However, the distinction is primarily a semantic one: a function
$2^{\{a, b\}} \to \mathbb{R}_{\ge 0}$ can represent $\Pr(a \mid b)$, $\Pr(b \mid
a)$, or something else entirely.

For a binary random variable $X$, $\mathrm{CPT}_X$ is simply a sum of smaller
functions, one for each row of the CPT. If $X$ has more than two values, we also
multiply $\mathrm{CPT}_X$ by some `clause' functions that restrict the value of
$\phi(T)$ to zero whenever $|\mathcal{E}(X) \cap T| \ne 1$. For the Bayesian
network in \cref{fig:example_bn}, we get:
\begin{align*}
  \mathrm{CPT_W} &= 0.5[\lambda_{W=1}]+0.5\overline{[\lambda_{W=1}]} = 0.5 \cdot 1, \\
  \mathrm{CPT_F} &= 0.6[\lambda_{F=1}] \cdot [\lambda_{W=1}] + 0.4[\lambda_{F=0}] \cdot [\lambda_{W=1}] + 0.1[\lambda_{F=1}] \cdot [\lambda_{W=0}] + 0.9[\lambda_{F=0}] \cdot [\lambda_{W=0}] \\
    &= 0.6[\lambda_{F=1}] \cdot [\lambda_{W=1}] + 0.4\overline{[\lambda_{F=1}]} \cdot [\lambda_{W=1}] + 0.1[\lambda_{F=1}] \cdot \overline{[\lambda_{W=1}]} + 0.9\overline{[\lambda_{F=1}]} \cdot \overline{[\lambda_{W=1}]}, \\
  \mathrm{CPT_T} &= ([\lambda_{T=l}] + [\lambda_{T=m}] + [\lambda_{T=h}]) \cdot (\overline{[\lambda_{T=l}]} + \overline{[\lambda_{T=m}]}) \cdot (\overline{[\lambda_{T=l}]} + \overline{[\lambda_{T=h}]}) \cdot (\overline{[\lambda_{T=m}]} + \overline{[\lambda_{T=h}]}) \cdot (\dots).
\end{align*}

\subsection{Proof of Correctness}

\todo[inline]{Introduce what this subsection is all about (and each result
  separately).}

\begin{lemma} \label{lemma:cpt}
  Let $X \in \mathcal{V}$ be a random variable with parents $\mathrm{pa}(X) = \{ Y_1,
  \dots, Y_n \}$. Then $\mathrm{CPT}_X\colon 2^{\mathcal{E}^*(X)} \to
  \mathbb{R}_{\ge 0}$ is such that for any $x \in \im X$ and $(y_1, \dots, y_n)
  \in \prod_{i=1}^n \im Y_i$,
  \[
    \mathrm{CPT}_X (\{ \lambda_{X=x} \} \cup \{ \lambda_{Y_i=y_i} \mid i = 1,
    \dots, n \}) = \Pr(X = x \mid Y_1 = y_1, \dots, Y_n = y_n).
  \]
\end{lemma}
\begin{proof}
  Let $T = \{ \lambda_{X=x} \} \cup \{ \lambda_{Y_i=y_i} \mid i = 1, \dots, n
  \}$. If $X$ is binary, then $\mathrm{CPT}_X$ is a sum of $2\prod_{i=1}^n |\im
  Y_i|$ terms, one for each possible assignment of values to variables $X, Y_1,
  \dots, Y_n$. Exactly one of these terms is nonzero when applied to $T$, and
  it is equal to $\Pr(X = x \mid Y_1 = y_1, \dots, Y_n = y_n)$ by definition.

  If $X$ is not binary, then $\left( \sum_{i=1}^m [\lambda_{X = x_i}]
  \right)(T) = 1$, and $\left( \prod_{i=1}^m \prod_{j=i+1}^m
    (\overline{[\lambda_{X = x_i}]} + \overline{[\lambda_{X = x_j}]})
  \right)(T) = 1$, so $\mathrm{CPT}_X(T) = \Pr(X = x \mid Y_1 = y_1,
  \dots, Y_n = y_n)$ by a similar argument as before.
\end{proof}

\begin{proposition} \label{lemma:full_distribution}
  The function $\phi\colon 2^U \to \mathbb{R}_{\ge 0}$ represents the full
  probability distribution of the Bayesian network, i.e., if $\mathcal{V} = \{
  X_1, \dots, X_n\}$, then
  \[
    \phi(T) =
    \begin{cases}
      \Pr(X_1 = x_1, \dots, X_n = x_n) & \text{if } T = \{ \lambda_{X_i=x_i}
      \mid i = 1, \dots, n \} \text{ for some } (x_1, \dots, x_n) \in
      \prod_{i=1}^n \im X_i \\
      0 & \text{otherwise,}
    \end{cases}
  \]
  for all $T \in 2^U$.
\end{proposition}
\begin{proof}
  If $T = \{ \lambda_{X=v_X} \mid X \in \mathcal{V} \}$ for some $(v_X)_{X
    \in \mathcal{V}} \in \prod_{X \in \mathcal{V}} \im X$, then
  \[
    \phi(T) = \prod_{X \in \mathcal{V}} \Pr \left( X=v_X \;\middle|\;
      \bigwedge_{Y \in \mathrm{pa}(X)} Y=v_Y \right) = \Pr \left( \bigwedge_{X
        \in \mathcal{V}} X=v_X \right)
  \]
  by \cref{lemma:cpt} and the definition of a Bayesian network. Otherwise there
  must be some non-binary random variable $X \in \mathcal{V}$ such that
  $|\mathcal{E}(X) \cap T| \ne 1$. If $\mathcal{E}(X) \cap T = \emptyset$, then
  $\left( \sum_{i=1}^m [\lambda_{X = x_i}] \right)(T) = 0$, and so
  $\mathrm{CPT}_X(T) = 0$, and $\phi(T) = 0$. If $|\mathcal{E}(X) \cap T| > 1$,
  then we must have two different values $x_1, x_2 \in \im X$ such that
  $\{\lambda_{X=x_1}, \lambda_{X=x_2} \} \subseteq T$ which means that
  $(\overline{[\lambda_{X=x_1}]} + \overline{[\lambda_{X=x_2}]})(T) = 0$, and
  so, again, $\mathrm{CPT}_X(T) = 0$, and $\phi(T) = 0$.
\end{proof}

\begin{theorem}
  Let $\phi\colon 2^U \to \mathbb{R}_{\ge 0}$ be the function generated by
  \cref{alg:encoding}. Then $(\exists_U(\phi \cdot [\lambda_{X=x}]))(\emptyset)
  = \Pr(X = x)$.
\end{theorem}
\begin{proof}
  Let $\mathcal{V} = \{ X, Y_1, \dots, Y_n \}$. Then
  \begin{align*}
    (\exists_U (\phi \cdot [\lambda_{X=x}]))(\emptyset) &= \sum_{T \in 2^U} (\phi \cdot [\lambda_{X=x}])(T) = \sum_{\lambda_{X=x} \in T \in 2^U} \phi(T) = \sum_{\lambda_{X=x} \in T \in 2^U} \left( \prod_{Y \in \mathcal{V}} \mathrm{CPT}_Y \right)(T) \\
    &= \sum_{(y_1, \dots, y_n) \in \prod_{i=1}^n \im Y_i} \Pr(X = x, Y_1 = y_1, \dots, Y_n = y_n) = \Pr(X = x)
  \end{align*}
  by the following arguments:
  \begin{itemize}
  \item the proof of Theorem~1 in the ADDMC paper \cite{DBLP:conf/aaai/DudekPV20};
  \item if $\lambda_{X=x} \not\in T \in 2^U$, then $(\phi \cdot
    [\lambda_{X=x}])(T) = \phi(T) \cdot [\lambda_{X=x}](T \cap \{
    \lambda_{X=x} \}) = \phi(T) \cdot 0 = 0$;
  \item \cref{lemma:full_distribution};
  \item marginalisation of a probability distribution.
  \end{itemize}
\end{proof}

\subsection{Textual Representation} \label{sec:textual_representation}

\todo[inline]{Describe how this is an intermediate step before we get the $\phi$
from the algorithm. Each CPT is treated as a clause.}

The Bayesian network in \cref{fig:example_bn} can be represented in a textual
format as
\[
  \begin{array}{l r r l l}
    \lambda\sb{T=l} &\lambda\sb{T=m} &\lambda\sb{T=h} & &0 \\
                    &-\lambda\sb{T=l} &-\lambda\sb{T=m} & &0 \\
                    &-\lambda\sb{T=l} &-\lambda\sb{T=h} & &0 \\
                    &-\lambda\sb{T=m} &-\lambda\sb{T=h} & &0 \\
    w &\lambda\sb{W=1} & &0.5 &0.5 \\
    w &\lambda\sb{F=1} &\lambda\sb{W=1} &0.6 &0.4 \\
    w &\lambda\sb{F=1} &-\lambda\sb{W=1} &0.1 &0.9 \\
    w &\lambda\sb{T=l} &\lambda\sb{W=1} &0.2 &1 \\
    w &\lambda\sb{T=m} &\lambda\sb{W=1} &0.4 &1 \\
    w &\lambda\sb{T=h} &\lambda\sb{W=1} &0.4 &1 \\
    w &\lambda\sb{T=l} &\lambda\sb{W=0} &0.6 &1 \\
    w &\lambda\sb{T=m} &\lambda\sb{W=0} &0.3 &1 \\
    w &\lambda\sb{T=h} &\lambda\sb{W=0} &0.1 &1
  \end{array}
\]
with each $\lambda$ replaced with a unique positive integer. This format is
based on the format used by the Cachet solver \cite{DBLP:conf/sat/SangBBKP04}
to encode WMC problems, which extends the DIMACS format for conjunctive normal
form (CNF) formulas with weight clauses. Subsequently, we extend it in two ways:
a single weight clause now supports an arbitrary number of literals, and each
weight clause has two probabilities instead of one (i.e., we no longer assume
that $\Pr(v) + \Pr(\neg v) = 1$ for all $v \in U$). In our use of the encoding,
it is always the case that either both probabilities sum to one, or the second
probability (i.e., the probability for the complement of the variable) is equal
to one.

\subsection{Changes to ADDMC}

ADDMC constructs the \emph{Gaifman graph} \cite{gaifman1982local} of the input
CNF formula as an aid for the algorithm's heuristics. This graph has as vertices
the variables of the formula, and there is an edge between two variables $u$ and
$v$ if there is a clause in the formula that contains both $u$ and $v$. We
extend this definition to functions on Boolean algebras, i.e., the factors of
$\phi$. For any pair of distinct variables $u, v \in U$, we draw an edge between
them in the Gaifman graph if there is a function $A\colon 2^X \to
\mathbb{R}_{\ge 0}$ that is a factor of $\phi$ such that $u \in X$ and $v \in
X$. For instance, a factor such as $\mathrm{CPT}_X$ will enable edges between
all distinct pairs of variables in $\mathcal{E}^*(X)$.

\todo[inline]{This needs rewriting.}
ADDMC multiplies the final answer by $w(u)+w(-u)$ for every $u \in U$ that was
`simplified out' of the ADD for $\phi$ and so is not featured in the $\exists_U$
projection. In our format, for every such $u \in U$, we multiply the answer by
two. Each such $u \in U$ must satisfy two properties:
\begin{itemize}
\item all the probabilities/weights associated with $u$ are equal to $0.5$
  (otherwise the corresponding CPT would depend on $u$),
\item and all other CPTs are independent of $u$ (or they may have a trivial
  dependence, where the probability stays the same if $u$ is replaced with its
  complement).
\end{itemize}
Thus, the CPT that corresponds to $u$ still multiplies every model by $0.5$, but
the number of models being considered by the ADDMC is halved. To correct for
this, we need to perform this multiplication.

\section{Experimental Comparison} \label{sec:experiments}

\begin{figure}
  \centering
  \begin{minipage}{0.59\textwidth}
    \centering
    \input{cumulative.tex}%
    \captionof{figure}{Cumulative number of instances solved by ADDMC over time
      using each encoding}
  \end{minipage}
  \begin{minipage}{0.39\textwidth}
    \centering
    \captionof{table}{The numbers of instances solved by ADDMC with each
      encoding (uniquely, faster than with others, and in total) under the
      described time and memory constraints (out of 1216 instances)}%
    \begin{tabular}{lrrr}
      \toprule
      Encoding & Unique & Fastest & Total \\
      \midrule
      \texttt{cd05} & 2 & 3 & 372 \\
      \texttt{cd06} & 0 & 1 & 351 \\
      \texttt{d02} & 41 & 99 & 726 \\
      \texttt{db20} & 228 & 871 & 901 \\
      \texttt{sbk05} & 6 & 36 & 687 \\
      \bottomrule
    \end{tabular}
  \end{minipage}
\end{figure}

\begin{figure}
  \centering
  \input{scatter.tex}%
  \caption{ADDMC inference time using \texttt{db20} compared to \texttt{d02}
    (left) and \texttt{sbk05} (right) on an instance-by-instance basis across
    all data sets.}
\end{figure}

\begin{itemize}
\item We chose not to measure or compare encoding time because of different
  languages of implementation and the fact that our encoding algorithm (i.e.,
  the process that converts a Bayesian network into a textual encoding such as
  in \cref{sec:textual_representation}) is linear in the total number of CPT
  rows and so is unlikely to be slower than, e.g., \texttt{cd06}, which relies
  on solving $\NP$-complete problems as part of the encoding process
  \cite{DBLP:conf/sat/ChaviraD06}.
\item Whenever a Bayesian network comes with an evidence file, we compute the
  probability of evidence. Otherwise, let $X$ denote the last-mentioned vertex
  in the Bayesian network. If $\mathsf{true}$ is a valid value of $X$, we
  compute the marginal probability of $X = \mathsf{true}$. Otherwise, we pick
  the value of $X$ which is listed first and calculate its marginal probability.
\item The experiments were run on Intel Xeon Gold 6138 processor with an
  \SI{8}{\giga\byte} memory limit.
\item For all other encodings, we use their implementation in
  Ace~3.0\footnote{\url{http://reasoning.cs.ucla.edu/ace/}} with
  \texttt{-encodeOnly} and \texttt{-noEclause} flags. However, Ace was not used
  to encode evidence, as preliminary experiments revealed that the
  evidence-encoding implementation contains bugs that can lead to incorrect
  answers or a Java exception being thrown on some instances of the data set
  (and the source code is not publicly available). Instead, we simply list all
  the evidence as additional clauses in the encoding (regardless of which
  encoding is used).
\item Note that \texttt{cd05} and \texttt{cd06} purposefully produce overly
  relaxed encodings that contain extra models and thus yield incorrect
  probabilities \cite{DBLP:conf/ijcai/ChaviraD05,DBLP:conf/sat/ChaviraD06}.
  These additional models are supposed to be filtered out during circuit
  compilation \cite{DBLP:conf/ijcai/ChaviraD05}, but this is not easily
  achievable with ADDMC. Nonetheless, we include both encodings in our timing
  experiments.
\end{itemize}

\paragraph{Data.} For experiments, we use the Bayesian networks available
with Ace and
Cachet\footnote{\url{https://www.cs.rochester.edu/u/kautz/Cachet/}}, most of
which happen to be binary. We classify them into the following seven categories:
\begin{itemize*}
\item DQMR and
\item Grid networks as described by Sang et al. \cite{DBLP:conf/aaai/SangBK05},
\item Friends and Smokers,
\item Mastermind, and
\item Random Blocks from the work of Chavira et al.
  \cite{DBLP:journals/ijar/ChaviraDJ06},
\item remaining binary Bayesian networks that include Plan Recognition
  \cite{DBLP:conf/aaai/SangBK05}, Students and Professors
  \cite{DBLP:journals/ijar/ChaviraDJ06}, and \texttt{tcc4f}, and
\item non-binary classic Bayesian networks (\texttt{alarm}, \texttt{diabetes},
  \texttt{hailfinder}, \texttt{mildew}, \texttt{munin1}--\texttt{4},
  \texttt{pathfinder}, \texttt{pigs}, \texttt{water}).
\end{itemize*}

\paragraph{Observations.}
\begin{itemize}
\item The order of the encodings from best to worst is exactly the opposite of
  that in the previous literature.
\item Our encoding is particularly promising on instances of Grid networks.
  They are set up so that the entire Bayesian network is relevant to the
  query.
\item However, \texttt{db20} struggles with instances from Mastermind and
  Random Blocks domains (but so does \texttt{sbk05}). We conjecture that this
  is so either because the particular structure of these problems confuse the
  heuristics used by ADDMC or because these instances allow for significant
  simplifications that are exploited by $\mathtt{d02}$ but not
  $\mathtt{db20}$.
\item Everything \texttt{d02} can do in \SI{1000}{\second}, \texttt{db20} can do
  in \SI{10}{\second} (find the exact number).
\end{itemize}

\subsection{Explaining the Performance Benefits}

\todo[inline]{Maybe replace `upper bound' with `worst-case'.}

\begin{table}
  \centering
  \caption{Asymptotic upper bounds on the numbers of variables and clauses/ADDs
    for each encoding}
  \label{tbl:asymptotes}
  \begin{tabular}{lcc}
    \toprule
    Encoding(s) & Variables & Clauses/ADDs \\
    \midrule
    \texttt{cd05}, \texttt{cd06}, \texttt{sbk05} & $\mathcal{O}(nv^{d+1})$ & $\mathcal{O}(nv^{d+1})$ \\
    \texttt{d02} & $\mathcal{O}(nv^{d+1})$ & $\mathcal{O}(ndv^{d+1})$ \\
    \texttt{db20} & $\mathcal{O}(nv)$ & $\mathcal{O}(nv^2)$ \\
    \bottomrule
  \end{tabular}
\end{table}

Let $n = |\mathcal{V}|$ be the number of vertices in the Bayesian network, $d =
\max_{X \in \mathcal{V}} |\mathrm{pa}(X)|$ the maximum in-degree (i.e., number
of parents), and $v = \max_{X \in \mathcal{V}} |\im X|$ the maximum number of
values per variable. Then... \cref{tbl:asymptotes}. However, note that these are
upper bounds and most encodings (including \texttt{db20}) can be smaller in
certain situations (e.g., with binary random variables or when a CPT has
repeating probabilities).

We equate clauses and ADDs (more specifically, factors of the function $\phi$
from \cref{alg:encoding}) here because ADDMC interprets each clause of any WMC
encoding as a multiplicative factor of the ADD that represents the entire WMC
instance \cite{DBLP:conf/aaai/DudekPV20}. For literal-weight encodings, each
weight is also a factor, but that has no effect on the asymptotic bounds in the
table.

\section{Conclusion and Future Work}

\begin{itemize}
\item Bayesian networks and ADDMC are only particular examples. This should also
  work with Cachet \cite{DBLP:conf/sat/SangBBKP04}.
  \begin{itemize}
  \item Potential criticism may be that this doesn't allow us to use SAT-based
    techniques for probabilistic inference. However, they can still be used for
    a significant part of the encoding.
    \begin{itemize}
    \item Zero-probability weights and one-probability weights can be
      interpreted as logical clauses. This doesn't affect ADDMC but could be
      useful for other solvers.
    \end{itemize}
  \end{itemize}
\item Extra benefit: one does not need to come up with a way to turn some
  probability distribution to into a fully independent one.
\item Important future work: replacing ADDs with
  AADDs \cite{DBLP:conf/ijcai/SannerM05} is likely to bring performance
  benefits.
  Other extensions:
  \begin{itemize}
  \item FOADDs can represent first order statements;
  \item XADDs can replace WMI for continuous variables;
  \item ADDs with intervals can do approximations.
  \end{itemize}
\item Filtering out ADDs that have nothing to do with the answer helps
  tremendously, but I'm purposefully not doing that.
\item Other things can be compiled into WMC, e.g., probabilistic programs
  \cite{DBLP:journals/corr/abs-2005-09089}, ProbLog
  \cite{DBLP:conf/uai/FierensBTGR11}.
\end{itemize}

\paragraph{Acknowledgements.} This work has made use of the resources provided
by the Edinburgh Compute and Data Facility (ECDF)
(\url{http://www.ecdf.ed.ac.uk/}).

\bibliographystyle{plain}
\bibliography{paper}

\end{document}
