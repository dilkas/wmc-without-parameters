\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage[UKenglish]{babel}
\usepackage[UKenglish]{isodate}
\usepackage{fullpage}
\usepackage{amsthm}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{mathtools}
\usepackage{hyperref}
\usepackage[ruled,vlined]{algorithm2e}
\usepackage[capitalise]{cleveref}
\usepackage{bm}
\usepackage{booktabs}
\usepackage[table]{xcolor}
\usepackage{tikz}
\usepackage[backgroundcolor=lightgray]{todonotes}
\usepackage{complexity}
\usepackage{subcaption}
\usepackage{multirow}
\usepackage[binary-units]{siunitx}
\usepackage[inline]{enumitem}

\usetikzlibrary{cd}
\usetikzlibrary{bayesnet}
\usetikzlibrary{calc}

\newtheorem{theorem}{Theorem}
\newtheorem{observation}{Observation}
\newtheorem{lemma}{Lemma}
\newtheorem{proposition}{Proposition}
\newtheorem{corollary}{Corollary}
\theoremstyle{definition}
\newtheorem{definition}{Definition}
\newtheorem{example}{Example}
\theoremstyle{remark}
\newtheorem*{remark}{Remark}

\DeclareMathOperator{\im}{im}

\title{Weighted Model Counting with Conditional Weights}
\author{Paulius Dilkas}

\begin{document}
\maketitle

\section{Introduction}

\begin{itemize}
\item The main narrative
  \begin{enumerate}
  \item Putting weights on literals is restrictive.
  \item To overcome this, it is common practice to add more literals.
  \item This is indeed always possible but much slower.
  \item We show how we can define conditional weights on literals, allowing us
    to encode any measure.
  \item We demonstrate a specific example of this by presenting a new way to
    encode Bayesian networks into instances of WMC and adapting a WMC algorithm
    (ADDMC) to run on the new format.
  \item We show that this results in significantly faster inference.
  \item We show that our encoding results in asymptotically fewer literals and
    fewer ADDs.
  \end{enumerate}
\item Introduce
  \begin{itemize}
  \item WMC \cite{DBLP:conf/aaai/SangBK05}
  \item ADDMC \cite{DBLP:conf/aaai/DudekPV20} and ADDs
    \cite{DBLP:journals/fmsd/BaharFGHMPS97}
  \item A Bayesian network is a directed acyclic graph with random variables as
    vertices that defines a probability distribution over them. The full
    probability distribution can be expressed as the product of the conditional
    probability distributions of all random variables. For discrete Bayesian
    networks (and we only consider discrete networks in this paper), each such
    distribution can be represented by a table known as a conditional
    probability table (CPT).
  \end{itemize}
\item[F] What are the main claims, what are the main takeaways, intuitive [???]
  of theorems to follow.
\item Our work is, in spirit, similar to... using ROBDDs for Inference in
  Bayesian Networks with Troubleshooting as an Example
  \cite{DBLP:conf/uai/NielsenWJK00}: this is an interesting approach where
  deterministic parts of a BN are expressed as Boolean functions, i.e.,
  extracting the logical from the probabilistic. Maybe I can make a case that my
  encoding (the textual version) kind of does the same, even though the
  algorithm doesn't.
\end{itemize}

\section{Related Work}

\paragraph{Using WMC to perform inference on Bayesian networks.} Hitherto, four
techniques have been proposed for encoding Bayesian networks into instances of
WMC. We will identify them based on the initials of authors as well as
publications years: \texttt{d02} \cite{DBLP:conf/kr/Darwiche02}, \texttt{sbk05}
\cite{DBLP:conf/aaai/SangBK05}, \texttt{cd05} \cite{DBLP:conf/ijcai/ChaviraD05},
and \texttt{cd06} \cite{DBLP:conf/sat/ChaviraD06}. Below we summarise the
observed performance differences among them. Sang et al.
\cite{DBLP:conf/aaai/SangBK05} claim that \texttt{sbk05} is a smaller encoding
than \texttt{d02} with respect to both the number of clauses and the number of
variables but provide no experimental comparison. Chavira and Darwiche
\cite{DBLP:conf/ijcai/ChaviraD05} compare \texttt{cd05} with \texttt{d02} by
measuring the time it takes to compile either encoding into an arithmetic
circuit (but do not measure inference time). The more recent encoding
\texttt{cd05} always compiles faster and results in a smaller arithmetic circuit
(as measured by the number of edges). In their subsequent paper, the same
authors perform two sets of experiments (that are relevant to this summary)
\cite{DBLP:conf/sat/ChaviraD06}. First, they compile \texttt{cd05} and
\texttt{cd06} encodings into d-DNNF (i.e., deterministic decomposable negation
normal form \cite{DBLP:journals/jancl/Darwiche01}), measuring both compilation
time and numbers of edges in the d-DNNF diagram. The results are mostly in
favour of \texttt{cd06}. Second, they compare the inference time of
\texttt{sbk05} run with Cachet \cite{DBLP:conf/sat/SangBBKP04} with the compile
times of \texttt{cd05} and \texttt{cd06}, but only on five (types of) instances.
In these experiments, \texttt{cd06} is always faster than \texttt{cd05}, while
the comparison with \texttt{sbk05} is mixed. Sometimes \texttt{cd06} is orders
of magnitude faster than \texttt{sbk05}, sometimes slightly slower. The
performance difference between \texttt{sbk05} and \texttt{cd05} is even harder
to judge: \texttt{sbk05} is better on three out of five instances and worse on
the remaining two. Based on this description, one would expect \texttt{cd06} to
be faster than both \texttt{cd05} and \texttt{sbk05}, both of which should be
faster than \texttt{d02}. The experiments in \cref{sec:experiments}, however,
strongly disagree with this prediction, showing that the quality of an encoding
depends strongly on the underlying search algorithm or compilation technique.

\paragraph{ADDs and their use in probabilistic inference.} ADDs provide an
efficient way to manipulate functions from a Boolean algebra to any algebraic
structure (most commonly the real numbers)
\cite{DBLP:journals/fmsd/BaharFGHMPS97}. They have been used to represent value
functions in Markov decision processes \cite{DBLP:conf/uai/HoeySHB99} and, for
Bayesian network inference, to represent each CPT as an ADD
\cite{DBLP:conf/icml/ZhaoMP15} as well as by combining ADDs and arithmetic
circuits into a  single representation \cite{DBLP:conf/ijcai/ChaviraD07}. ADDs
are particularly advantageous in this situation because of their ability to
fully exploit context-specific independence, i.e., observable structure within a
CPT that is not inherited from the structure of the Bayesian network
\cite{DBLP:conf/uai/BoutilierFGK96}.

\section{Boolean Algebras, Power Sets, and Propositional
  Logic} \label{sec:prelims}

\begin{table}
  \centering
  \caption{A comparison of Boolean-algebraic (BA) and set-theoretic (ST)
    concepts for $2^U$ for any set $U$}
  \label{tbl:notation}
  \begin{tabular}{lccl}
    \toprule
    BA name & BA symbol & ST symbol & ST name \\
    \midrule
    bottom & $\bot$ & $\emptyset$ & empty set \\
    top & $\top$ & $U$ & \\
    meet, and & $\land$ & $\cap$ & intersection \\
    join, or & $\lor$ & $\cup$ & union \\
    complement, not & $\neg$ & $^c$ & complement \\
            & $\le$ & $\subseteq$ & subset relation, set inclusion \\
    atom & & & singleton, unit set \\
    \bottomrule
  \end{tabular}
\end{table}

\begin{table}
  \caption{Notation for a logic with two atoms. The elements in both columns are
    listed in the same order.}
  \label{tbl:notation_example}
  \centering
  \begin{tabular}{lcc}
    \toprule
    Name in logic & Boolean-algebraic notation & Set-theoretic notation \\
    \midrule
    Atoms (elements of $U$) & $a, b$ & $a, b$ \\
    \rowcolor{gray!10} Models (elements of $2^U$) & $\neg a \land \neg b, a \land \neg b, \neg a \land b, a \land b$ & $\emptyset, \{a\}, \{b\}, \{a, b\}$ \\
    & $\top$ & $\{ \emptyset, \{a\}, \{b\}, \{a, b\} \}$ \\
    & $\neg a \lor \neg b, a \to b$ & $\{ \emptyset, \{a\}, \{b\} \}, \{ \emptyset, \{a\}, \{a, b\} \}$ \\
    & $b \to a, a \lor b$ & $\{ \emptyset, \{b\}, \{a, b\} \}, \{ \{a\}, \{b\}, \{a, b\} \}$ \\
    & $\neg b, \neg a, a \leftrightarrow b$ & $\{\emptyset, \{a\}\}, \{\emptyset, \{b\}\}, \{\emptyset, \{a, b\}\}$ \\
    & $(a \land \neg b) \lor (b \land \neg a), a, b$ & $\{\{a\}, \{b\}\}, \{\{a\}, \{a, b\}\}, \{\{b\}, \{a, b\}\}$ \\
    & $\neg a \land \neg b, a \land \neg b, \neg a \land b, a \land b$ & $\{\emptyset\}, \{\{a\}\}, \{\{b\}\}, \{\{a, b\}\}$ \\
    \multirow{-7}{*}{Formulas (elements of $2^{2^U}$)} & $\bot$ & $\emptyset$ \\
    \bottomrule
  \end{tabular}
\end{table}

Let $\mathcal{L}$ be a propositional logic with atoms $a$ and $b$, and let $U =
\{ a, b \}$. Then $2^U$, the power set of $U$, is the set of all models of
$\mathcal{L}$, and $2^{2^U}$ is the set of all formulas. These sets can also be
represented as Boolean algebras---see \cref{tbl:notation} for an overview of
notational differences and \cref{tbl:notation_example} for examples of how
various elements can be represented in both notations. Most importantly, note
that the word \emph{atom} has completely different meanings in logic and in
Boolean algebras. An atom in $\mathcal{L}$ is an atomic formula, i.e., an
element of $U$, whereas an atom in a Boolean algebra is (in set-theoretic terms)
a singleton set. For instance, an atom in $2^{2^U}$ corresponds to a model of
$\mathcal{L}$, i.e., an element of $2^U$. Unless referring specifically to a
logic, we will use the algebraic definition of an atom while referring to
logical atoms as \emph{variables}. In the rest of the paper, for any set $U$, we
will use set-theoretic notation for $2^U$ and Boolean-algebraic notation for
$2^{2^U}$, except for (Boolean) atoms in $2^{2^U}$ that are denoted as $\{x\}$
for some model $x \in 2^U$.

\subsection{The Space of Functions on Boolean Algebras}

We build on the definitions of multiplication and projection in the ADDMC paper
\cite{DBLP:conf/aaai/DudekPV20} and define more operations that can be used to
manipulate functions from Boolean algebras to non-negative real numbers. All of
these operations have efficient implementations in the CUDD
\cite{somenzi1998cudd} package for manipulating ADDs (among other things) that
is used by ADDMC {\cite{DBLP:conf/aaai/DudekPV20}.

\begin{definition}[Operations on functions]
  Let $\alpha\colon 2^X \to \mathbb{R}_{\ge 0}$ and $\beta\colon 2^Y \to
  \mathbb{R}_{\ge 0}$ be functions, $p \in \mathbb{R}_{\ge 0}$, and $x \in X$.
  We define the following operations:
  \begin{description}
  \item[Addition:] $\alpha + \beta\colon 2^{X \cup Y} \to \mathbb{R}_{\ge 0}$ is
    such that $(\alpha + \beta)(T) = \alpha(T \cap X) + \beta(T \cap Y)$ for all
    $T \in 2^{X \cup Y}$.
  \item[Multiplication:] $\alpha \cdot \beta\colon 2^{X \cup Y} \to
    \mathbb{R}_{\ge 0}$ is such that $(\alpha \cdot \beta)(T) = \alpha(T \cap X)
    \cdot \beta(T \cap Y)$ for all $T \in 2^{X \cup Y}$.
  \item[Scalar multiplication:] $p\alpha\colon 2^X \to \mathbb{R}_{\ge 0}$ is
    such that $(p\alpha)(T) = p \cdot \alpha(T)$ for all $T \in 2^X$.
  \item[Complement:] $\overline{\alpha}\colon 2^X \to \mathbb{R}_{\ge 0}$ is
    such that $\overline{\alpha}(T) = 1 - \alpha(T)$ for all $T \in 2^X$.
  \item[Projection:] $\exists_x\alpha\colon 2^{X \setminus \{ x \}} \to
    \mathbb{R}_{\ge 0}$ is such that $(\exists_x\alpha)(T) = \alpha(T) +
    \alpha(T \cup \{ x \})$ for all $T \in 2^{X \setminus \{x \}}$. For any $Z =
    \{ z_1, \dots, z_n \} \subseteq X$, we write $\exists_Z$ to mean
    $\exists_{z_1}\dots\exists_{z_n}$.
  \end{description}
\end{definition}

\begin{observation}
  Let $U$ be a set, and $\mathcal{V} = \{ \alpha\colon 2^X \to \mathbb{R}_{\ge 0}
  \mid X \subseteq U \}$. Then $\mathcal{V}$ is a semi-vector space with three
  additional operations: (non-scalar) multiplication, complement, and projection.
  Specifically, note that both addition and multiplication are both associative
  and commutative.
\end{observation}

We end the discussion on function spaces by defining several special functions:
unit $1\colon 2^\emptyset \to \mathbb{R}_{\ge 0}$ defined as $1(\emptyset) = 1$,
zero $0\colon 2^\emptyset \to \mathbb{R}_{\ge 0}$ defined as $0(\emptyset) = 0$,
and function $[a]\colon 2^{\{a\}} \to \mathbb{R}_{\ge 0}$ defined as
$[a](\emptyset) = 0$, $[a](\{a\}) = 1$ for any $a$. Henceforth, for any function
$\alpha\colon 2^X \to \mathbb{R}_{\ge 0}$ and any set $T$, we will write
$\alpha(T)$ to mean $\alpha(T \cap X)$.

\section{WMC as a Measure}

Let $U$ be a set. A \emph{measure} is a function $\mu\colon 2^{2^U} \to
\mathbb{R}_{\ge 0}$ such that $\mu(\bot) = 0$, and $\mu(a \lor b) = \mu(a) +
\mu(b)$ for all $a, b \in 2^{2^U}$ whenever $a \land b = \bot$. A \emph{weight
  function} is a function $\nu\colon 2^U \to \mathbb{R}_{\ge 0}$. A weight
function is \emph{factored} if $\nu = \prod_{x \in U} \nu_x$ for some functions
$\nu_x\colon 2^{\{x\}} \to \mathbb{R}_{\ge 0}$, $x \in U$. We say that a weight
function $\nu\colon 2^U \to \mathbb{R}_{\ge 0}$ \emph{induces} a measure
$\mu_\nu\colon 2^{2^U} \to \mathbb{R}_{\ge 0}$ if
\begin{equation} \label{eq:induced_measure}
  \mu_\nu(x) = \sum_{\{u\} \le x} \nu(u).
\end{equation}
Finally, a measure $\mu\colon 2^{2^U} \to \mathbb{R}_{\ge 0}$ is
\emph{factorable} if there exists a factored weight function $\nu\colon 2^U \to
\mathbb{R}_{\ge 0}$ that induces $\mu$.

\begin{lemma} \label{prop:measure}
  The function $\mu_\nu$, as defined by \cref{eq:induced_measure}, is a measure.
\end{lemma}
\begin{proof}
  Note that $\mu_\nu(\bot) = 0$ since there are no atoms below $\bot$. Let $a, b
  \in 2^{2^{U}}$ be such that $a \land b = \bot$. By elementary properties of
  Boolean algebras, all atoms below $a \lor b$ are either below $a$ or below
  $b$. Moreover, none of them can be below both $a$ and $b$ because then they
  would have to be below $a \land b = \bot$. Thus
  \[
    \mu_\nu(a \lor b) = \sum_{\{u\} \le a \lor b} \nu(u) = \sum_{\{u\} \le a}
    \nu(u) + \sum_{\{u\} \le b} \nu(u) = \mu_\nu(a) + \mu_\nu(b)
  \]
  as required.
\end{proof}

In this formulation, the process of calculating the value of $\mu_\nu(x)$ for
some $x \in 2^{2^U}$ with a given definition of $\nu$ is known as WMC.

\paragraph{Relation to the classical (logic-based) view of WMC.} Let
$\mathcal{L}$ be a propositional logic with two atoms $a$ and $b$ as in
\cref{sec:prelims} and $w\colon \{ a, b, \neg a, \neg b \} \to \mathbb{R}_{\ge
  0}$ a \emph{weight function} defined as $w(a) = 0.3$, $w(\neg a) = 0.7$, $w(b)
= 0.2$, $w(\neg b) = 0.8$. Furthermore, let $\Delta$ be a theory in
$\mathcal{L}$ with a sole axiom $a$. Then $\Delta$ has two models: $\{ a, b \}$
and $\{ a, \neg b \}$ and its WMC \cite{DBLP:journals/ai/ChaviraD08} is
\begin{equation} \label{eq:wmc_example}
  \mathrm{WMC}(\Delta) = \sum_{\omega \models \Delta} \prod_{\omega \models l} w(l) = w(a)w(b) + w(a)w(\neg b) = 0.3.
\end{equation}
Alternatively, we can define $\nu_a\colon 2^{\{a\}} \to \mathbb{R}_{\ge 0}$ as
$\nu_a(\{ a \}) = 0.3$, $\nu_a(\emptyset) = 0.7$ and $\nu_b\colon 2^{\{b\}} \to
\mathbb{R}_{\ge 0}$ as $\nu_b(\{ b \}) = 0.2$, $\nu_b(\emptyset) = 0.8$. Let
$\mu$ be the measure on $2^{2^U}$ induced by $\nu = \nu_a \cdot \nu_b$. Then,
equivalently to \cref{eq:wmc_example}, we can write
\[
  \mu(a) = \nu(\{ a, b \}) + \nu(\{ a \}) = \nu_a(\{a\})\nu_b(\{b\}) +
  \nu_a(\{a\})\nu_b(\emptyset) = 0.3.
\]

\subsection{Not All Measures Are Factorable}

\todo[inline]{Introduce the subsection and give it some context.}

\begin{example}
  Let $U = \{a, b\}$ be a set of atoms and $\mu\colon 2^{2^U} \to
  \mathbb{R}_{\ge 0}$ a measure defined as:\footnote{The value of $\mu$ on any
    other element of the Boolean algebra can be deduced using the definition.}
  \begin{align*}
    \mu(a \land b) &= 0.72, \\
    \mu(a \land \neg b) &= 0.18, \\
    \mu(\neg a \land b) &= 0.07, \\
    \mu(\neg a \land \neg b) &= 0.03.
  \end{align*}
  If $\mu$ could be represented using traditional (factored) WMC, we would have
  to find two weight functions $\nu_a\colon 2^{\{a\}} \to \mathbb{R}_{\ge 0}$
  and $\nu_b\colon 2^{\{b\}} \to \mathbb{R}_{\ge 0}$ such that $\nu = \nu_a
  \cdot \nu_b$ induces $\mu$, i.e., $\nu_a$ and $\nu_b$ would have to satisfy
  this system of equations:
  \begin{align*}
    \nu_a(\{a\}) \cdot \nu_b(\{b\}) &= 0.72 \\
    \nu_a(\{a\}) \cdot \nu_b(\emptyset) &= 0.18 \\
    \nu_a(\emptyset) \cdot \nu_b(\{b\}) &= 0.07 \\
    \nu_a(\emptyset) \cdot \nu_b(\emptyset) &= 0.03,
  \end{align*}
  which has no solutions.

  Alternatively, we can let $b$ depend on $a$ and consider weight functions
  $\nu_a\colon 2^{\{a\}} \to \mathbb{R}_{\ge 0}$ and $\nu_b\colon 2^{\{a, b\}}
  \to \mathbb{R}_{\ge 0}$ defined as $\nu_a(\{a\}) = 0.9$, $\nu_a(\emptyset) =
  0.1$, and $\nu_b(\{a, b\}) = 0.8$, $\nu_b(\{a\}) = 0.2$, $\nu_b(\{b\}) = 0.7$,
  $\nu_b(\emptyset) = 0.3$. One can easily check that with these definitions
  $\nu$ indeed induces $\mu$.
\end{example}

Note that in this case we chose to interpret $\nu_b$ as $\Pr(b \mid a)$
while---with a different definition of $\nu_b$ that represents the joint
probability distribution $\Pr(a, b)$---$\nu_b$ by itself could induce $\mu$. In
general, however, factorising the full weight function into several smaller
functions often results in weight functions with smaller domains which leads to
increased efficiency and decreased memory usage \cite{DBLP:conf/aaai/DudekPV20}.

\todo[inline]{Add the proof that one can always go from any measure to a
  factorable measure by adding more literals and say: Not all measures are
  factorable but it is common practice to add more literals to make the measure
  factorable. We show that it's always possible and yet it can be faster not to
  do it.}

\section{Encoding Bayesian Networks Using Conditional Weights}

\begin{figure}
  \centering
  \begin{subfigure}{0.2\textwidth}
    \centering
    \begin{tikzpicture}[edge from parent/.style={draw,-latex}]
      \node[draw,circle] {$W$}
      child {node[draw,circle] {$F$}}
      child {node[draw,circle] {$T$}};
    \end{tikzpicture}
  \end{subfigure}%
  \begin{subfigure}{0.8\textwidth}
    \centering
    \begin{tabular}[t]{cc}
      \toprule
      $w$ & $\Pr(W = w)$ \\
      \midrule
      1 & 0.5 \\
      0 & 0.5 \\
      \bottomrule
    \end{tabular}
    \begin{tabular}[t]{ccc}
      \toprule
      $w$ & $f$ & $\Pr(F = f \mid W = w)$ \\
      \midrule
      1 & 1 & 0.6 \\
      1 & 0 & 0.4 \\
      0 & 1 & 0.1 \\
      0 & 0 & 0.9 \\
      \bottomrule
    \end{tabular}
    \begin{tabular}[t]{ccc}
      \toprule
      $w$ & $t$ & $\Pr(T = t \mid W = w)$ \\
      \midrule
      1 & $l$ & 0.2 \\
      1 & $m$ & 0.4 \\
      1 & $h$ & 0.4 \\
      0 & $l$ & 0.6 \\
      0 & $m$ & 0.3 \\
      0 & $h$ & 0.1 \\
      \bottomrule
    \end{tabular}
  \end{subfigure}
  \caption{An example Bayesian network with its CPTs}
  \label{fig:example_bn}
\end{figure}

In this section, we describe a way to encode Bayesian networks into WMC without
restricting oneself to factorable measures and thus having to add extra
variables to maintain literal independence. In line with the names of previous
encodings, we shall call this encoding \texttt{db21}. Recall that a Bayesian
network is a directed acyclic graph with random variables as vertices. Let
$\mathcal{V}$ denote the set of random variables. For any random variable $X \in
\mathcal{V}$, let $\im X$ denote its set of possible values and $\mathrm{pa}(X)$
the set of its parents. The full probability distribution is then equal to
$\prod_{X \in \mathcal{V}} \Pr(X \mid \mathrm{pa}(X))$. See
\cref{fig:example_bn} for an example Bayesian network which we will refer to
throughout this section. For this network, $\mathcal{V} = \{ W, F, T \}$,
$\mathrm{pa}(W) = \emptyset$, $\mathrm{pa}(F) = \mathrm{pa}(T) = \{ W \}$, $\im
W = \im F = \{0, 1 \}$, and $\im T = \{ l, m, h \}$.

\begin{definition}[Indicator variables]
  Let $X \in \mathcal{V}$ be a random variable. If $X$ is binary (i.e., $|\im X|
  = 2$), we can arbitrary identify one of the values as $1$ and the other one as
  $0$ (i.e, $\im X \cong \{ 0, 1 \}$). Then $X$ can be represented by a single
  \emph{indicator variable} $\lambda_{X=1}$. For notational simplicity, for any
  set $S$, we write $\lambda_{X=0} \in S$ or $S = \{ \lambda_{X=0}, \dots \}$ to
  mean $\lambda_{X=1} \not\in S$.

  On the other hand, if $X$ is not binary, we represent $X$ with $|\im X|$
  indicator variables, one for each value. We let
  \[
    \mathcal{E}(X) = \begin{cases}
      \{ \lambda_{X=1} \} & \text{if } |\im X| = 2 \\
      \{ \lambda_{X=x} \mid x \in \im X \} & \text{otherwise.}
    \end{cases}
  \]
  denote the set of indicator variables for $X$ and $\mathcal{E}^*(X) =
  \mathcal{E}(X) \cup \bigcup_{Y \in \mathrm{pa}(X)} \mathcal{E}(Y)$ denote the
  set of indicator variables for $X$ and its parents in the Bayesian network.
  Finally, let $U = \bigcup_{X \in \mathcal{V}} \mathcal{E}(X)$ denote the set
  of all indicator variables for all random variables in the Bayesian network.
\end{definition}

For the Bayesian network in \cref{fig:example_bn}, this gives us:
\begin{align*}
  \mathcal{E}(W) &= \{ \lambda_{W=1} \}, \\
  \mathcal{E}(F) &= \{ \lambda_{F=1} \}, \\
  \mathcal{E}(T) &= \{ \lambda_{T=l}, \lambda_{T=m}, \lambda_{T=h} \}, \\
  \mathcal{E}^*(W) &= \{ \lambda_{W=1} \}, \\
  \mathcal{E}^*(F) &= \{ \lambda_{F=1}, \lambda_{W=1} \}, \\
  \mathcal{E}^*(T) &= \{ \lambda_{T=l}, \lambda_{T=m}, \lambda_{T=h}, \lambda_{W=1} \}.
\end{align*}

\begin{algorithm}
  \caption{Encoding a Bayesian network as a function $2^U \to \mathbb{R}_{\ge
      0}$}
  \label{alg:encoding}
  \KwData{a Bayesian network with vertices $\mathcal{V}$ and probability
    distribution $\Pr$}
  \KwResult{a function $\phi\colon 2^U \to \mathbb{R}_{\ge 0}$}
  $\phi \gets 1$\;
  \For{$X \in \mathcal{V}$}{
    \textit{let} $\mathrm{pa}(X) = \{ Y_1, \dots, Y_n \}$\;
    $\mathrm{CPT}_X \gets 0$\;
    \eIf{$|\im X| = 2$}{
      \For{$(y_1, \dots, y_n) \in \prod_{i = 1}^n \im Y_i$}{
        $p_1 \gets \Pr(X = 1 \mid Y_1 = y_1, \dots, Y_n = y_n)$\;
        $p_0 \gets \Pr(X \ne 1 \mid Y_1 = y_1, \dots, Y_n = y_n)$\;
        $\mathrm{CPT}_X \gets \mathrm{CPT}_X + p_1[\lambda_{X=1}] \cdot
        \prod_{i=1}^n [\lambda_{Y_i=y_i}] + p_0 \overline{[\lambda_{X=1}]} \cdot
        \prod_{i=1}^n [\lambda_{Y_i=y_i}]$\;
      }
    }{
      \textit{let} $\im X = \{ x_1, \dots, x_m \}$\;
      \For{$x \in \im X$ {\rm \textbf{and}} $(y_1, \dots, y_n) \in \prod_{i =
          1}^n \im Y_i$}{
        $p_x \gets \Pr(X = x \mid Y_1 = y_1, \dots, Y_n = y_n)$\;
        $\mathrm{CPT}_X \gets \mathrm{CPT}_X + p_x[\lambda_{X=x}] \cdot
        \prod_{i=1}^n [\lambda_{Y_i=y_i}] + \overline{[\lambda_{X=1}]} \cdot
        \prod_{i=1}^n [\lambda_{Y_i=y_i}]$\;
      }
      $\mathrm{CPT}_X \gets \mathrm{CPT}_X \cdot \left( \sum_{i=1}^m [\lambda_{X
          = x_i}] \right) \cdot \prod_{i=1}^m \prod_{j=i+1}^m
      (\overline{[\lambda_{X = x_i}]} + \overline{[\lambda_{X = x_j}]})$\;
    }
    $\phi \gets \phi \cdot \mathrm{CPT}_X$\;
  }
  \Return{$\phi$}\;
\end{algorithm}

\Cref{alg:encoding} shows how a Bayesian network with vertices $\mathcal{V}$
can be represented as a weight function $\phi\colon 2^U \to \mathbb{R}_{\ge 0}$.
The algorithm begins with the unit function and multiplies it by
$\mathrm{CPT}_X\colon 2^{\mathcal{E}^*(X)} \to \mathbb{R}_{\ge 0}$ for each
random variable $X \in \mathcal{V}$. We call each such function a
\emph{conditional weight function} as it represents a conditional probability
distribution. However, the distinction is primarily a semantic one: a function
$2^{\{a, b\}} \to \mathbb{R}_{\ge 0}$ can represent $\Pr(a \mid b)$, $\Pr(b \mid
a)$, or something else entirely.

For a binary random variable $X$, $\mathrm{CPT}_X$ is simply a sum of smaller
functions, one for each row of the CPT. If $X$ has more than two values, we also
multiply $\mathrm{CPT}_X$ by some `clause' functions that restrict the value of
$\phi(T)$ to zero whenever $|\mathcal{E}(X) \cap T| \ne 1$. For the Bayesian
network in \cref{fig:example_bn}, we get:
\begin{align*}
  \mathrm{CPT_W} &= 0.5[\lambda_{W=1}]+0.5\overline{[\lambda_{W=1}]} = 0.5 \cdot 1, \\
  \mathrm{CPT_F} &= 0.6[\lambda_{F=1}] \cdot [\lambda_{W=1}] + 0.4[\lambda_{F=0}] \cdot [\lambda_{W=1}] + 0.1[\lambda_{F=1}] \cdot [\lambda_{W=0}] + 0.9[\lambda_{F=0}] \cdot [\lambda_{W=0}] \\
    &= 0.6[\lambda_{F=1}] \cdot [\lambda_{W=1}] + 0.4\overline{[\lambda_{F=1}]} \cdot [\lambda_{W=1}] + 0.1[\lambda_{F=1}] \cdot \overline{[\lambda_{W=1}]} + 0.9\overline{[\lambda_{F=1}]} \cdot \overline{[\lambda_{W=1}]}, \\
  \mathrm{CPT_T} &= ([\lambda_{T=l}] + [\lambda_{T=m}] + [\lambda_{T=h}]) \cdot (\overline{[\lambda_{T=l}]} + \overline{[\lambda_{T=m}]}) \cdot (\overline{[\lambda_{T=l}]} + \overline{[\lambda_{T=h}]}) \cdot (\overline{[\lambda_{T=m}]} + \overline{[\lambda_{T=h}]}) \cdot (\dots).
\end{align*}

\subsection{Proof of Correctness}

\Cref{alg:encoding} produces a function with a Boolean algebra as its domain.
This function can be represented by an ADD. The core of ADDMC works by taking an
ADD $\psi\colon 2^{U} \to \mathbb{R}_{\ge 0}$ (expressed as a product of smaller
ADDs) and returning $(\exists_U\psi)(\emptyset)$
\cite{DBLP:conf/aaai/DudekPV20}. In this section, we prove that the function
$\phi$ produced by \cref{alg:encoding} can be used by ADDMC to correctly compute
any marginal probability of the Bayesian network that was encoded as
$\phi$.\footnote{It can just as well compute any probability expressed using the
random variables in $\mathcal{V}$, but we focus mostly on marginal probabilities
in our experiments.} First, \cref{lemma:cpt} shows that any conditional weight function
produces the right answer when given an appropriate encoding of
variable-value assignments for a conditional probability in the Bayesian
network. Then, \cref{lemma:full_distribution} shows that $\phi$ represents the
full probability distribution of the Bayesian network, i.e., it gives the right
probabilities for the right inputs and zero otherwise. We end with
\cref{thm:correctness} that shows how $\phi$ can be combined with an encoding of
a single variable-value assignment so that ADDMC would compute its marginal
probability.

\begin{lemma} \label{lemma:cpt}
  Let $X \in \mathcal{V}$ be a random variable with parents $\mathrm{pa}(X) = \{ Y_1,
  \dots, Y_n \}$. Then $\mathrm{CPT}_X\colon 2^{\mathcal{E}^*(X)} \to
  \mathbb{R}_{\ge 0}$ is such that for any $x \in \im X$ and $(y_1, \dots, y_n)
  \in \prod_{i=1}^n \im Y_i$,
  \[
    \mathrm{CPT}_X (\{ \lambda_{X=x} \} \cup \{ \lambda_{Y_i=y_i} \mid i = 1,
    \dots, n \}) = \Pr(X = x \mid Y_1 = y_1, \dots, Y_n = y_n).
  \]
\end{lemma}
\begin{proof}
  Let $T = \{ \lambda_{X=x} \} \cup \{ \lambda_{Y_i=y_i} \mid i = 1, \dots, n
  \}$. If $X$ is binary, then $\mathrm{CPT}_X$ is a sum of $2\prod_{i=1}^n |\im
  Y_i|$ terms, one for each possible assignment of values to variables $X, Y_1,
  \dots, Y_n$. Exactly one of these terms is nonzero when applied to $T$, and
  it is equal to $\Pr(X = x \mid Y_1 = y_1, \dots, Y_n = y_n)$ by definition.

  If $X$ is not binary, then $\left( \sum_{i=1}^m [\lambda_{X = x_i}]
  \right)(T) = 1$, and $\left( \prod_{i=1}^m \prod_{j=i+1}^m
    (\overline{[\lambda_{X = x_i}]} + \overline{[\lambda_{X = x_j}]})
  \right)(T) = 1$, so $\mathrm{CPT}_X(T) = \Pr(X = x \mid Y_1 = y_1,
  \dots, Y_n = y_n)$ by a similar argument as before.
\end{proof}

\begin{proposition} \label{lemma:full_distribution}
  Let $\mathcal{V} = \{X_1, \dots, X_n\}$. Then
  \[
    \phi(T) =
    \begin{cases}
      \Pr(X_1 = x_1, \dots, X_n = x_n) & \text{if } T = \{ \lambda_{X_i=x_i}
      \mid i = 1, \dots, n \} \text{ for some } (x_1, \dots, x_n) \in
      \prod_{i=1}^n \im X_i \\
      0 & \text{otherwise,}
    \end{cases}
  \]
  for all $T \in 2^U$.
\end{proposition}
\begin{proof}
  If $T = \{ \lambda_{X=v_X} \mid X \in \mathcal{V} \}$ for some $(v_X)_{X
    \in \mathcal{V}} \in \prod_{X \in \mathcal{V}} \im X$, then
  \[
    \phi(T) = \prod_{X \in \mathcal{V}} \Pr \left( X=v_X \;\middle|\;
      \bigwedge_{Y \in \mathrm{pa}(X)} Y=v_Y \right) = \Pr \left( \bigwedge_{X
        \in \mathcal{V}} X=v_X \right)
  \]
  by \cref{lemma:cpt} and the definition of a Bayesian network. Otherwise there
  must be some non-binary random variable $X \in \mathcal{V}$ such that
  $|\mathcal{E}(X) \cap T| \ne 1$. If $\mathcal{E}(X) \cap T = \emptyset$, then
  $\left( \sum_{i=1}^m [\lambda_{X = x_i}] \right)(T) = 0$, and so
  $\mathrm{CPT}_X(T) = 0$, and $\phi(T) = 0$. If $|\mathcal{E}(X) \cap T| > 1$,
  then we must have two different values $x_1, x_2 \in \im X$ such that
  $\{\lambda_{X=x_1}, \lambda_{X=x_2} \} \subseteq T$ which means that
  $(\overline{[\lambda_{X=x_1}]} + \overline{[\lambda_{X=x_2}]})(T) = 0$, and
  so, again, $\mathrm{CPT}_X(T) = 0$, and $\phi(T) = 0$.
\end{proof}

\begin{theorem} \label{thm:correctness}
  For any $X \in \mathcal{V}$ and $x \in \im X$, $(\exists_U(\phi \cdot
  [\lambda_{X=x}]))(\emptyset) = \Pr(X = x)$.
\end{theorem}
\begin{proof}
  Let $\mathcal{V} = \{ X, Y_1, \dots, Y_n \}$. Then
  \begin{align*}
    (\exists_U (\phi \cdot [\lambda_{X=x}]))(\emptyset) &= \sum_{T \in 2^U} (\phi \cdot [\lambda_{X=x}])(T) = \sum_{\lambda_{X=x} \in T \in 2^U} \phi(T) = \sum_{\lambda_{X=x} \in T \in 2^U} \left( \prod_{Y \in \mathcal{V}} \mathrm{CPT}_Y \right)(T) \\
    &= \sum_{(y_1, \dots, y_n) \in \prod_{i=1}^n \im Y_i} \Pr(X = x, Y_1 = y_1, \dots, Y_n = y_n) = \Pr(X = x)
  \end{align*}
  by the following arguments:
  \begin{itemize}
  \item the proof of Theorem~1 in the ADDMC paper \cite{DBLP:conf/aaai/DudekPV20};
  \item if $\lambda_{X=x} \not\in T \in 2^U$, then $(\phi \cdot
    [\lambda_{X=x}])(T) = \phi(T) \cdot [\lambda_{X=x}](T \cap \{
    \lambda_{X=x} \}) = \phi(T) \cdot 0 = 0$;
  \item \cref{lemma:full_distribution};
  \item marginalisation of a probability distribution.
  \end{itemize}
\end{proof}
\todo[inline]{Need to make the proof nicer.}

\subsection{Textual Representation} \label{sec:textual_representation}

\Cref{alg:encoding} encodes a Bayesian network into a function on a Boolean
algebra, but how does it relate to the standard interpretation of a WMC encoding
as a formula in conjunctive normal form (CNF) together with a collection of
weights? The factors of $\phi$ that restrict the values of indicator variables
for non-binary random variables are already expressed as a product of sums of
0/1-valued functions, i.e., a kind of CNF. Disregarding these functions, each
conditional weight function $\mathrm{CPT}_X$ is represented by a sum with a term
for every subset of $\mathcal{E}^*(X)$. To encode these terms, we introduce
\emph{extended weight clauses} to the WMC format used by Cachet
\cite{DBLP:conf/sat/SangBBKP04}. For instance, here is a representation of the
Bayesian network from \cref{fig:example_bn}:
\[
  \begin{array}{lrrll}
    \lambda\sb{T=l} &\lambda\sb{T=m} &\lambda\sb{T=h} & &0 \\
                    &-\lambda\sb{T=l} &-\lambda\sb{T=m} & &0 \\
                    &-\lambda\sb{T=l} &-\lambda\sb{T=h} & &0 \\
                    &-\lambda\sb{T=m} &-\lambda\sb{T=h} & &0 \\
    w &\lambda\sb{W=1} & &0.5 &0.5 \\
    w &\lambda\sb{F=1} &\lambda\sb{W=1} &0.6 &0.4 \\
    w &\lambda\sb{F=1} &-\lambda\sb{W=1} &0.1 &0.9 \\
    w &\lambda\sb{T=l} &\lambda\sb{W=1} &0.2 &1 \\
    w &\lambda\sb{T=m} &\lambda\sb{W=1} &0.4 &1 \\
    w &\lambda\sb{T=h} &\lambda\sb{W=1} &0.4 &1 \\
    w &\lambda\sb{T=l} &-\lambda\sb{W=1} &0.6 &1 \\
    w &\lambda\sb{T=m} &-\lambda\sb{W=1} &0.3 &1 \\
    w &\lambda\sb{T=h} &-\lambda\sb{W=1} &0.1 &1
  \end{array}
\]
where each indicator variable is eventually replaced with a unique positive
integer. Each line prefixed with a $w$ can be split into four parts: the `main'
variable (always not negated), conditions (possibly none), and two weights. For
example, the line
\[
  \begin{array}{lrrll}
    w &\lambda\sb{T=m} &-\lambda\sb{W=1} &0.3 &1
  \end{array}
\]
encodes the function $0.3[\lambda_{T=m}] \cdot \overline{[\lambda_{W=1}]} +
1\overline{[\lambda_{T=m}]} \cdot \overline{[\lambda_{W=1}]}$ and can be
interpreted as defining two conditional weights: $\nu(T = m \mid W = 0) = 0.3$,
and $\nu(T \ne m \mid W = 0) = 1$, the former of which corresponds to a row in
the CPT of $T$ while the latter is artificially added as part of the encoding.
In our encoding of Bayesian networks, it is always the case that, in each weight
clause, either both weights sum to one, or the second weight is equal to one.
Finally, note that (without any additional restrictions), the measure induced by
these weight functions is not probabilistic (i.e., $\mu(\top)$ may not be equal
to one).

\subsection{Changes to ADDMC}

ADDMC constructs the \emph{Gaifman graph} \cite{gaifman1982local} of the input
CNF formula as an aid for the algorithm's heuristics. This graph has as vertices
the variables of the formula, and there is an edge between two variables $u$ and
$v$ if there is a clause in the formula that contains both $u$ and $v$. We
extend this definition to functions on Boolean algebras, i.e., the factors of
$\phi$. For any pair of distinct variables $u, v \in U$, we draw an edge between
them in the Gaifman graph if there is a function $\alpha\colon 2^X \to
\mathbb{R}_{\ge 0}$ that is a factor of $\phi$ such that $u \in X$ and $v \in
X$. For instance, a factor such as $\mathrm{CPT}_X$ will enable edges between
all distinct pairs of variables in $\mathcal{E}^*(X)$.

Even though the function $\phi$ produced by \cref{alg:encoding} is constructed
to have $2^U$ as its domain, sometimes the domain is effectively reduced to
$2^V$ for some $V \subset U$ by the ADD manipulation algorithms that optimise
the ADD representation of a function. For a simple example, consider $\alpha:
2^{\{a\}} \to \mathbb{R}_{\ge 0}$ defined as $\alpha(\{a\}) = \alpha(\emptyset)
= 0.5$. Then $\alpha$ can be reduced to $\alpha'\colon 2^{\emptyset} \to
\mathbb{R}_{\ge 0}$ defined as $\alpha'(\emptyset) = 0.5$. To compensate for
these reductions, for the original WMC format with a weight function $w\colon U
\cup \{ \neg u \mid u \in U \} \to \mathbb{R}_{\ge 0}$, ADDMC would multiply its
computed answer by $\prod_{u \in U \setminus V} w(u) + w(\neg u)$. With the new
WMC format, we instead multiply the answer by $2^{|U \setminus V|}$. Each
`excluded' variable $u \in U \setminus V$ satisfies two properties:
all weights associated with $u$ are equal to $0.5$ (otherwise the corresponding
CPT would depend on $u$, and $u$ would not be excluded), and all other CPTs are
independent of $u$ (or they may have a trivial dependence, where the probability
stays the same if $u$ is replaced with its complement).
Thus, the CPT that corresponds to $u$ still multiplies every model by $0.5$, but
the number of models being considered by the ADDMC is halved. To correct for
this, we multiply the final answer by two for every $u \in U \setminus V$.

\section{Experimental Comparison} \label{sec:experiments}

\begin{figure}
  \centering
  \begin{minipage}{0.59\textwidth}
    \centering
    \input{cumulative.tex}%
    \captionof{figure}{Cumulative number of instances solved by ADDMC over time
      using each encoding}
  \end{minipage}
  \begin{minipage}{0.39\textwidth}
    \centering
    \captionof{table}{The numbers of instances solved by ADDMC with each
      encoding (uniquely, faster than with others, and in total) under the
      described time and memory constraints (out of 1216 instances)}%
    \begin{tabular}{lrrr}
      \toprule
      Encoding & Unique & Fastest & Total \\
      \midrule
      \texttt{cd05} & 2 & 3 & 372 \\
      \texttt{cd06} & 0 & 1 & 351 \\
      \texttt{d02} & 41 & 99 & 726 \\
      \texttt{db21} & 228 & 871 & 901 \\
      \texttt{sbk05} & 6 & 36 & 687 \\
      \bottomrule
    \end{tabular}
  \end{minipage}
\end{figure}

\begin{figure}
  \centering
  \input{scatter.tex}%
  \caption{ADDMC inference time using \texttt{db21} compared to \texttt{d02}
    (left) and \texttt{sbk05} (right) on an instance-by-instance basis across
    all data sets.}
\end{figure}

\todo[inline]{Write an introductory paragraph.}

\begin{itemize}
\item We chose not to measure or compare encoding time because of different
  languages of implementation and the fact that our encoding algorithm (i.e.,
  the process that converts a Bayesian network into a textual encoding such as
  in \cref{sec:textual_representation}) is linear in the total number of CPT
  rows and so is unlikely to be slower than, e.g., \texttt{cd06}, which relies
  on solving $\NP$-complete problems as part of the encoding process
  \cite{DBLP:conf/sat/ChaviraD06}.
\item Whenever a Bayesian network comes with an evidence file, we compute the
  probability of evidence. Otherwise, let $X$ denote the last-mentioned vertex
  in the Bayesian network. If $\mathsf{true}$ is a valid value of $X$, we
  compute the marginal probability of $X = \mathsf{true}$. Otherwise, we pick
  the value of $X$ which is listed first and calculate its marginal probability.
\item The experiments were run on Intel Xeon Gold 6138 processor with an
  \SI{8}{\giga\byte} memory limit.
\item For all other encodings, we use their implementation in
  Ace~3.0\footnote{\url{http://reasoning.cs.ucla.edu/ace/}} with
  \texttt{-encodeOnly} and \texttt{-noEclause} flags. However, Ace was not used
  to encode evidence, as preliminary experiments revealed that the
  evidence-encoding implementation contains bugs that can lead to incorrect
  answers or a Java exception being thrown on some instances of the data set
  (and the source code is not publicly available). Instead, we simply list all
  the evidence as additional clauses in the encoding (regardless of which
  encoding is used).
\item Note that \texttt{cd05} and \texttt{cd06} purposefully produce overly
  relaxed encodings that contain extra models and thus yield incorrect
  probabilities \cite{DBLP:conf/ijcai/ChaviraD05,DBLP:conf/sat/ChaviraD06}.
  These additional models are supposed to be filtered out during circuit
  compilation \cite{DBLP:conf/ijcai/ChaviraD05}, but this is not easily
  achievable with ADDMC. Nonetheless, we include both encodings in our timing
  experiments.
\end{itemize}

\paragraph{Data.} For experiments, we use the Bayesian networks available
with Ace and
Cachet\footnote{\url{https://www.cs.rochester.edu/u/kautz/Cachet/}}, most of
which happen to be binary. We classify them into the following seven categories:
\begin{itemize*}
\item DQMR and
\item Grid networks as described by Sang et al. \cite{DBLP:conf/aaai/SangBK05},
\item Friends and Smokers,
\item Mastermind, and
\item Random Blocks from the work of Chavira et al.
  \cite{DBLP:journals/ijar/ChaviraDJ06},
\item remaining binary Bayesian networks that include Plan Recognition
  \cite{DBLP:conf/aaai/SangBK05}, Students and Professors
  \cite{DBLP:journals/ijar/ChaviraDJ06}, and \texttt{tcc4f}, and
\item non-binary classic Bayesian networks (\texttt{alarm}, \texttt{diabetes},
  \texttt{hailfinder}, \texttt{mildew}, \texttt{munin1}--\texttt{4},
  \texttt{pathfinder}, \texttt{pigs}, \texttt{water}).
\end{itemize*}

\paragraph{Observations.}
\begin{itemize}
\item The order of the encodings from best to worst is exactly the opposite of
  that in the previous literature.
\item Our encoding is particularly promising on instances of Grid networks.
  They are set up so that the entire Bayesian network is relevant to the
  query.
\item However, \texttt{db21} struggles with instances from Mastermind and
  Random Blocks domains (but so does \texttt{sbk05}). We conjecture that this
  is so either because the particular structure of these problems confuse the
  heuristics used by ADDMC or because these instances allow for significant
  simplifications that are exploited by $\mathtt{d02}$ but not
  $\mathtt{db21}$.
\item An observation from the cumulative plot: \texttt{db21} solves the same
  number of instances in \SI{6.374}{\second} as \texttt{d02} does in
  \SI{1000}{\second}.
\end{itemize}

\begin{table}
  \centering
  \caption{Asymptotic upper bounds on the numbers of variables and clauses/ADDs
    for each encoding}
  \label{tbl:asymptotes}
  \begin{tabular}{lcc}
    \toprule
    Encoding(s) & Variables & Clauses/ADDs \\
    \midrule
    \texttt{cd05}, \texttt{cd06}, \texttt{sbk05} & $\mathcal{O}(nv^{d+1})$ & $\mathcal{O}(nv^{d+1})$ \\
    \texttt{d02} & $\mathcal{O}(nv^{d+1})$ & $\mathcal{O}(ndv^{d+1})$ \\
    \texttt{db21} & $\mathcal{O}(nv)$ & $\mathcal{O}(nv^2)$ \\
    \bottomrule
  \end{tabular}
\end{table}

\paragraph{Explaining the performance benefits.} Let $n = |\mathcal{V}|$ be the
number of vertices in the Bayesian network, $d = \max_{X \in \mathcal{V}}
|\mathrm{pa}(X)|$ the maximum in-degree (i.e., number of parents), and $v =
\max_{X \in \mathcal{V}} |\im X|$ the maximum number of values per variable.
\Cref{tbl:asymptotes} shows how \texttt{db21} has both fewer variables and fewer
ADDs than any other encoding. However, note that these are upper bounds and most
encodings (including \texttt{db21}) can be smaller in certain situations (e.g.,
with binary random variables or when a CPT has repeating probabilities). We
equate clauses and ADDs (more specifically, factors of the function $\phi$ from
\cref{alg:encoding}) here because ADDMC interprets each clause of any WMC
encoding as a multiplicative factor of the ADD that represents the entire WMC
instance \cite{DBLP:conf/aaai/DudekPV20}. For literal-weight encodings, each
weight is also a factor, but that has no effect on the asymptotic bounds in the
table.

% While some ADDs may be larger as a result, experimental evidence still shows
% significant performance benefits.

\section{Conclusions and Future Work}

\begin{itemize}
\item Bayesian networks and ADDMC are only particular examples. This should also
  work with Cachet \cite{DBLP:conf/sat/SangBBKP04}.
  \begin{itemize}
  \item Potential criticism may be that this doesn't allow us to use SAT-based
    techniques for probabilistic inference. However, they can still be used for
    a significant part of the encoding.
    \begin{itemize}
    \item Zero-probability weights and one-probability weights can be
      interpreted as logical clauses. This doesn't affect ADDMC but could be
      useful for other solvers.
    \end{itemize}
  \end{itemize}
\item Extra benefit: one does not need to come up with a way to turn some
  probability distribution to into a fully independent one.
\item Important future work: replacing ADDs with
  AADDs \cite{DBLP:conf/ijcai/SannerM05} is likely to bring performance
  benefits.
  Other extensions:
  \begin{itemize}
  \item FOADDs can represent first order statements;
  \item XADDs can replace WMI for continuous variables;
  \item ADDs with intervals can do approximations.
  \end{itemize}
\item Filtering out ADDs that have nothing to do with the answer helps
  tremendously, but I'm purposefully not doing that.
\item Other things can be compiled into WMC, e.g., probabilistic programs
  \cite{DBLP:journals/corr/abs-2005-09089}, ProbLog
  \cite{DBLP:conf/uai/FierensBTGR11}.
\end{itemize}

\paragraph{Acknowledgements.} This work has made use of the resources provided
by the Edinburgh Compute and Data Facility (ECDF)
(\url{http://www.ecdf.ed.ac.uk/}).

\bibliographystyle{plain}
\bibliography{paper}

\end{document}
