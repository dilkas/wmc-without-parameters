\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage[UKenglish]{babel}
\usepackage[UKenglish]{isodate}
\usepackage{fullpage}
\usepackage{amsthm}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{mathtools}
\usepackage{hyperref}
\usepackage[ruled,vlined]{algorithm2e}
\usepackage[capitalise]{cleveref}
\usepackage{bm}
\usepackage{booktabs}
\usepackage[table]{xcolor}
\usepackage{tikz}
\usepackage[backgroundcolor=lightgray]{todonotes}
\usepackage{complexity}
\usepackage{soul}
\usepackage{subcaption}
\usepackage{multirow}
\usepackage[binary-units]{siunitx}

\newtheorem{theorem}{Theorem}
\newtheorem{observation}{Observation}
\newtheorem{lemma}{Lemma}
\newtheorem{proposition}{Proposition}
\newtheorem{corollary}{Corollary}
\newtheorem{conjecture}{Conjecture}
\theoremstyle{definition}
\newtheorem{definition}{Definition}
\newtheorem{example}{Example}
\theoremstyle{remark}
\newtheorem*{remark}{Remark}

\Crefname{property}{Property}{Properties}
\Crefname{condition}{Condition}{Conditions}
\creflabelformat{condition}{#2(#1)#3}

\DeclareMathOperator{\WMC}{WMC}
\DeclareMathOperator{\nWMC}{NWMC}
\DeclareMathOperator{\id}{id}
\DeclareMathOperator{\End}{End}
\DeclareMathOperator{\im}{im}

\usetikzlibrary{cd}
\usetikzlibrary{bayesnet}
\usetikzlibrary{calc}

\tikzset{
  Subset/.style={
    draw=none,
    every to/.append style={
      edge node={node [sloped, allow upside down, auto=false]{$\subset$}}}
  }
}

\title{Weighted Model Counting with Conditional Measures}
\author{Paulius Dilkas}

\begin{document}
\maketitle

\section{Introduction}

\begin{itemize}
\item The Main Narrative
  \begin{enumerate}
  \item When weights are defined on literals, the measure on the free BA is
    fully independent.
  \item This means that the BA itself must be larger (i.e., have additional
    `meaningless' literals) to turn any probability distribution into an
    independent one.
  \item We show how we can define conditional weights on literals, allowing us
    to encode any probability distribution into a Boolean algebra that's not
    necessarily independent and thus can be smaller.
  \item We demonstrate a specific example of this by presenting a new way to
    encode Bayesian networks into instances of WMC and adapting a WMC algorithm
    (ADDMC) to run on the new format.
  \item We show that this results in significantly faster inference.
  \item We show that our encoding results in asymptotically fewer literals and
    fewer ADDs, and thus a simpler problem.
  \item (Maybe) we experimentally demonstrate a phase transition based on the
    number of variables per ADD.
  \end{enumerate}
\item Potential criticism may be that this is a step backwards and doesn't allow
  us to use SAT-based techniques for probabilistic inference. However, they can
  still be used for the 'theory+query' part.
  \begin{itemize}
  \item Zero-probability weights and one-probability weights can be interpreted
    as logical clauses. This doesn't affect ADDMC but could be useful for other
    solvers.
  \end{itemize}
\item[F] What are the main claims, what are the main takeaways, intuitive [???]
  of theorems to follow. To do this, we appeal to algebraic constructions to
  define the main concepts for introducing measures on Boolean algebras.
\item
  Algorithms\footnote{\url{http://beyondnp.org/pages/solvers/model-counters-exact/}}
  \begin{itemize}
  \item ADDMC \cite{DBLP:conf/aaai/DudekPV20} (rediscovered the
    multiplicativity of BAs in different words) (with optimal settings)
  \item Cachet \cite{DBLP:conf/sat/SangBBKP04}
  \item c2d \cite{DBLP:conf/ecai/Darwiche04}
  \item d4 \cite{DBLP:conf/ijcai/LagniezM17} (closed source, boo!)
  \item miniC2D  \cite{DBLP:conf/ijcai/OztokD15}
  \end{itemize}
\item Notable previous/related work
  \begin{itemize}
  \item Hailperin's approach to probability logic
    \cite{DBLP:journals/ndjfl/Hailperin84}
  \item Nilsson's (somewhat successful) probabilistic logic
    \cite{DBLP:journals/ai/Nilsson86,DBLP:journals/ai/Nilsson93}
  \item Logical induction: a big paper with a good overview of previous attempts
    to assign probabilities to logical sentences in a sensible way
    \cite{DBLP:journals/eccc/GarrabrantBCST16}
  \item Measures on Boolean algebras
    \begin{itemize}
    \item On possibility and probability measures in finite Boolean algebras
      \cite{DBLP:journals/soco/CastineiraCT02}
    \item Representation of conditional probability measures
      \cite{krauss1968representation}
    \end{itemize}
  \end{itemize}
\item Intuitively, a measure is just like a probability, except it's in
  $\mathbb{R}_{\ge 0}$ instead of $[0, 1]$.
\item Somewhere: explain things line CNF, CPT, etc.
\end{itemize}

\section{Boolean Algebras and Power Sets}

\begin{table}
  \centering
  \caption{A comparison of Boolean-algebraic (BA) and set-theoretic (ST)
    concepts for $2^X$ for some set $X$}
  \label{tbl:notation}
  \begin{tabular}{lccl}
    \toprule
    BA name & BA symbol & ST symbol & ST name \\
    \midrule
    bottom & $\bot$ & $\emptyset$ & empty set \\
    top & $\top$ & $X$ & \\
    meet, and & $\land$ & $\cap$ & intersection \\
    join, or & $\lor$ & $\cup$ & union \\
    complement, not & $\neg$ & $^c$ & complement \\
            & $\le$ & $\subseteq$ & subset relation, set inclusion \\
    atom & & & singleton, unit set \\
    \bottomrule
  \end{tabular}
\end{table}

Let $X$ be a set and let $2^X$ be its power set. We can equivalently interpret
$2^X$ as a Boolean algebra. See \cref{tbl:notation} for a summary of the
differences in terminology and notation. We will use both.

\subsection{The Space of Functions on Boolean Algebras}

\begin{definition}[Operations on functions]
  Let $A\colon 2^X \to \mathbb{R}_{\ge 0}$ and $B\colon 2^Y \to \mathbb{R}_{\ge
    0}$ be arbitrary functions, $\alpha \in \mathbb{R}_{\ge 0}$, and $x \in X$.
  We define the following operations:
  \begin{description}
  \item[Addition:] $A+B$ is a function $A+B\colon 2^{X \cup Y} \to
    \mathbb{R}_{\ge 0}$ such that
    \[
      (A+B)(\tau) = A(\tau \cap X) + B(\tau \cap Y)
    \]
    for all $\tau \in 2^{X \cup Y}$.
  \item[Inverse:] $\overline{A}$ is a function $\overline{A}\colon 2^X \to
    \mathbb{R}_{\ge 0}$ such that
    \[
      \overline{A}(\tau) = 1 - A(\tau)
    \]
    for all $\tau \in 2^X$.
  \item[Multiplication:] $A \cdot B$ is a function $A \cdot B\colon 2^{X \cup Y}
    \to \mathbb{R}_{\ge 0}$ such that
    \[
      (A \cdot B)(\tau) = A(\tau \cap X) \cdot B(\tau \cap Y)
    \]
    for all $\tau \in 2^{X \cup Y}$.
  \item[Scalar multiplication:] $\alpha A$ is a function $\alpha A\colon 2^X \to
    \mathbb{R}_{\ge 0}$ such that
    \[
      (\alpha A)(\tau) = \alpha \cdot A(\tau)
    \]
    for all $\tau \in 2^X$.
  \item[Projection:] $\exists_xA$ is a function $\exists_xA\colon 2^{X \setminus
      \{ x \}} \to \mathbb{R}_{\ge 0}$ such that
    \[
      (\exists_xA)(\tau) = A(\tau) + A(\tau \cup \{ x \})
    \]
    for all $\tau \in 2^{X \setminus \{x \}}$.
  \end{description}
\end{definition}

\begin{observation}
  Let $U$ be a set, and $\mathcal{V} = \{ A\colon 2^X \to \mathbb{R}_{\ge 0}
  \mid X \subseteq U \}$. Then $\mathcal{V}$ is a semi-vector space with three
  additional operations: inverse, (non-scalar) multiplication, and projection.
  Specifically, note that both addition and multiplication are both associative
  and commutative.
\end{observation}

\begin{definition}[Special functions]
  \phantom{}
  \begin{itemize}
  \item unit $1\colon 2^\emptyset \to \mathbb{R}_{\ge 0}$, $1(\emptyset) = 1$.
  \item zero $0\colon 2^\emptyset \to \mathbb{R}_{\ge 0}$, $0(\emptyset) = 0$.
  \item logical constant $[a]\colon 2^{\{a\}} \to \mathbb{R}_{\ge 0}$,
    $[a](\emptyset) = 0$, $[a](\{a\}) = 1$.
  \end{itemize}
\end{definition}

Henceforth, for any function $A\colon 2^X \to \mathbb{R}_{\ge 0}$ and any set
$\tau$, we will write $A(\tau)$ to mean $A(\tau \cap X)$.

\section{Weighted Model Counting as a Measure}

\begin{definition}[Measures and weight functions] \label{def:wmc}
  Let $U$ be a set.
  \begin{itemize}
  \item A \emph{measure} is a function $M\colon 2^{2^U} \to \mathbb{R}_{\ge 0}$
    such that $M(\bot) = 0$ and
    \[
      M(a \lor b) = M(a) + M(b)
    \]
    for all $a, b \in 2^{2^U}$ whenever $a \land b = \bot$.
  \item A \emph{weight function} is a function $W\colon 2^U \to \mathbb{R}_{\ge
      0}$.
  \item A weight function is \emph{factored} if $W = \prod_{x \in U} W_x$ for
    some functions $W_x\colon 2^{\{x\}} \to \mathbb{R}_{\ge 0}$, $x \in U$.
  \item We say that a weight function $W\colon 2^U \to \mathbb{R}_{\ge 0}$
    \emph{induces} a measure $M_W\colon 2^{2^U} \to \mathbb{R}_{\ge 0}$ if
    \begin{equation} \label{eq:induced_measure}
      M_W(x) = \sum_{\{u\} \le x} W(u).
    \end{equation}
  \item A measure $M\colon 2^{2^U} \to \mathbb{R}_{\ge 0}$ is
    \emph{factorable} if there exists a factored weight function $W\colon 2^U
    \to \mathbb{R}_{\ge 0}$ that induces $M$.
  \end{itemize}
\end{definition}

\begin{lemma} \label{prop:measure}
  The function $M_W$, as defined by \cref{eq:induced_measure}, is a measure.
\end{lemma}
\begin{proof}
  Note that $M_W(\bot) = 0$ since there are no atoms below $\bot$. Let $a, b \in
  2^{2^{U}}$ be such that $a \land b = \bot$. By elementary properties of
  Boolean algebras, all atoms below $a \lor b$ are either below $a$ or below
  $b$. Moreover, none of them can be below both $a$ and $b$ because then they
  would have to be below $a \land b = \bot$. Thus
  \[
    M_W(a \lor b) = \sum_{\{u\} \le a \lor b} W(u) = \sum_{\{u\} \le a} W(u) +
    \sum_{\{u\} \le b} = M_W(a) + M_W(b)
  \]
  as required.
\end{proof}

In this formulation, the process of calculating the value of $M_W(x)$ for some
$x \in 2^{2^U}$ with a given definition of $W$ is known as \emph{weighted model
  counting}.

\subsection{Relation to the Classical (Logic-Based) View of WMC}

\todo[inline]{Would it be more accurate to replace the word `model' with
  `interpretation' in most situations?}

\begin{table}
  \caption{An overview of notation for a logic defined over two atoms. The
    elements in both columns are listed in the same order.}
  \label{tbl:notation_example}
  \centering
  \begin{tabular}{lcc}
    \toprule
    Name in logic & Set-theoretic notation & Boolean-algebraic notation \\
    \midrule
    Atoms (elements of $U$) & $a, b$ & $a, b$ \\
    \rowcolor{gray!10} Models (elements of $2^U$) & $\emptyset, \{a\}, \{b\}, \{a, b\}$ & $\neg a \land \neg b, a \land \neg b, \neg a \land b, a \land b$ \\
    & $\{ \emptyset, \{a\}, \{b\}, \{a, b\} \}$ & $\top$ \\
    & $\{ \emptyset, \{a\}, \{b\} \}, \{ \emptyset, \{a\}, \{a, b\} \}$ & $\neg a \lor \neg b, a \to b$ \\
    & $\{ \emptyset, \{b\}, \{a, b\} \}, \{ \{a\}, \{b\}, \{a, b\} \}$ & $b \to a, a \lor b$ \\
    & $\{\emptyset, \{a\}\}, \{\emptyset, \{b\}\}, \{\emptyset, \{a, b\}\}$ & $\neg b, \neg a, a \leftrightarrow b$ \\
    & $\{\{a\}, \{b\}\}, \{\{a\}, \{a, b\}\}, \{\{b\}, \{a, b\}\}$ & $(a \land \neg b) \lor (b \land \neg a), a, b$ \\
    & $\{\emptyset\}, \{\{a\}\}, \{\{b\}\}, \{\{a, b\}\}$ & $\neg a \land \neg b, a \land \neg b, \neg a \land b, a \land b$ \\
    \multirow{-7}{*}{Formulas (elements of $2^{2^U}$)} & $\emptyset$ & $\bot$ \\
    \bottomrule
  \end{tabular}
\end{table}

Let $\mathcal{L}$ be a propositional logic with atoms $U = \{ a, b \}$. See
\cref{tbl:notation_example} for an overview of the models and formulas in
$\mathcal{L}$ as well as their set-theoretic and Boolean-algebraic
representations. Note the differences in the meaning of the word `atom' between
the logical and the Boolean-theoretic interpretations. In the Boolean algebra
$2^{2^U}$, an atom is a set with a single element which corresponds to a model
of $\mathcal{L}$ (also known as an element of $2^U$); whereas an atom of
$\mathcal{L}$ is an atomic formula, i.e., an element of $U$. We will primarily
use the term `atom' to mean the former.

Let $w\colon \{ a, b, \neg a, \neg b \} \to \mathbb{R}_{\ge 0}$
be the \emph{weight function} defined by
\begin{align*}
  w(a) = 0.3, \quad w(\neg a) = 0.7, \quad w(b) = 0.2, \quad w(\neg b) = 0.8.
\end{align*}
Let $\Delta$ be a theory in $\mathcal{L}$ with a sole axiom $a$. Then
$\Delta$ has two models, i.e., $\{ a, b \}$ and $\{ a, \neg b \}$. The
\emph{weighted model count} (WMC) \cite{DBLP:journals/ai/ChaviraD08} of
$\Delta$ is then
\begin{equation} \label{eq:wmc_example}
  \mathrm{WMC}(\Delta) = \sum_{\omega \models \Delta} \prod_{\omega \models l} w(l) = w(a)w(b) + w(a)w(\neg b) = 0.3.
\end{equation}

Alternatively, we can define $W_a\colon 2^{\{a\}} \to \mathbb{R}_{\ge 0}$ as
\[
  W_a(\{ a \}) = 0.3, \quad W_a(\emptyset) = 0.7
\]
and $W_b\colon 2^{\{q\}} \to \mathbb{R}_{\ge 0}$ as
\[
  W_b(\{ b \}) = 0.2, \quad W_b(\emptyset) = 0.8.
\]
Let $M$ be the measure on $2^{2^U}$ induced by $W = W_a \cdot W_b$. Then,
equivalently to \cref{eq:wmc_example}, we can write
\[
  M(a) = W(\{ a, b \}) + W(\{ a \}) = W_a(\{a\})W_b(\{b\}) +
  W_a(\{a\})W_b(\emptyset) = 0.3.
\]

Given the theory $\Delta$, we can also compute the probability of a query $b$ as
\cite{DBLP:conf/uai/Belle17,DBLP:conf/aaai/SangBK05}
\[
  \Pr_{\Delta, w}(b) = \frac{\WMC(\Delta \land b)}{\WMC(\Delta)}.
\]
The same thing can be accomplished using the algebraic formulation, with $M$
replacing $\mathrm{WMC}$.

In the rest of the paper, for any set $U$, we will use set-theoretic notation
for $2^U$ and Boolean-algebraic notation for $2^{2^U}$, except for (Boolean)
atoms in $2^{2^U}$ that are denoted as $\{x\}$ for some model $x \in 2^U$.

\section{Limitations of Factorable Measures}

\todo[inline,caption={}]{
  \begin{itemize}
  \item[F] Give a concrete example of something impossible to represent
    using WMC.
  \item[F] Can you say something here about factorized vs non-factorized weight
    function definitions? That is, factorized is when $w$ maps literals to
    $R_{\ge 0}$, non-factorized is when $w$ maps models to $R_{\ge 0}$ and
    \begin{itemize}
    \item come up with nice example when non-factorized weights are intuitive;
    \end{itemize}
  \end{itemize}
}

\begin{lemma} \label{lemma:before_theorem}
  For any measure $M\colon 2^{2^U} \to \mathbb{R}_{\ge 0}$ and elements $a, b
  \in 2^{2^U}$,
  \begin{equation} \label{eq:to_prove}
    M(a \land b) = M(a)M(b)
  \end{equation}
  if and only if
  \begin{equation} \label{eq:to_prove2}
    M(a \land b) \cdot M(\neg a \land \neg b) = M(a \land \neg b)
    \cdot M(\neg a \land b).
  \end{equation}
\end{lemma}
\begin{proof}
  First, note that $a = (a \land b) \lor (a \land \neg b)$ and $(a \land b)
  \land (a \land \neg b) = 0$, so, by properties of a measure,
  \begin{equation} \label{eq:temp}
    M(a) = M(a \land b) + M(a \land \neg b).
  \end{equation}
  Applying \cref{eq:temp} and the equivalent expression for $M(b)$ allows us
  to rewrite \cref{eq:to_prove} as
  \[
    M(a \land b) = [M(a \land b) + M(a \land \neg b)][M(a \land b) + M(\neg a
    \land b)]
  \]
  which is equivalent to
  \begin{equation} \label{eq:temp6}
    M(a \land b)[1 - M(a \land b) - M(a \land \neg b) - M(\neg a \land b)] = M(a
    \land \neg b)M(\neg a \land b).
  \end{equation}
  Since $a \land b$, $a \land \neg b$, $\neg a \land b$, $\neg a \land \neg b$
  are pairwise disjoint and their supremum is $1$,
  \[
    M(a \land b) + M(a \land \neg b) + M(\neg a \land b) + M(\neg a \land \neg
    b) = 1,
  \]
  and this allows us to rewrite \cref{eq:temp6} into \cref{eq:to_prove2}. As all
  transformations are invertible, the two expressions are equivalent.
\end{proof}

\begin{theorem}
  A measure $M\colon 2^{2^U} \to \mathbb{R}_{\ge 0}$ is factorable if and only
  if
  \begin{equation} \label{eq:wmccondition}
  M(u \land v) = M(u)M(v)
  \end{equation}
  for all distinct $u, v \in U \cup \{ \neg w \mid w \in U \}$ such that $u \ne
  \neg v$.
\end{theorem}
\begin{proof}
  ($\Leftarrow$) For each $x \in U$, let $W_x\colon 2^{\{x\}} \to
  \mathbb{R}_{\ge 0}$ be defined by $W_x(\{ x \}) = M(x)$ and $W_x(\emptyset) =
  M(\neg x)$. Let $M_W$ be the measure induced by
  \[
    W = \prod_{x \in U} W_x.
  \]
  We will show that $M = M_W$. First, note that $M_w(\bot) = 0 = M(\bot)$ by
  \cref{def:wmc,prop:measure}. Second, let
  \[
    a = \bigwedge_{u \in U} a_u
  \]
  be an atom in $2^{2^U}$ such that $a_u \in \{ u, \neg u \}$ for all $u \in U$.
  Then
  \[
    M_W(a) = W(\{ a_u \mid u \in U \}) = \prod_{u \in U} W_u(\{a_u\}) = \prod_{u
      \in U} M(a_u) = M \left(\bigwedge_{u \in U} a_u \right) = M(a)
  \]

  Finally, note that if $M_W$ and
  $M$ agree on all atoms, then they must also agree on all other non-zero
  elements of the Boolean algebra.

  ($\Rightarrow$) For the other direction, we are given a factored weight
  function
  \[
    W = \prod_{x \in U} W_x,
  \]
  and we want to show that its induced measure $M_W$ satisfies
  \cref{eq:wmccondition}. Let $k_u, k_v \in U \cup \{ \neg w \mid w \in U \}$ be
  such that $k_u \in \{ u, \neg u \}$, $k_v \in \{ v, \neg v \}$, and $u \ne v$.
  We then want to show that
  \begin{equation} \label{eq:to_prove3}
    M_W(k_u \land k_v) = M_W(k_u)M_W(k_v)
  \end{equation}
  which is equivalent to
  \begin{equation} \label{eq:to_prove4}
    M_W(k_u \land k_v) \cdot M_W(\neg k_u \land \neg k_v) = M_W(k_u \land \neg k_v) \cdot M_w(\neg k_u \land k_v)
  \end{equation}
  by \cref{lemma:before_theorem}. Then
  \begin{align*}
    M_W(k_u \land k_v) &= \sum_{\{a\} \le k_u \land k_v} W(a) = \sum_{\{a\} \le k_u \land k_v} \prod_{x \in U} W_x(a) \\
                        &= \sum_{\{a\} \le k_u \land k_v} W_u(a_u)W_v(a_v) \prod_{x \in U \setminus \{ u, v \}} W_x(a) = \sum_{\{a\} \le k_u \land k_v} W_u(k_u)W_v(k_v) \prod_{x \in U \setminus \{ u, v \}} W_x(a) \\
    &= W_u(k_u)W_v(k_v) \sum_{\{a\} \le k_u \land k_v} \prod_{x \in U \setminus \{ u, v \}} W_x(a) = W_u(k_u)W_v(k_v)C,
  \end{align*}
  where
  \[
    C = \sum_{\{a\} \le k_u \land k_v} \prod_{x \in U \setminus \{ u, v \}} W_x(a),
  \]
  and is the same for $M_W(\neg k_u \land k_v)$, $M_W(k_u \land \neg k_v)$, and
  $M_W(\neg k_u \land \neg k_v)$ as well. But then \cref{eq:to_prove4} becomes
  \[
    W_u(k_u)W_v(k_v)W_u(\neg k_u)W_v(\neg k_v)C^2 = W_u(k_u)W_v(\neg
    k_v)W_u(\neg k_u)W_v(k_v)C^2
  \]
  which is trivially true.
\end{proof}

\section{Previous Work}

\subsection{Using WMC to Perform Inference on Bayesian Networks}

Hitherto, four techniques have been proposed for encoding Bayesian networks into
instances of WMC. We will identify them based on the initials of authors as well
as publications years: \texttt{d02} \cite{DBLP:conf/kr/Darwiche02},
\texttt{sbk05} \cite{DBLP:conf/aaai/SangBK05}, \texttt{cd05}
\cite{DBLP:conf/ijcai/ChaviraD05}, and \texttt{cd06}
\cite{DBLP:conf/sat/ChaviraD06}. Below we friendly summarise the observed
performance differences among them. Sang et al. \cite{DBLP:conf/aaai/SangBK05}
claim that \texttt{sbk05} is a smaller encoding than \texttt{d02} with respect
to both the number of clauses and the number of variables but provide no
experimental comparison. Chavira and Darwiche \cite{DBLP:conf/ijcai/ChaviraD05}
compare \texttt{cd05} with \texttt{d02} by measuring the time it takes to
compile either encoding into an arithmetic circuit (but do not measure inference
time). The more recent encoding \texttt{cd05} always compiles faster and results
in a smaller arithmetic circuit (as measured by the number of edges). In their
subsequent paper, the same authors perform two sets of experiments (that are
relevant to this summary) \cite{DBLP:conf/sat/ChaviraD06}. First, they compile
\texttt{cd05} and \texttt{cd06} encodings into d-DNNF (i.e., deterministic
decomposable negation normal form \cite{DBLP:journals/jancl/Darwiche01}),
measuring both compilation time and numbers of edges in the d-DNNF diagram. The
results are mostly in favour of \texttt{cd06}. Second, they compare the
inference time of \texttt{sbk05} run with Cachet \cite{DBLP:conf/sat/SangBBKP04}
with the compile times of \texttt{cd05} and \texttt{cd06}, but only on five
(types of) instances. In these experiments, \texttt{cd06} is always faster than
\texttt{cd05}, while the comparison with \texttt{sbk05} is mixed. Sometimes
\texttt{cd06} is orders of magnitude faster than \texttt{sbk05}, sometimes
slightly slower. The performance difference between \texttt{sbk05} and
\texttt{cd05} is even harder to judge: \texttt{sbk05} is better on three out of
five instances and worse on the remaining two. Based on this description, one
would expect \texttt{cd06} to be faster than both \texttt{cd05} and
\texttt{sbk05}, both of which should be faster than \texttt{d02}. The
experiments in \cref{sec:experiments}, however, strongly disagree with this
prediction, showing that the quality of an encoding depends strongly on the
underlying search algorithm or compilation technique.

\subsection{Algebraic Decision Diagrams and Their Use in Probabilistic
  Inference}

Cover:
\begin{itemize}
\item ADDs provide an efficient way to manipulate functions from BAs/power sets
  \cite{DBLP:journals/fmsd/BaharFGHMPS97}.
\item background reading
  \begin{itemize}
  \item Compiling Bayesian Networks Using Variable Elimination (Chavira and
    Darwiche) \cite{DBLP:conf/ijcai/ChaviraD07}
  \item On the Relationship between Sum-Product Networks and Bayesian Networks
    \cite{DBLP:conf/icml/ZhaoMP15}
  \item Using ROBDDs for Inference in Bayesian Networks with Troubleshooting as
    an Example \cite{DBLP:conf/uai/NielsenWJK00}
  \item SPUDD: Stochastic Planning using Decision Diagrams
    \cite{DBLP:conf/uai/HoeySHB99}
  \end{itemize}
\end{itemize}

\section{Encoding Bayesian Networks Using Conditional Weights}

\begin{figure}
  \centering
  \begin{subfigure}{0.2\textwidth}
    \centering
    \begin{tikzpicture}[edge from parent/.style={draw,-latex}]
      \node[draw,circle] {$W$}
      child {node[draw,circle] {$F$}}
      child {node[draw,circle] {$T$}};
    \end{tikzpicture}
  \end{subfigure}%
  \begin{subfigure}{0.8\textwidth}
    \centering
    \begin{tabular}[t]{cc}
      \toprule
      $w$ & $\Pr(W = w)$ \\
      \midrule
      1 & 0.5 \\
      0 & 0.5 \\
      \bottomrule
    \end{tabular}
    \begin{tabular}[t]{ccc}
      \toprule
      $w$ & $f$ & $\Pr(F = f \mid W = w)$ \\
      \midrule
      1 & 1 & 0.6 \\
      1 & 0 & 0.4 \\
      0 & 1 & 0.1 \\
      0 & 0 & 0.9 \\
      \bottomrule
    \end{tabular}
    \begin{tabular}[t]{ccc}
      \toprule
      $w$ & $t$ & $\Pr(T = t \mid W = w)$ \\
      \midrule
      1 & $l$ & 0.2 \\
      1 & $m$ & 0.4 \\
      1 & $h$ & 0.4 \\
      0 & $l$ & 0.6 \\
      0 & $m$ & 0.3 \\
      0 & $h$ & 0.1 \\
      \bottomrule
    \end{tabular}
  \end{subfigure}
  \caption{An example Bayesian network with its CPTs}
  \label{fig:example_bn}
\end{figure}

Let $V$ denote the set of random variables in a Bayesian network. For any random
variable $X \in V$, let $\mathrm{pa}(X)$ denote the set of parents of $X$ and
$\im X$ denote the set of possible values. We will use the Bayesian network in
\cref{fig:example_bn} (loosely based on an example by Sang et al.
\cite{DBLP:conf/aaai/SangBK05}) as a running example throughout this section.
For the Bayesian network in \cref{fig:example_bn}, we have:
\begin{align*}
  V &= \{ W, F, T \}, \\
  \mathrm{pa}(W) &= \emptyset, \\
  \mathrm{pa}(F) &= \mathrm{pa}(T) = \{ W \}, \\
  \im W &= \im F = \{ 0, 1 \}, \\
  \im T &= \{ l, m, h \}.
\end{align*}

\begin{definition}[Indicator variables]
  Let $X \in V$ be a random variable. If $X$ is binary (i.e., $|\im X| = 2$), we
  can arbitrary identify one of the values as $1$ and the other one as $0$ (i.e,
  $\im X \cong \{ 0, 1 \}$). Then $X$ can be represented by a single
  \emph{indicator variable} $\lambda_{X=1}$. For notational simplicity, for any
  set $S$, whenever we write $\lambda_{X=0} \in S$ or $S = \{ \lambda_{X=0},
  \dots \}$, we actually mean $\lambda_{X=1} \not\in S$,

  On the other hand, if $X$ is not binary, we represent $X$ with $|\im X|$
  indicator variables, one for each value. We let
  \[
    E(X) = \begin{cases}
      \{ \lambda_{X=1} \} & \text{if } |\im X| = 2 \\
      \{ \lambda_{X=x} \mid x \in \im X \} & \text{otherwise.}
    \end{cases}
  \]
  denote the set of indicator variables for $X$ and
  \[
    E^*(X) = E(X) \cup \bigcup_{Y \in \mathrm{pa}(X)} E(Y).
  \]
  denote the set of indicator variables for $X$ and its parents in the Bayesian
  network. Finally, let
  \[
    U = \bigcup_{X \in V} E(X)
  \]
  denote the set of all indicator variables for all random variables in the
  Bayesian network.
\end{definition}

\todo[inline]{Come up with a less repetitive wording.}
For the Bayesian network in \cref{fig:example_bn}, we have:
\begin{align*}
  E(W) &= \{ \lambda_{W=1} \}, \\
  E(F) &= \{ \lambda_{F=1} \}, \\
  E(T) &= \{ \lambda_{T=l}, \lambda_{T=m}, \lambda_{T=h} \}, \\
  E^*(W) &= \{ \lambda_{W=1} \}, \\
  E^*(F) &= \{ \lambda_{F=1}, \lambda_{W=1} \}, \\
  E^*(T) &= \{ \lambda_{T=l}, \lambda_{T=m}, \lambda_{T=h}, \lambda_{W=1} \}.
\end{align*}

\begin{algorithm}
  \caption{Encoding a Bayesian network as a function $2^U \to \mathbb{R}_{\ge
      0}$}
  \label{alg:encoding}
  \KwData{a Bayesian network with vertices $V$ and probability distribution
    $\Pr$}
  \KwResult{a function $\phi\colon 2^U \to \mathbb{R}_{\ge 0}$}
  $\phi \gets 1$\;
  \For{$X \in V$}{
    \textit{let} $\mathrm{pa}(X) = \{ Y_1, \dots, Y_n \}$\;
    $\mathrm{CPT}_X \gets 0$\;
    \eIf{$|\im X| = 2$}{
      \For{$(y_1, \dots, y_n) \in \prod_{i = 1}^n \im Y_i$}{
        $p_1 \gets \Pr(X = 1 \mid Y_1 = y_1, \dots, Y_n = y_n)$\;
        $p_0 \gets \Pr(X \ne 1 \mid Y_1 = y_1, \dots, Y_n = y_n)$\;
        $\mathrm{CPT}_X \gets \mathrm{CPT}_X + p_1[\lambda_{X=1}] \cdot
        \prod_{i=1}^n [\lambda_{Y_i=y_i}] + p_0 \overline{[\lambda_{X=1}]} \cdot
        \prod_{i=1}^n [\lambda_{Y_i=y_i}]$\;
      }
    }{
      \textit{let} $\im X = \{ x_1, \dots, x_m \}$\;
      \For{$x \in \im X$ {\rm \textbf{and}} $(y_1, \dots, y_n) \in \prod_{i =
          1}^n \im Y_i$}{
        $p_x \gets \Pr(X = x \mid Y_1 = y_1, \dots, Y_n = y_n)$\;
        $\mathrm{CPT}_X \gets \mathrm{CPT}_X + p_x[\lambda_{X=x}] \cdot
        \prod_{i=1}^n [\lambda_{Y_i=y_i}] + \overline{[\lambda_{X=1}]} \cdot
        \prod_{i=1}^n [\lambda_{Y_i=y_i}]$\;
      }
      $\mathrm{CPT}_X \gets \mathrm{CPT}_X \cdot \left( \sum_{i=1}^m [\lambda_{X
          = x_i}] \right) \cdot \prod_{i=1}^m \prod_{j=i+1}^m
      (\overline{[\lambda_{X = x_i}]} + \overline{[\lambda_{X = x_j}]})$\;
    }
    $\phi \gets \phi \cdot \mathrm{CPT}_X$\;
  }
  \Return{$\phi$}\;
\end{algorithm}

\todo[inline]{Ditto.}
For the Bayesian network in \cref{fig:example_bn}, we have:
\begin{align*}
  \mathrm{CPT_W} &= 0.5[\lambda_{W=1}]+0.5\overline{[\lambda_{W=1}]} = 0.5 \cdot 1, \\
  \mathrm{CPT_F} &= 0.6[\lambda_{F=1}] \cdot [\lambda_{W=1}] + 0.4[\lambda_{F=0}] \cdot [\lambda_{W=1}] + 0.1[\lambda_{F=1}] \cdot [\lambda_{W=0}] + 0.9[\lambda_{F=0}] \cdot [\lambda_{W=0}] \\
    &= 0.6[\lambda_{F=1}] \cdot [\lambda_{W=1}] + 0.4\overline{[\lambda_{F=1}]} \cdot [\lambda_{W=1}] + 0.1[\lambda_{F=1}] \cdot \overline{[\lambda_{W=1}]} + 0.9\overline{[\lambda_{F=1}]} \cdot \overline{[\lambda_{W=1}]}, \\
  \mathrm{CPT_T} &= ([\lambda_{T=l}] + [\lambda_{T=m}] + [\lambda_{T=h}]) \cdot (\overline{[\lambda_{T=l}]} + \overline{[\lambda_{T=m}]}) \cdot (\overline{[\lambda_{T=l}]} + \overline{[\lambda_{T=h}]}) \cdot (\overline{[\lambda_{T=m}]} + \overline{[\lambda_{T=h}]}) \cdot (\dots).
\end{align*}

\todo[inline]{Describe \cref{alg:encoding}}

\subsection{Proof of Correctness}

\begin{lemma} \label{lemma:cpt}
  Let $X \in V$ be a random variable with parents $\mathrm{pa}(X) = \{ Y_1,
  \dots, Y_n \}$. Then $\mathrm{CPT}_X\colon 2^{E^*(X)} \to \mathbb{R}_{\ge 0}$
  is such that for any $x \in \im X$ and $(y_1, \dots, y_n) \in \prod_{i=1}^n
  \im Y_i$,
  \[
    \mathrm{CPT}_X (\{ \lambda_{X=x} \} \cup \{ \lambda_{Y_i=y_i} \mid i = 1,
    \dots, n \}) = \Pr(X = x \mid Y_1 = y_1, \dots, Y_n = y_n).
  \]
\end{lemma}
\begin{proof}
  Let $\tau = \{ \lambda_{X=x} \} \cup \{ \lambda_{Y_i=y_i} \mid i = 1, \dots, n
  \}$. If $X$ is binary, then $\mathrm{CPT}_X$ is a sum of $2\prod_{i=1}^n |\im
  Y_i|$ terms, one for each possible assignment of values to variables $X, Y_1,
  \dots, Y_n$. Exactly one of these terms is nonzero when applied to $\tau$, and
  it is equal to $\Pr(X = x \mid Y_1 = y_1, \dots, Y_n = y_n)$ by definition.

  If $X$ is not binary, then
  \[
    \left( \sum_{i=1}^m [\lambda_{X = x_i}] \right)(\tau) = 1,
  \]
  and
  \[
    \left( \prod_{i=1}^m \prod_{j=i+1}^m (\overline{[\lambda_{X = x_i}]} +
      \overline{[\lambda_{X = x_j}]}) \right)(\tau) = 1,
  \]
  so, by a similar argument as before,
  \[
    \mathrm{CPT}_X(\tau) = \Pr(X = x \mid Y_1 = y_1, \dots, Y_n = y_n).
  \]
\end{proof}

\begin{proposition} \label{lemma:full_distribution}
  $\phi\colon 2^U \to \mathbb{R}_{\ge 0}$ represents the full probability
  distribution of the Bayesian network, i.e., if $V = \{ X_1, \dots, X_n\}$,
  then
  \[
    \phi(\tau) =
    \begin{cases}
      \Pr(X_1 = x_1, \dots, X_n = x_n) & \text{if } \tau = \{ \lambda_{X_i=x_i}
      \mid i = 1, \dots, n \} \text{ for some } (x_1, \dots, x_n) \in
      \prod_{i=1}^n \im X_i \\
      0 & \text{otherwise,}
    \end{cases}
  \]
  for all $\tau \in 2^U$.
\end{proposition}
\begin{proof}
  If $\tau = \{ \lambda_{X=v_X} \mid X \in V \}$ for some $(v_X)_{X \in V} \in
  \prod_{X \in V} \im X$, then
  \[
    \phi(\tau) = \prod_{X \in V} \Pr \left( X=v_X \;\middle|\; \bigwedge_{Y \in
        \mathrm{pa}(X)} Y=v_Y \right) = \Pr \left( \bigwedge_{X \in V} X=v_X
    \right)
  \]
  by \cref{lemma:cpt} and the definition of a Bayesian network. Otherwise there
  must be some non-binary random variable $X \in V$ such that $|E(X) \cap \tau|
  \ne 1$. If $E(X) \cap \tau = \emptyset$, then
  \[
    \left( \sum_{i=1}^m [\lambda_{X = x_i}] \right)(\tau) = 0,
  \]
  and so $\mathrm{CPT}_X(\tau) = 0$, and $\phi(\tau) = 0$. If $|E(X) \cap
  \tau| > 1$, then we must have two different values $x_1, x_2 \in \im X$ such
  that $\{\lambda_{X=x_1}, \lambda_{X=x_2} \} \subseteq \tau$ which means that
  \[
    (\overline{[\lambda_{X=x_1}]} + \overline{[\lambda_{X=x_2}]})(\tau) = 0,
  \]
  and so, again, $\mathrm{CPT}_X(\tau) = 0$, and $\phi(\tau) = 0$.
\end{proof}

\begin{theorem}
  Let $\phi\colon 2^U \to \mathbb{R}_{\ge 0}$ be a function generated by the
  algorithm. Then
  \[
    (\exists_U(\phi \cdot [\lambda_{X=x}]))(\emptyset) = \Pr(X = x).
  \]
\end{theorem}
\begin{proof}
  Let $V = \{ X, Y_1, \dots, Y_n \}$. Then
  \begin{align*}
    (\exists_U (\phi \cdot [\lambda_{X=x}]))(\emptyset) &= \sum_{\tau \in 2^U} (\phi \cdot [\lambda_{X=x}])(\tau) = \sum_{\lambda_{X=x} \in \tau \in 2^U} \phi(\tau) = \sum_{\lambda_{X=x} \in \tau \in 2^U} \left( \prod_{Y \in V} \mathrm{CPT}_Y \right)(\tau) \\
    &= \sum_{(y_1, \dots, y_n) \in \prod_{i=1}^n \im Y_i} \Pr(X = x, Y_1 = y_1, \dots, Y_n = y_n) = \Pr(X = x)
  \end{align*}
  by the following arguments:
  \begin{itemize}
  \item the proof of Theorem~1 in the ADDMC paper \cite{DBLP:conf/aaai/DudekPV20};
  \item if $\lambda_{X=x} \not\in \tau \in 2^U$, then $(\phi \cdot
    [\lambda_{X=x}])(\tau) = \phi(\tau) \cdot [\lambda_{X=x}](\tau \cap \{
    \lambda_{X=x} \}) = \phi(\tau) \cdot 0 = 0$;
  \item \cref{lemma:full_distribution};
  \item marginalisation of a probability distribution.
  \end{itemize}
\end{proof}

\subsection{Textual Representation}

The Bayesian network in \cref{fig:example_bn} can be represented in a textual
format as
\[
  \begin{array}{l r r l l}
    \lambda\sb{T=l} &\lambda\sb{T=m} &\lambda\sb{T=h} & &0 \\
                    &-\lambda\sb{T=l} &-\lambda\sb{T=m} & &0 \\
                    &-\lambda\sb{T=l} &-\lambda\sb{T=h} & &0 \\
                    &-\lambda\sb{T=m} &-\lambda\sb{T=h} & &0 \\
    w &\lambda\sb{W=1} & &0.5 &0.5 \\
    w &\lambda\sb{F=1} &\lambda\sb{W=1} &0.6 &0.4 \\
    w &\lambda\sb{F=1} &-\lambda\sb{W=1} &0.1 &0.9 \\
    w &\lambda\sb{T=l} &\lambda\sb{W=1} &0.2 &1 \\
    w &\lambda\sb{T=m} &\lambda\sb{W=1} &0.4 &1 \\
    w &\lambda\sb{T=h} &\lambda\sb{W=1} &0.4 &1 \\
    w &\lambda\sb{T=l} &\lambda\sb{W=0} &0.6 &1 \\
    w &\lambda\sb{T=m} &\lambda\sb{W=0} &0.3 &1 \\
    w &\lambda\sb{T=h} &\lambda\sb{W=0} &0.1 &1
  \end{array}
\]
with each $\lambda$ replaced with a unique positive integer. This format is
based on the format used by the Cachet solver \cite{DBLP:conf/sat/SangBBKP04}
to encode WMC problems, which extends the DIMACS format for CNF formulas with
weight clauses. Subsequently, we extend it in two ways:
\begin{itemize}
\item a single weight clause now supports an arbitrary number of literals,
\item and each weight clause has two probabilities instead of one (i.e., we no
  longer assume that $\Pr(v) + \Pr(\neg v) = 1$ for all variables $v \in U$).
\end{itemize}
The way we use this encoding, it is always the case that either both
probabilities sum to one, or the second probability (i.e., the probability for
the complement of the variable) is equal to one.

\subsection{Changes to ADDMC}

ADDMC constructs the \emph{Gaifman graph} \cite{gaifman1982local} of the input
CNF formula as an aid for the algorithm's heuristics. This graph has as vertices
the variables of the formula, and there is an edge between two variables $u$ and
$v$ if there is a clause in the formula that contains both $u$ and $v$. We
extend this definition to functions on Boolean algebras, i.e., the factors of
$\phi$. For any pair of distinct variables $u, v \in U$, we draw an edge between
them in the Gaifman graph if there is a function $A\colon 2^X \to
\mathbb{R}_{\ge 0}$ that is a factor of $\phi$ such that $u \in X$ and $v \in
X$. For instance, a factor such as $\mathrm{CPT}_X$ will enable edges between
all distinct pairs of variables in $E^*(X)$.

\todo[inline]{This needs rewriting.}
ADDMC multiplies the final answer by $w(u)+w(-u)$ for every $u \in U$ that was
`simplified out' of the ADD for $\phi$ and so is not featured in the $\exists_U$
projection. In our format, for every such $u \in U$, we multiply the answer by
two. Each such $u \in U$ must satisfy two properties:
\begin{itemize}
\item all the probabilities/weights associated with $u$ are equal to $0.5$
  (otherwise the corresponding CPT would depend on $u$),
\item and all other CPTs are independent of $u$ (or they may have a trivial
  dependence, where the probability stays the same if $u$ is replaced with its
  complement).
\end{itemize}
Thus, the CPT that corresponds to $u$ still multiplies every model by $0.5$, but
the number of models being considered by the ADDMC is halved. To correct for
this, we need to perform this multiplication.

\section{Experimental Comparison} \label{sec:experiments}

\begin{itemize}
\item We don't compare 'compile times' because our encoding time is linear, so
  we would easily beat everyone else.
\item When the Bayesian network has an evidence file, we compute the probability
  of evidence. Otherwise, let $X$ denote the last-mentioned node in the Bayesian
  network. If $\mathsf{true}$ is a valid value of $X$, we compute the marginal
  probability of $X = \mathsf{true}$. Otherwise, we pick the first value of $X$
  and calculate its marginal probability. This applies to the Grid data set (as
  intended) and also to two instances of Plan Reconstruction and roughly half of
  the instances from 2004-PGM that have empty evidence files.
\item After the experiments are finished, note the processor, memory per thread.
\item The experiments were run on Intel Xeon Gold 6138 processor with an
  \SI{8}{\giga\byte} memory limit.
\item I will call my encoding \texttt{db20}.
\item All other encodings are implemented in
  Ace~3.0\footnote{\url{http://reasoning.cs.ucla.edu/ace/}} and were compiled
  with \texttt{-encodeOnly} (i.e., don't compile the CNF into an AC) and
  \texttt{-noEclause} (i.e., only use standard syntax) flags.
\item We don't use Ace's evidence-encoding mechanism because it has multiple
  significant bugs (and the code is closed-source). Ace's implementation
  (depending on the exact encoding) is supposed to perform some simplifications
  when encoding evidence. We just do the trivial thing---no simplifications.
\item Datasets
  \begin{itemize}
  \item binary Bayesian networks from Sang et
    al.\footnote{\url{https://www.cs.rochester.edu/u/kautz/Cachet/}}
    \cite{DBLP:conf/aaai/SangBK05}
    \begin{itemize}
    \item Grid (networks) (ratio 75 means that 75\% of the nodes are
      deterministic),
    \item Plan recognition (problems),
    \item Deterministic quick medical reference (what do the numbers mean? the
      README doesn't say).
    \end{itemize}
  \item Bayesian networks available with Ace
    \begin{itemize}
    \item \texttt{2004-pgm} \cite{DBLP:journals/ijar/ChaviraDJ06} (binary)
    \item \texttt{2005-ijcai} \cite{DBLP:conf/ijcai/ChaviraD05}. The Genie/Smile
      files have their own citation data that I should probably extract. This is
      the only dataset that has some non-binary networks.
    \item \texttt{2006-ijar} \cite{DBLP:journals/ijar/ChaviraDJ06} (binary)
    \end{itemize}
  \end{itemize}
\item \texttt{cd05} relaxes the encoding so much that extra models become
  possible. They are supposed to be filtered out by the algorithm, but mine
  can't do that because it doesn't deal with models. Same for \texttt{cd06}
  because it's based on \texttt{cd05}.
\end{itemize}

\section{Explaining The Performance Benefits}

\begin{itemize}
\item \texttt{d02} has
  \[
    \sum_{X \in V} |\im X| + |\im X|\prod_{Y \in \mathrm{pa}(X)}|\im Y|
  \]
  variables and
  \[
    \sum_{X \in V} 1 + \binom{|\im X|}{2} + |\im X|(2 +
    |\mathrm{pa}(X)|)\prod_{Y \in \mathrm{pa}(X)} |\im Y|
  \]
  clauses (along with one ADD per variable to encode the weights).
\item \texttt{sbk05} is a bit harder to evaluate due to a handful of small
  optimisations in the encoding. Could find an upper bound anyway.
\item \texttt{db20} (my encoding) has
  \[
    \sum_{X \in V} |\im X|
  \]
  variables (less for binary) and
  \[
    \sum_{X \in V} |\im X| + 1 + \binom{|\im X|}{2}
  \]
  ADDs.
\item Let:
  \begin{itemize}
  \item $N = |V|$ (i.e., the number of nodes in the Bayesian network),
  \item $D = \max_{X \in V} |\mathrm{pa}(X)|$ (i.e., the maximum in-degree or
    the number of parents),
  \item $V = \max_{X \in V} |\im X|$ (i.e., the maximum number of values per
    variables).
  \end{itemize}
\item Then my encoding has $\mathcal{O}(NV)$ variables and $\mathcal{O}(NV^2)$
  ADDs while \texttt{d02} has $\mathcal{O}(NV^{D+1})$ variables and
  $\mathcal{O}(NDV^{D+1})$ ADDs.
\end{itemize}

\todo[inline]{Calculate numVariables/numClauses (or the other way around) for
  each instance and plot this ratio vs runtime (for each encoding, or at least
  mine and D02). The new CP paper kind of beat me to it...}

\section{Conclusion and Future Work}

\begin{itemize}
\item Bayesian networks and ADDMC are only particular examples. This should also
  work with Cachet.
\item Extra benefit: one does not need to come up with a way to turn some
  probability distribution to into a fully independent one.
\item Important future work: replacing ADDs with
  AADDs\footnote{\url{https://github.com/ssanner/dd-inference}}
  \cite{DBLP:conf/ijcai/SannerM05} is likely to bring performance benefits.
  Other extensions:
  \begin{itemize}
  \item FOADDs can represent first order statements;
  \item XADDs can replace WMI for continuous variables;
  \item ADDs with intervals can do approximations.
  \end{itemize}
\item Filtering out ADDs that have nothing to do with the answer helps
  tremendously, but I'm purposefully not doing that. Perhaps a heuristic could
  do the same thing?
\item Encodings for everything else
  \begin{itemize}
  \item probabilistic programs \cite{DBLP:journals/corr/abs-2005-09089}
  \item ProbLog \cite{DBLP:conf/uai/FierensBTGR11}
    \begin{itemize}
    \item For the ProbLog to WMC conversion, check out this guy:
      \url{https://users.ics.aalto.fi/ttj/}.
    \item proof-based \cite{DBLP:conf/iclp/MantadelisJ10}
    \item rule-based \cite{DBLP:conf/ecai/Janhunen04}
    \item For ground ProbLog, we can encode a program
\begin{verbatim}
p :: a :- b
q :: a :- c
\end{verbatim}
      into $P(a \mid b)=p$, $P(a \mid c)=q$ instead of having clauses $b
      \Rightarrow a$, $c \Rightarrow a$. Some logical structure is likely to
      remain.
    \end{itemize}
  \end{itemize}
\item Bayesian networks are often solved in a compile once, query many times
  fashion. This can be achieved using ADDMC by selecting a subset $S$ of
  variable we may want to query over and running ADDMC while excluding $S$ from
  variable elimination/projection/$\exists$.
\item More references
  \begin{itemize}
  \item Measures on/in Boolean algebras: Horn and Tarski
    \cite{horn1948measures}, Jech \cite{jech2017measures}
  \item On Boolean algebras and their role in analysis \cite{winkowska1996boolean}
  \item Infinite domains
    \begin{itemize}
    \item Markov Logic in Infinite Domains (Singla and Domingos)
      \cite{DBLP:conf/uai/SinglaD07}
    \item Objective Bayesian probabilistic logic (Williamson)
      \cite{DBLP:journals/jal/Williamson08}
    \item Unifying Logic and Probability (Russell)
      \cite{DBLP:journals/cacm/Russell15}
    \end{itemize}
  \item Logical induction \cite{DBLP:journals/eccc/GarrabrantBCST16}
  \item Quantum probabilistic logic programming \cite{balu2015quantum}
  \item WMC
    \begin{itemize}
    \item algebraic model counting \cite{DBLP:journals/japll/KimmigBR17}
    \item Explanation-Based Approximate Weighted Model Counting for
      Probabilistic Logics \cite{DBLP:conf/aaai/RenkensKBR14}
    \item OUWMC \cite{DBLP:conf/aaai/Belle17}
    \item Formula-Based Probabilistic Inference \cite{DBLP:conf/uai/GogateD10}
    \item Parallel Probabilistic Inference by WMC \cite{DBLP:conf/pgm/DalLL18}
    \item Semiring Programming \cite{DBLP:journals/corr/BelleR16}
    \item theoretical extension: WMC beyond two-variable logic
      \cite{DBLP:conf/lics/KuusistoL18}
    \item from weighted to unweighted model counting
      \cite{DBLP:conf/ijcai/ChakrabortyFMV15}
    \item theory behind WMC algorithms: solving \#\SAT{} and Bayesian inference
      with backtracking search \cite{DBLP:journals/jair/BacchusDP09}
    \end{itemize}
  \end{itemize}
\end{itemize}

\paragraph{Acknowledgements.} This work has made use of the resources provided
by the Edinburgh Compute and Data Facility (ECDF)
(\url{http://www.ecdf.ed.ac.uk/}).

\bibliographystyle{plain}
\bibliography{paper}

\end{document}
