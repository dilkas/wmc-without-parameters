\documentclass{uai2021} % for initial submission
% \documentclass[accepted]{uai2021} % after acceptance, for a revised
                                    % version; also before submission to
                                    % see how the non-anonymous paper
                                    % would look like
%% There is a class option to choose the math font
% \documentclass[mathfont=cm]{uai2021} % Computer Modern math instead of
                                       % ptmx, like default for UAI ≤2020
% \documentclass[mathfont=newtx]{uai2021} % newtx fonts (improves upon
                                          % ptmx; less tested, no support)
% NOTE: Only keep *one* line above as appropriate, as it will be replaced
%       automatically for papers to be published. Do not make any other
%       change above this note for an accepted version.

%% Choose your variant of English; be consistent
% \usepackage[american]{babel}
\usepackage[british]{babel}

%% Some suggested packages, as needed:
\usepackage{natbib} % has a nice set of citation styles and commands
    \bibliographystyle{plainnat}
    \renewcommand{\bibsection}{\subsubsection*{References}}
\usepackage{mathtools} % amsmath with fixes and additions
\usepackage[binary-units]{siunitx} % for proper typesetting of numbers and units
\usepackage{booktabs} % commands to create good-looking tables
\usepackage[table]{xcolor}
\usepackage{tikz} % nice language for creating drawings and diagrams

\usepackage{complexity}
\usepackage[capitalise]{cleveref}
\usepackage{multirow}
\usepackage{amsfonts}
\usepackage{amsthm}
\usepackage{thm-restate}
\usepackage{subcaption}
\usepackage[ruled,vlined]{algorithm2e}

\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
\theoremstyle{definition}
\newtheorem{definition}{Definition}
\newtheorem{example}{Example}

\DeclareMathOperator{\im}{im}

\makeatletter
\newcommand{\nosemic}{\renewcommand{\@endalgocfline}{\relax}}% Drop semi-colon ;
\newcommand{\dosemic}{\renewcommand{\@endalgocfline}{\algocf@endline}}% Reinstate semi-colon ;
\newcommand{\pushline}{\Indp}% Indent
\newcommand{\popline}{\Indm\dosemic}% Undent
\makeatother

\title{Weighted Model Counting with Conditional Weights for Bayesian Networks}

% The standard author block has changed for UAI 2021 to provide
% more space for long author lists and allow for complex affiliations
%
% All author information is authomatically removed by the class for the
% anonymous submission version of your paper, so you can already add your
% information below.
%
% Add authors in order of decreasing contribution
\author[1]{\href{mailto:Harry Q. Bovik <harryq@example.edu>?Subject=Your UAI 2021 paper}{Harry~Q.~Bovik}{}} % Lead author
\author[2]{Coauthor~One}
\author[1,2]{Coauthor~Two}
\author[3]{Further~Coauthor}
\author[1]{Further~Coauthor}
\author[3]{Further~Coauthor}
\author[3,1]{Further~Coauthor}
% Add affiliations after the authors
\affil[1]{%
    Computer Science Dept.\\
    Cranberry University\\
    Pittsburgh, Pennsylvania, USA
}
\affil[2]{%
    Affiliation\\
    Address\\
    …
}
\affil[3]{…}

\begin{document}
\maketitle

\begin{abstract}
  Weighted model counting (WMC) has emerged as the unifying inference mechanism
  across many (probabilistic) domains. Encoding an inference problem as an
  instance of WMC typically necessitates adding extra literals and clauses. This
  is partly so because the predominant definition of WMC assigns weights to
  models based on weights on literals, and this severely restricts what
  probability distributions can be represented. We develop a measure-theoretic
  perspective on WMC and propose a way to encode conditional weights on literals
  analogously to conditional probabilities. This representation can be as
  succinct as standard WMC with weights on literals but can also expand as
  needed to represent probability distributions with less structure. To
  demonstrate the performance benefits of conditional weights over the addition
  of extra literals, we develop a new WMC encoding for Bayesian networks and
  adapt a state-of-the-art WMC algorithm ADDMC to the new format. Our
  experiments show that the new encoding results in significantly faster
  inference on most benchmark instances. Furthermore, the same idea can be
  adapted to other WMC algorithms and other problem domains.
\end{abstract}

\section{Introduction}

Weighted model counting (WMC), i.e., an extension of model counting ($\#\SAT{}$)
that assigns a weight to every model \citep{DBLP:conf/aaai/SangBK05}, has
emerged as one of the most dominant and competitive approaches for handling
inference tasks in a wide range of formalisms including Bayesian networks
\citep{DBLP:conf/aaai/SangBK05,DBLP:books/daglib/0024906}, probabilistic
graphical models more generally \citep{DBLP:conf/ecsqaru/ChoiKD13}, and
probabilistic programs
\citep{DBLP:journals/tplp/FierensBRSGTJR15,DBLP:journals/corr/abs-2005-09089}.
Over the last fifteen years, WMC has been extended and generalised in many ways,
e.g., to handle continuous probability distributions
\citep{DBLP:conf/ijcai/BellePB15}, first-order probabilistic theories
\citep{DBLP:conf/ijcai/BroeckTMDR11,DBLP:journals/cacm/GogateD16}, and infinite
domains \citep{DBLP:conf/aaai/Belle17}. Furthermore, by generalising the notion
of weights to an arbitrary semiring, a range of other problems are also captured
\citep{DBLP:journals/japll/KimmigBR17}. Exact WMC solvers typically rely on
either knowledge compilation
\citep{DBLP:conf/ijcai/OztokD15,DBLP:conf/ijcai/LagniezM17} or exhaustive DPLL
search \citep{DBLP:conf/aaai/SangBK05}, whereas approximate solvers work by
sampling \citep{DBLP:conf/aaai/ChakrabortyFMSV14} and performing local search
\citep{DBLP:conf/sat/WeiS05}.

The most well-studied version of WMC assigns weights to models based on
weights on literals, i.e., the weight of a model is the product of the weights
of all literals in it. This simplification is motivated by the fact that the
number of models scales exponentially with the number of atoms, so listing the
weight of every model is intractable. However, this also severely restricts what
probability distributions can be represented. A common way to overcome this
limitation is by adding more literals. While we show that this is always
possible, we also demonstrate that it can be significantly more efficient to
encode weights in a more flexible format instead.

After briefly reviewing the background in \cref{sec:related}, in
\cref{sec:prelims} we  describe three equivalent perspectives on the subject
based on logic, set theory, and Boolean algebras. Furthermore, we describe the
space of functions on Boolean algebras and various operations on those
functions. \Cref{sec:wmc_as_measure} introduces WMC as the problem of computing
the value of a measure on a Boolean algebra. We show that not all measures can
be represented using literal-based WMC, but all Boolean algebras can be extended
to make any measure representable in such a manner.

This new measure-theoretic perspective allows us to not only encode any
discrete probability distribution but also improve inference speed. In
\cref{sec:bns}, we demonstrate this by developing a new WMC encoding for
Bayesian networks that uses \emph{conditional weights} on literals (in the
spirit of conditional probabilities) that have literal-based WMC as a special
case. We prove the correctness of the encoding and show how a state-of-the-art
WMC solver ADDMC \citep{DBLP:conf/aaai/DudekPV20} can be adapted to the new
format. ADDMC is a recently-proposed algorithm for WMC based on manipulating
functions on Boolean algebras using an efficient representation for such
functions known as algebraic decision diagrams (ADDs)
\citep{DBLP:journals/fmsd/BaharFGHMPS97}. Finally, in \cref{sec:experiments}, we
show how the new encoding results in faster inference on the vast majority of
benchmark instances, often by one or two orders of magnitude. We explain the
performance benefits by showing how our encoding has asymptotically fewer
variables and ADDs.

\section{Related Work} \label{sec:related}

Hitherto, four techniques have been proposed for encoding Bayesian networks into
instances of WMC. We will identify them based on the initials of authors as well
as publications years: \texttt{d02} \citep{DBLP:conf/kr/Darwiche02},
\texttt{sbk05} \citep{DBLP:conf/aaai/SangBK05}, \texttt{cd05}
\citep{DBLP:conf/ijcai/ChaviraD05}, and \texttt{cd06}
\citep{DBLP:conf/sat/ChaviraD06}. Below we summarise the observed performance
differences among them. \citet{DBLP:conf/aaai/SangBK05} claim that
\texttt{sbk05} is a smaller encoding than \texttt{d02} with respect to both the
number of clauses and the number of variables but provide no experimental
comparison. \citet{DBLP:conf/ijcai/ChaviraD05} compare \texttt{cd05} with
\texttt{d02} by measuring the time it takes to compile either encoding into an
arithmetic circuit (but do not measure inference time). The more recent encoding
\texttt{cd05} always compiles faster and results in a smaller arithmetic circuit
(as measured by the number of edges). In their subsequent paper, the same
authors perform two sets of experiments (that are relevant to this summary)
\citep{DBLP:conf/sat/ChaviraD06}. First, they compile \texttt{cd05} and
\texttt{cd06} encodings into d-DNNF (i.e., deterministic decomposable negation
normal form \citep{DBLP:journals/jancl/Darwiche01}), measuring both compilation
time and numbers of edges in the d-DNNF diagram. The results are mostly in
favour of \texttt{cd06}. Second, they compare the inference time of
\texttt{sbk05} run with Cachet \citep{DBLP:conf/sat/SangBBKP04} with the compile
times of \texttt{cd05} and \texttt{cd06}, but only on five (types of) instances.
In these experiments, \texttt{cd06} is always faster than \texttt{cd05}, while
the comparison with \texttt{sbk05} is mixed. Sometimes \texttt{cd06} is orders
of magnitude faster than \texttt{sbk05}, sometimes slightly slower. The
performance difference between \texttt{sbk05} and \texttt{cd05} is even harder
to judge: \texttt{sbk05} is better on three out of five instances and worse on
the remaining two. Based on this description, one would expect \texttt{cd06} to
be faster than both \texttt{cd05} and \texttt{sbk05}, both of which should be
faster than \texttt{d02}. The experiments in \cref{sec:experiments}, however,
strongly disagree with this prediction, showing that the quality of an encoding
depends strongly on the underlying search algorithm or compilation technique.

% \paragraph{ADDs and their use in inference.} ADDs provide an efficient way to
% manipulate functions from a Boolean algebra to any algebraic structure (most
% commonly real numbers) \citep{DBLP:journals/fmsd/BaharFGHMPS97}. They have been
% used to represent value functions in Markov decision processes (MDPs)
% \citep{DBLP:conf/uai/HoeySHB99} and extended to handle first-order MDPs
% \citep{DBLP:journals/ai/SannerB09} and discrete and continuous state MDPs
% \citep{DBLP:conf/uai/SannerDB11}. The latter extension has also been used for
% weighted model integration---an extension of WMC that can handle continuous
% variables \citep{DBLP:conf/ijcai/KolbMSBK18}. For Bayesian networks, ADDS have
% been used to represent each conditional probability table (CPT) as an ADD
% \citep{DBLP:conf/icml/ZhaoMP15} and as a combination of ADDs and arithmetic
% circuits \citep{DBLP:conf/ijcai/ChaviraD07}. ADDs are particularly advantageous
% in this situation because of their ability to fully exploit context-specific
% independence, i.e., the observable structure within a CPT that is not inherited
% from the structure of the Bayesian network \citep{DBLP:conf/uai/BoutilierFGK96}.

\section{Boolean Algebras, Power Sets, and Propositional
  Logic} \label{sec:prelims}

\begin{table*}
  \caption{Notation for a logic with two atoms. The elements in both columns are
    listed in the same order.}
  \label{tbl:notation_example}
  \centering
  \begin{tabular}{lcc}
    \toprule
    Name in logic & Boolean-algebraic notation & Set-theoretic notation \\
    \midrule
    Atoms (elements of $U$) & $a, b$ & $a, b$ \\
    \rowcolor{gray!10} Models (elements of $2^U$) & $\neg a \land \neg b, a \land \neg b, \neg a \land b, a \land b$ & $\emptyset, \{a\}, \{b\}, \{a, b\}$ \\
    & $\top$ & $\{ \emptyset, \{a\}, \{b\}, \{a, b\} \}$ \\
    & $\neg a \lor \neg b, a \to b$ & $\{ \emptyset, \{a\}, \{b\} \}, \{ \emptyset, \{a\}, \{a, b\} \}$ \\
    & $b \to a, a \lor b$ & $\{ \emptyset, \{b\}, \{a, b\} \}, \{ \{a\}, \{b\}, \{a, b\} \}$ \\
    & $\neg b, \neg a, a \leftrightarrow b$ & $\{\emptyset, \{a\}\}, \{\emptyset, \{b\}\}, \{\emptyset, \{a, b\}\}$ \\
    & $(a \land \neg b) \lor (b \land \neg a), a, b$ & $\{\{a\}, \{b\}\}, \{\{a\}, \{a, b\}\}, \{\{b\}, \{a, b\}\}$ \\
    & $\neg a \land \neg b, a \land \neg b, \neg a \land b, a \land b$ & $\{\emptyset\}, \{\{a\}\}, \{\{b\}\}, \{\{a, b\}\}$ \\
    \multirow{-7}{*}{Formulas (elements of $2^{2^U}$)} & $\bot$ & $\emptyset$ \\
    \bottomrule
  \end{tabular}
\end{table*}

In this section, we give a brief introduction to two alternative ways to think
about logical constructs such as models and formulas. Let us consider a simple
example of a propositional logic $\mathcal{L}$ with only two atoms $a$ and $b$,
and let $U = \{ a, b \}$. Then $2^U$, the power set of $U$, is the set of all
models of $\mathcal{L}$, and $2^{2^U}$ is the set of all formulas. These sets
can also be represented as Boolean algebras (e.g., using the syntax $(2^{2^U},
\land, \lor, \neg, \bot, \top)$) with a partial order $\le$ that corresponds to
set inclusion $\subseteq$---see \cref{tbl:notation_example} for examples of how
various elements can be represented in both notations. Most importantly, note
that the word \emph{atom} has completely different meanings in logic and in
Boolean algebras. An atom in $\mathcal{L}$ is an atomic formula, i.e., an
element of $U$, whereas an atom in a Boolean algebra is (in set-theoretic terms)
a singleton set. For instance, an atom in $2^{2^U}$ corresponds to a model of
$\mathcal{L}$, i.e., an element of $2^U$. Unless referring specifically to a
logic, we will use the algebraic definition of an atom and refer to logical
atoms as \emph{variables}. In the rest of the paper, for any set $U$, we will
use set-theoretic notation for $2^U$ and Boolean-algebraic notation for
$2^{2^U}$, except for (Boolean) atoms in $2^{2^U}$ that are denoted as $\{x\}$
for some model $x \in 2^U$.

\subsection{The Space of Functions on Boolean Algebras}

We also consider the space of all functions from any Boolean algebra to
$\mathbb{R}_{\ge 0}$ together with some operations on those functions. They will
be instrumental in defining WMC as a measure in \cref{sec:wmc_as_measure} and
can be efficiently represented using ADDs. Furthermore, all of the operations
are supported by the CUDD \citep{somenzi1998cudd} package that is used by ADDMC
for ADD manipulation \citep{DBLP:conf/aaai/DudekPV20}. The definitions of
multiplication and projection are as defined by
\citet{DBLP:conf/aaai/DudekPV20}, while others are new.

\begin{definition}[Operations on functions]
  Let $\alpha\colon 2^X \to \mathbb{R}_{\ge 0}$ and $\beta\colon 2^Y \to
  \mathbb{R}_{\ge 0}$ be functions, $p \in \mathbb{R}_{\ge 0}$, and $x \in X$.
  We define the following operations:
  \begin{description}
  \item[Addition:] $\alpha + \beta\colon 2^{X \cup Y} \to \mathbb{R}_{\ge 0}$ is
    such that $(\alpha + \beta)(T) = \alpha(T \cap X) + \beta(T \cap Y)$ for all
    $T \in 2^{X \cup Y}$.
  \item[Multiplication:] $\alpha \cdot \beta\colon 2^{X \cup Y} \to
    \mathbb{R}_{\ge 0}$ is such that $(\alpha \cdot \beta)(T) = \alpha(T \cap X)
    \cdot \beta(T \cap Y)$ for all $T \in 2^{X \cup Y}$.
  \item[Scalar multiplication:] $p\alpha\colon 2^X \to \mathbb{R}_{\ge 0}$ is
    such that $(p\alpha)(T) = p \cdot \alpha(T)$ for all $T \in 2^X$.
  \item[Complement:] $\overline{\alpha}\colon 2^X \to \mathbb{R}_{\ge 0}$ is
    such that $\overline{\alpha}(T) = 1 - \alpha(T)$ for all $T \in 2^X$.
  \item[Projection:] $\exists_x\alpha\colon 2^{X \setminus \{ x \}} \to
    \mathbb{R}_{\ge 0}$ is such that $(\exists_x\alpha)(T) = \alpha(T) +
    \alpha(T \cup \{ x \})$ for all $T \in 2^{X \setminus \{x \}}$. For any $Z =
    \{ z_1, \dots, z_n \} \subseteq X$, we write $\exists_Z$ to mean
    $\exists_{z_1}\dots\exists_{z_n}$.
  \end{description}
\end{definition}

Note that if $U$ is any set, then $\mathcal{V} = \{ \alpha\colon 2^X \to
\mathbb{R}_{\ge 0} \mid X \subseteq U \}$ is a semi-vector space with three
additional operations: (non-scalar) multiplication, complement, and projection.
Specifically, note that both addition and multiplication are both associative
and commutative.

We end the discussion on function spaces by defining several special functions:
unit $1\colon 2^\emptyset \to \mathbb{R}_{\ge 0}$ defined as $1(\emptyset) = 1$,
zero $0\colon 2^\emptyset \to \mathbb{R}_{\ge 0}$ defined as $0(\emptyset) = 0$,
and function $[a]\colon 2^{\{a\}} \to \mathbb{R}_{\ge 0}$ defined as
$[a](\emptyset) = 0$, $[a](\{a\}) = 1$ for any $a$. Henceforth, for any function
$\alpha\colon 2^X \to \mathbb{R}_{\ge 0}$ and any set $T$, we will write
$\alpha(T)$ to mean $\alpha(T \cap X)$.

\section{WMC as a Measure on a Boolean Algebra} \label{sec:wmc_as_measure}

In this section, we introduce an alternative definition of WMC and demonstrate
how it relates to the standard one. Let $U$ be a set. A \emph{measure} is a
function $\mu\colon 2^{2^U} \to \mathbb{R}_{\ge 0}$ such that $\mu(\bot) = 0$,
and $\mu(a \lor b) = \mu(a) + \mu(b)$ for all $a, b \in 2^{2^U}$ whenever $a
\land b = \bot$ \citep{gaifman1964concerning,DBLP:books/daglib/0090259}. A
\emph{weight function} is a function $\nu\colon 2^U \to \mathbb{R}_{\ge 0}$. A
weight function is \emph{factored} if $\nu = \prod_{x \in U} \nu_x$ for some
functions $\nu_x\colon 2^{\{x\}} \to \mathbb{R}_{\ge 0}$, $x \in U$. We say that
a weight function $\nu\colon 2^U \to \mathbb{R}_{\ge 0}$ \emph{induces} a
measure $\mu_\nu\colon 2^{2^U} \to \mathbb{R}_{\ge 0}$ if $\mu_\nu(x) =
\sum_{\{u\} \le x} \nu(u)$.

\begin{restatable}{theorem}{measure} \label{prop:measure}
  The function $\mu_\nu$ is a measure.
\end{restatable}

Finally, a measure $\mu\colon 2^{2^U} \to \mathbb{R}_{\ge 0}$ is
\emph{factorable} if there exists a factored weight function $\nu\colon 2^U \to
\mathbb{R}_{\ge 0}$ that induces $\mu$. In this formulation, WMC corresponds to
the process of calculating the value of $\mu_\nu(x)$ for some $x \in 2^{2^U}$
with a given definition of $\nu$.

\paragraph{Relation to the classical (logic-based) view of WMC.} Let
$\mathcal{L}$ be a propositional logic with two atoms $a$ and $b$ as in
\cref{sec:prelims} and $w\colon \{ a, b, \neg a, \neg b \} \to \mathbb{R}_{\ge
  0}$ a weight function defined as $w(a) = 0.3$, $w(\neg a) = 0.7$, $w(b)
= 0.2$, $w(\neg b) = 0.8$. Furthermore, let $\Delta$ be a theory in
$\mathcal{L}$ with a sole axiom $a$. Then $\Delta$ has two models: $\{ a, b \}$
and $\{ a, \neg b \}$ and its WMC \citep{DBLP:journals/ai/ChaviraD08} is
\begin{equation} \label{eq:wmc_example}
  \begin{split}
    \mathrm{WMC}(\Delta) &= \sum_{\omega \models \Delta} \prod_{\omega \models l} w(l) \\
    &= w(a)w(b) + w(a)w(\neg b) = 0.3.
  \end{split}
\end{equation}
Alternatively, we can define $\nu_a\colon 2^{\{a\}} \to \mathbb{R}_{\ge 0}$ as
$\nu_a(\{ a \}) = 0.3$, $\nu_a(\emptyset) = 0.7$ and $\nu_b\colon 2^{\{b\}} \to
\mathbb{R}_{\ge 0}$ as $\nu_b(\{ b \}) = 0.2$, $\nu_b(\emptyset) = 0.8$. Let
$\mu$ be the measure on $2^{2^U}$ induced by $\nu = \nu_a \cdot \nu_b$. Then,
equivalently to \cref{eq:wmc_example}, we can write
\begin{align*}
  \mu(a) &= \nu(\{ a, b \}) + \nu(\{ a \}) \\
         &= \nu_a(\{a\})\nu_b(\{b\}) + \nu_a(\{a\})\nu_b(\emptyset) = 0.3.
\end{align*}
Thus, one can equivalently think of WMC as summing over models of a theory or
over atoms below an element of a Boolean algebra.

\subsection{Not All Measures Are Factorable}

Using this new definition of WMC, we can show that WMC with weights defined on
literals is only able to capture a subset of all possible measures on a
Boolean algebra. This can be demonstrated with a simple example.

\begin{example}
  Let $U = \{a, b\}$ be a set of atoms and $\mu\colon 2^{2^U} \to
  \mathbb{R}_{\ge 0}$ a measure defined as $\mu(a \land b) = 0.72$, $\mu(a \land
  \neg b) = 0.18$, $\mu(\neg a \land b) = 0.07$, $\mu(\neg a \land \neg b) =
  0.03$.\footnote{The value of $\mu$ on any other element of $2^{2^U}$ can be
    deduced from the definition of a measure.} If $\mu$ could be represented
  using literal-weight (factored) WMC, we would have to find two weight
  functions $\nu_a\colon 2^{\{a\}} \to \mathbb{R}_{\ge 0}$ and $\nu_b\colon
  2^{\{b\}} \to \mathbb{R}_{\ge 0}$ such that $\nu = \nu_a \cdot \nu_b$ induces
  $\mu$, i.e., $\nu_a$ and $\nu_b$ would have to satisfy this system of
  equations:
  \begin{align*}
    \nu_a(\{a\}) \cdot \nu_b(\{b\}) &= 0.72 \\
    \nu_a(\{a\}) \cdot \nu_b(\emptyset) &= 0.18 \\
    \nu_a(\emptyset) \cdot \nu_b(\{b\}) &= 0.07 \\
    \nu_a(\emptyset) \cdot \nu_b(\emptyset) &= 0.03,
  \end{align*}
  which has no solutions.

  Alternatively, we can let $b$ depend on $a$ and consider weight functions
  $\nu_a\colon 2^{\{a\}} \to \mathbb{R}_{\ge 0}$ and $\nu_b\colon 2^{\{a, b\}}
  \to \mathbb{R}_{\ge 0}$ defined as $\nu_a(\{a\}) = 0.9$, $\nu_a(\emptyset) =
  0.1$, and $\nu_b(\{a, b\}) = 0.8$, $\nu_b(\{a\}) = 0.2$, $\nu_b(\{b\}) = 0.7$,
  $\nu_b(\emptyset) = 0.3$. One can easily check that with these definitions
  $\nu$ indeed induces $\mu$.
\end{example}

Note that in this case, we chose to interpret $\nu_b$ as $\Pr(b \mid a)$
while---with a different definition of $\nu_b$ that represents the joint
probability distribution $\Pr(a, b)$---$\nu_b$ by itself could induce $\mu$. In
general, however, factorising the full weight function into several smaller
functions often results in weight functions with smaller domains which leads to
increased efficiency and decreased memory usage
\citep{DBLP:conf/aaai/DudekPV20}. We can easily generalise this example further.

\begin{theorem}
  For any set $U$ such that $|U| \ge 2$, there exists a non-factorable measure
  $2^{2^{U}} \to \mathbb{R}_{\ge 0}$.
\end{theorem}

Since many measures of interest may not be factorable, a well-known way to
encode them into instances of WMC is by adding more literals
\citep{DBLP:journals/ai/ChaviraD08}. We can use the measure-theoretic
perspective on WMC to show that this is always possible, however, as ensuing
sections will demonstrate, it often makes the inference task much harder in
practice.\footnote{The proofs of this and other theoretical results can be found
  in the technical appendix.}

\begin{restatable}{theorem}{extendable}
  For any set $U$ and measure $\mu\colon 2^{2^U} \to \mathbb{R}_{\ge 0}$, there
  exists a set $V \supseteq U$, a factorable measure $\mu'\colon 2^{2^V} \to
  \mathbb{R}_{\ge 0}$, and a formula $f \in 2^{2^V}$ such that $\mu(x) = \mu'(x
  \land f)$ for all formulas $x \in 2^{2^U}$.
\end{restatable}

\section{Encoding Bayesian Networks Using Conditional Weights} \label{sec:bns}

\begin{figure*}
  \centering
  \begin{subfigure}{0.2\textwidth}
    \centering
    \begin{tikzpicture}[edge from parent/.style={draw,-latex}]
      \node[draw,circle] {$W$}
      child {node[draw,circle] {$F$}}
      child {node[draw,circle] {$T$}};
    \end{tikzpicture}
  \end{subfigure}%
  \begin{subfigure}{0.8\textwidth}
    \centering
    \begin{tabular}[t]{cc}
      \toprule
      $w$ & $\Pr(W = w)$ \\
      \midrule
      1 & 0.5 \\
      0 & 0.5 \\
      \bottomrule
    \end{tabular}
    \begin{tabular}[t]{ccc}
      \toprule
      $w$ & $f$ & $\Pr(F = f \mid W = w)$ \\
      \midrule
      1 & 1 & 0.6 \\
      1 & 0 & 0.4 \\
      0 & 1 & 0.1 \\
      0 & 0 & 0.9 \\
      \bottomrule
    \end{tabular}
    \begin{tabular}[t]{ccc}
      \toprule
      $w$ & $t$ & $\Pr(T = t \mid W = w)$ \\
      \midrule
      1 & $l$ & 0.2 \\
      1 & $m$ & 0.4 \\
      1 & $h$ & 0.4 \\
      0 & $l$ & 0.6 \\
      0 & $m$ & 0.3 \\
      0 & $h$ & 0.1 \\
      \bottomrule
    \end{tabular}
  \end{subfigure}
  \caption{An example Bayesian network with its CPTs.}
  \label{fig:example_bn}
\end{figure*}

In this section, we describe a way to encode Bayesian networks into WMC without
restricting oneself to factorable measures and thus having to add extra
variables. We will refer to it as \texttt{cw}.

A Bayesian network is a directed acyclic graph with random variables as vertices
that defines a probability distribution over them. Let $\mathcal{V}$ denote this
set of random variables. For any random variable $X \in \mathcal{V}$, let $\im
X$ denote its set of values and $\mathrm{pa}(X)$ its set of parents. The
full probability distribution is then equal to $\prod_{X \in \mathcal{V}} \Pr(X
\mid \mathrm{pa}(X))$ \citep{DBLP:books/daglib/0023820}. For discrete Bayesian
networks (and we only consider discrete networks in this paper), each factor of
this product can be represented by a CPT. See \cref{fig:example_bn} for an
example Bayesian network that we will refer to throughout this section. For
this network, $\mathcal{V} = \{ W, F, T \}$, $\mathrm{pa}(W) = \emptyset$,
$\mathrm{pa}(F) = \mathrm{pa}(T) = \{ W \}$, $\im W = \im F = \{0, 1 \}$, and
$\im T = \{ l, m, h \}$.

\begin{definition}[Indicator variables]
  Let $X \in \mathcal{V}$ be a random variable. If $X$ is binary (i.e., $|\im X|
  = 2$), we can arbitrary identify one of the values as $1$ and the other one as
  $0$ (i.e, $\im X \cong \{ 0, 1 \}$). Then $X$ can be represented by a single
  \emph{indicator variable} $\lambda_{X=1}$. For notational simplicity, for any
  set $S$, we write $\lambda_{X=0} \in S$ or $S = \{ \lambda_{X=0}, \dots \}$ to
  mean $\lambda_{X=1} \not\in S$.

  On the other hand, if $X$ is not binary, we represent $X$ with $|\im X|$
  indicator variables, one for each value. We let
  \[
    \mathcal{E}(X) = \begin{cases}
      \{ \lambda_{X=1} \} & \text{if } |\im X| = 2 \\
      \{ \lambda_{X=x} \mid x \in \im X \} & \text{otherwise.}
    \end{cases}
  \]
  denote the set of indicator variables for $X$ and $\mathcal{E}^*(X) =
  \mathcal{E}(X) \cup \bigcup_{Y \in \mathrm{pa}(X)} \mathcal{E}(Y)$ denote the
  set of indicator variables for $X$ and its parents in the Bayesian network.
  Finally, let $U = \bigcup_{X \in \mathcal{V}} \mathcal{E}(X)$ denote the set
  of all indicator variables for all random variables in the Bayesian network.
\end{definition}

For example, in the Bayesian network from \cref{fig:example_bn},
$\mathcal{E}^*(T) = \{ \lambda_{T=l}, \lambda_{T=m}, \lambda_{T=h},
\lambda_{W=1} \}$.

\begin{algorithm}[t]
  \caption{Encoding a Bayesian network.}
  \label{alg:encoding}
  \KwData{vertices $\mathcal{V}$, probability distribution $\Pr$}
  \KwResult{$\phi\colon 2^U \to \mathbb{R}_{\ge 0}$}
  $\phi \gets 1$\;
  \For{$X \in \mathcal{V}$}{
    \textit{let} $\mathrm{pa}(X) = \{ Y_1, \dots, Y_n \}$\;
    $\mathrm{CPT}_X \gets 0$\;
    \eIf{$|\im X| = 2$}{
      \For{$(y_i)_{i=1}^n \in \prod_{i = 1}^n \im Y_i$}{
        $p_1 \gets \Pr(X = 1 \mid y_1, \dots, y_n)$\;
        $p_0 \gets \Pr(X \ne 1 \mid y_1, \dots, y_n)$\;
        \nosemic $\mathrm{CPT}_X \gets \mathrm{CPT}_X$\;
        \pushline $+ p_1[\lambda_{X=1}] \cdot \prod_{i=1}^n
        [\lambda_{Y_i=y_i}]$\;
        \dosemic $+ p_0 \overline{[\lambda_{X=1}]} \cdot \prod_{i=1}^n
        [\lambda_{Y_i=y_i}]$\;
      }
    }{
      \textit{let} $\im X = \{ x_1, \dots, x_m \}$\;
      \For{$x \in \im X$ {\rm \textbf{and}} $(y_i)_{i=1}^n \in \prod_{i = 1}^n
        \im Y_i$}{
        $p_x \gets \Pr(X = x \mid y_1, \dots, y_n)$\;
        \nosemic $\mathrm{CPT}_X \gets \mathrm{CPT}_X$\;
        \pushline $+ p_x[\lambda_{X=x}] \cdot \prod_{i=1}^n
        [\lambda_{Y_i=y_i}]$\;
        \dosemic $+ \overline{[\lambda_{X=x}]} \cdot \prod_{i=1}^n
        [\lambda_{Y_i=y_i}]$\;
      }
      \nosemic $\mathrm{CPT}_X \gets \mathrm{CPT}_X \cdot \left( \sum_{i=1}^m
        [\lambda_{X = x_i}] \right)$\;
      \pushline\dosemic $\cdot \prod_{i=1}^m \prod_{j=i+1}^m
      (\overline{[\lambda_{X = x_i}]} + \overline{[\lambda_{X = x_j}]})$\;
    }
    $\phi \gets \phi \cdot \mathrm{CPT}_X$\;
  }
  \Return{$\phi$}\;
\end{algorithm}

\Cref{alg:encoding} shows how a Bayesian network with vertices $\mathcal{V}$
can be represented as a weight function $\phi\colon 2^U \to \mathbb{R}_{\ge 0}$.
The algorithm begins with the unit function and multiplies it by
$\mathrm{CPT}_X\colon 2^{\mathcal{E}^*(X)} \to \mathbb{R}_{\ge 0}$ for each
random variable $X \in \mathcal{V}$. We call each such function a
\emph{conditional weight function} as it represents a conditional probability
distribution. However, the distinction is primarily a semantic one: a function
$2^{\{a, b\}} \to \mathbb{R}_{\ge 0}$ can represent $\Pr(a \mid b)$, $\Pr(b \mid
a)$, or something else entirely, e.g., $\Pr(a \land b)$, $\Pr(a \lor b)$, etc.

For a binary random variable $X$, $\mathrm{CPT}_X$ is simply a sum of smaller
functions, one for each row of the CPT. If $X$ has more than two values, we also
multiply $\mathrm{CPT}_X$ by `clause' functions that restrict the value of
$\phi(T)$ to zero whenever $|\mathcal{E}(X) \cap T| \ne 1$. For the Bayesian
network in \cref{fig:example_bn}, we get:
\begin{align*}
  \mathrm{CPT_F} &= 0.6[\lambda_{F=1}] \cdot [\lambda_{W=1}] + 0.4[\lambda_{F=0}] \cdot [\lambda_{W=1}] \\
                 &+ 0.1[\lambda_{F=1}] \cdot [\lambda_{W=0}] + 0.9[\lambda_{F=0}] \cdot [\lambda_{W=0}], \\
  \mathrm{CPT_T} &= ([\lambda_{T=l}] + [\lambda_{T=m}] + [\lambda_{T=h}]) \\
                 &\cdot (\overline{[\lambda_{T=l}]} + \overline{[\lambda_{T=m}]}) \cdot (\overline{[\lambda_{T=l}]} + \overline{[\lambda_{T=h}]}) \\
                 &\cdot (\overline{[\lambda_{T=m}]} + \overline{[\lambda_{T=h}]}) \cdot \dots.
\end{align*}

\subsection{Correctness}

\Cref{alg:encoding} produces a function with a Boolean algebra as its domain.
This function can be represented by an ADD
\citep{DBLP:journals/fmsd/BaharFGHMPS97}. The core of ADDMC works by taking an
ADD $\psi\colon 2^{U} \to \mathbb{R}_{\ge 0}$ (expressed as a product of smaller
ADDs) and returning $(\exists_U\psi)(\emptyset)$
\citep{DBLP:conf/aaai/DudekPV20}. In this section, we prove that the function
$\phi$ produced by \cref{alg:encoding} can be used by ADDMC to correctly compute
any marginal probability of the Bayesian network that was encoded as
$\phi$.\footnote{Note that it can just as well compute \emph{any} probability
  expressed using the random variables in $\mathcal{V}$.} We begin with
\cref{lemma:cpt} which shows that any conditional weight function produces the
right answer when given a valid encoding of variable-value assignments relevant
to the CPT.

\begin{restatable}{lemma}{cptlemma} \label{lemma:cpt}
  Let $X \in \mathcal{V}$ be a random variable with parents $\mathrm{pa}(X) = \{ Y_1,
  \dots, Y_n \}$. Then $\mathrm{CPT}_X\colon 2^{\mathcal{E}^*(X)} \to
  \mathbb{R}_{\ge 0}$ is such that for any $x \in \im X$ and $(y_1, \dots, y_n)
  \in \prod_{i=1}^n \im Y_i$,
  \[
    \mathrm{CPT}_X (T) = \Pr(X = x \mid Y_1 = y_1, \dots, Y_n = y_n),
  \]
  where $T = \{ \lambda_{X=x} \} \cup \{ \lambda_{Y_i=y_i} \mid i = 1, \dots, n
  \}$.
\end{restatable}

Now, \cref{lemma:full_distribution} shows that $\phi$ represents the full
probability distribution of the Bayesian network, i.e., it gives the right
probabilities for the right inputs and zero otherwise. 

\begin{restatable}{lemma}{fulldistribution} \label{lemma:full_distribution}
  Let $\mathcal{V} = \{X_1, \dots, X_n\}$. Then
  \[
    \phi(T) =
    \begin{cases}
      \Pr(x_1, \dots, x_n) &
      \begin{aligned}
        &\text{if } T = \{ \lambda_{X_i=x_i} \}_{i = 1}^n \text{ for} \\
        &\text{some } \textstyle (x_i)_{i=1}^n \in \prod_{i=1}^n \im X_i
      \end{aligned} \\
      0 & \text{otherwise,}
    \end{cases}
  \]
  for all $T \in 2^U$.
\end{restatable}

We end with \cref{thm:correctness} that shows how $\phi$ can be combined with an
encoding of a single variable-value assignment so that ADDMC would compute its
marginal probability.

\begin{restatable}{theorem}{correctness} \label{thm:correctness}
  For any $X \in \mathcal{V}$ and $x \in \im X$,
  \[
    (\exists_U(\phi \cdot [\lambda_{X=x}]))(\emptyset) = \Pr(X = x).
  \]
\end{restatable}

\subsection{Textual Representation} \label{sec:textual_representation}

\Cref{alg:encoding} encodes a Bayesian network into a function on a Boolean
algebra, but how does it relate to the standard interpretation of a WMC encoding
as a formula in conjunctive normal form (CNF) together with a collection of
weights? The factors of $\phi$ that restrict the values of indicator variables
for non-binary random variables are already expressed as a product of sums of
0/1-valued functions, i.e., a kind of CNF. Disregarding these functions, each
conditional weight function $\mathrm{CPT}_X$ is represented by a sum with a term
for every subset of $\mathcal{E}^*(X)$. To encode these terms, we introduce
\emph{extended weight clauses} to the WMC format used by Cachet
\citep{DBLP:conf/sat/SangBBKP04}. For instance, here is a representation of the
Bayesian network from \cref{fig:example_bn}:
\[
  \begin{array}{lrrll}
    \lambda\sb{T=l} &\lambda\sb{T=m} &\lambda\sb{T=h} & &0 \\
                    &-\lambda\sb{T=l} &-\lambda\sb{T=m} & &0 \\
                    &-\lambda\sb{T=l} &-\lambda\sb{T=h} & &0 \\
                    &-\lambda\sb{T=m} &-\lambda\sb{T=h} & &0 \\
    w &\lambda\sb{W=1} & &0.5 &0.5 \\
    w &\lambda\sb{F=1} &\lambda\sb{W=1} &0.6 &0.4 \\
    w &\lambda\sb{F=1} &-\lambda\sb{W=1} &0.1 &0.9 \\
    w &\lambda\sb{T=l} &\lambda\sb{W=1} &0.2 &1 \\
    w &\lambda\sb{T=m} &\lambda\sb{W=1} &0.4 &1 \\
    w &\lambda\sb{T=h} &\lambda\sb{W=1} &0.4 &1 \\
    w &\lambda\sb{T=l} &-\lambda\sb{W=1} &0.6 &1 \\
    w &\lambda\sb{T=m} &-\lambda\sb{W=1} &0.3 &1 \\
    w &\lambda\sb{T=h} &-\lambda\sb{W=1} &0.1 &1
  \end{array}
\]
where each indicator variable is eventually replaced with a unique positive
integer. Each line prefixed with a $w$ can be split into four parts: the `main'
variable (always not negated), conditions (possibly none), and two weights. For
example, the line
\[
  \begin{array}{lrrll}
    w &\lambda\sb{T=m} &-\lambda\sb{W=1} &0.3 &1
  \end{array}
\]
encodes the function $0.3[\lambda_{T=m}] \cdot \overline{[\lambda_{W=1}]} +
1\overline{[\lambda_{T=m}]} \cdot \overline{[\lambda_{W=1}]}$ and can be
interpreted as defining two conditional weights: $\nu(T = m \mid W = 0) = 0.3$,
and $\nu(T \ne m \mid W = 0) = 1$, the former of which corresponds to a row in
the CPT of $T$ while the latter is artificially added as part of the encoding.
In our encoding of Bayesian networks, it is always the case that, in each weight
clause, either both weights sum to one, or the second weight is equal to one.
Finally, note that the measure induced by these weights is not probabilistic
(i.e., $\mu(\top) \ne 1$) by itself, but it becomes probabilistic when combined
with the additional clauses that restrict what combinations of indicator
variables can co-occur.

\subsection{Changes to ADDMC}

Here we describe two changes to
ADDMC\footnote{\url{https://github.com/vardigroup/ADDMC}}
\citep{DBLP:conf/aaai/DudekPV20} needed to adapt it to the new format.

First, ADDMC constructs the \emph{Gaifman graph} \citep{gaifman1982local} of the
input CNF formula as an aid for the algorithm's heuristics. This graph has as
vertices the variables of the formula, and there is an edge between two
variables $u$ and $v$ if there is a clause in the formula that contains both $u$
and $v$. We extend this definition to functions on Boolean algebras, i.e., the
factors of $\phi$. For any pair of distinct variables $u, v \in U$, we draw an
edge between them in the Gaifman graph if there is a function $\alpha\colon 2^X
\to \mathbb{R}_{\ge 0}$ that is a factor of $\phi$ such that $u \in X$ and $v
\in X$. For instance, a factor such as $\mathrm{CPT}_X$ will enable edges
between all distinct pairs of variables in $\mathcal{E}^*(X)$.

Second, even though the function $\phi$ produced by \cref{alg:encoding} is
constructed to have $2^U$ as its domain, sometimes the domain is effectively
reduced to $2^V$ for some $V \subset U$ by the ADD manipulation algorithms that
optimise the ADD representation of a function. For a simple example, consider
$\alpha: 2^{\{a\}} \to \mathbb{R}_{\ge 0}$ defined as $\alpha(\{a\}) =
\alpha(\emptyset) = 0.5$. Then $\alpha$ can be reduced to $\alpha'\colon
2^{\emptyset} \to \mathbb{R}_{\ge 0}$ defined as $\alpha'(\emptyset) = 0.5$. To
compensate for these reductions, for the original WMC format with a weight
function $w\colon U \cup \{ \neg u \mid u \in U \} \to \mathbb{R}_{\ge 0}$,
ADDMC would multiply its computed answer by $\prod_{u \in U \setminus V} w(u) +
w(\neg u)$. With the new WMC format, we instead multiply the answer by $2^{|U
  \setminus V|}$. Each `excluded' variable $u \in U \setminus V$ satisfies two
properties: all weights associated with $u$ are equal to $0.5$ (otherwise the
corresponding CPT would depend on $u$, and $u$ would not be excluded), and all
other CPTs are independent of $u$ (or they may have a trivial dependence, where
the probability stays the same if $u$ is replaced with its complement). Thus,
the CPT that corresponds to $u$ still multiplies the weight of every atom in the
Boolean algebra by $0.5$, but the number of atoms under consideration is halved.
To correct for this, we multiply the final answer by two for every $u \in U
\setminus V$.

\section{Experimental Comparison} \label{sec:experiments}

In this section, we describe an experimental study comparing the five WMC
encodings for Bayesian networks when run with ADDMC. The experiments were run on
servers with Intel Xeon Gold 6138 and Intel Xeon E5-2630
processors\footnote{Each instance is run on the same processor for all
  encodings.} running Scientific~Linux~7 with an \SI{8}{\gibi\byte} memory limit
and a \SI{1000}{\second} timeout.

For each Bayesian network, we need to choose a probability to compute. Whenever
a Bayesian network comes with an evidence file, we compute the probability of
evidence. Otherwise, let $X$ denote the last-mentioned vertex in the Bayesian
network. If $\mathsf{true} \in \im X$, then we compute the marginal probability
of $X = \mathsf{true}$. Otherwise, we pick the value of $X$ which is listed
first and calculate its marginal probability.

For all encodings other than \texttt{cw}, we use their implementation in
Ace~3.0\footnote{\url{http://reasoning.cs.ucla.edu/ace/}} with
\texttt{-encodeOnly} and \texttt{-noEclause} flags. However, Ace was not used to
encode evidence, as preliminary experiments revealed that the evidence-encoding
implementation contains bugs that can lead to incorrect answers or a Java
exception being thrown on some instances of the data set (and the source code is
not publicly available). Instead, we simply list all the evidence as additional
clauses in the encoding.

Note that both \texttt{cd05} and \texttt{cd06} purposefully produce overly
relaxed encodings that contain extra models and thus yield incorrect
probabilities \citep{DBLP:conf/ijcai/ChaviraD05,DBLP:conf/sat/ChaviraD06}. These
additional models are supposed to be filtered out during circuit compilation
\citep{DBLP:conf/ijcai/ChaviraD05}, but this is not easily achievable with
ADDMC. Nonetheless, we include both encodings in the experiments.

While encoding time itself was not measured because of different languages of
implementation, it is worth noting that \texttt{cw} is linear in the total
number of CPT rows and so is unlikely to be slower than, e.g., \texttt{cd06},
which relies on solving $\NP{}$-complete problems as part of the encoding process
\citep{DBLP:conf/sat/ChaviraD06}.

For experimental data, we use the Bayesian networks available with Ace and
Cachet\footnote{\url{https://www.cs.rochester.edu/u/kautz/Cachet/}}, most of
which happen to be binary. We classify them into the following seven categories:
\begin{itemize*}
\item DQMR and
\item Grid networks as described by \citet{DBLP:conf/aaai/SangBK05},
\item Mastermind, and
\item Random Blocks from the work of \citet{DBLP:journals/ijar/ChaviraDJ06},
\item remaining binary Bayesian networks that include Plan Recognition
  \citep{DBLP:conf/aaai/SangBK05}, Friends and Smokers, Students and Professors
  \citep{DBLP:journals/ijar/ChaviraDJ06}, and \texttt{tcc4f}, and
\item non-binary classic Bayesian networks (\texttt{alarm}, \texttt{diabetes},
  \texttt{hailfinder}, \texttt{mildew}, \texttt{munin1}--\texttt{4},
  \texttt{pathfinder}, \texttt{pigs}, \texttt{water}).
\end{itemize*}
We run ADDMC with each of the five encodings once on each Bayesian network.

\begin{figure}
  \centering
  \input{cumulative.tex}%
  \caption{Cumulative number of instances solved by ADDMC over time using each
    encoding.}
  \label{fig:cumulative}
\end{figure}

\begin{table}
  \centering
  \caption{The numbers of instances (out of 1466) solved by ADDMC with each
    encoding (uniquely, faster than with others, and in total) under the
    described constraints.}%
  \begin{tabular}{lrrr}
    \toprule
    Encoding & Unique & Fastest & Total \\
    \midrule
    \texttt{cd05} & 2 & 3 & 380 \\
    \texttt{cd06} & 0 & 0 & 363 \\
    \texttt{cw} & 234 & 935 & 980 \\
    \texttt{d02} & 40 & 111 & 797 \\
    \texttt{sbk05} & 12 & 44 & 765 \\
    \bottomrule
  \end{tabular}
  \label{tbl:tallies}
\end{table}

\begin{figure*}
  \centering
  \input{scatter.tex}%
  \caption{ADDMC inference time using \texttt{cw} compared to \texttt{d02}
    (left) and \texttt{sbk05} (right) for each instance in all data sets.}
  \label{fig:scatter}
\end{figure*}

According to the cumulative plot in \cref{fig:cumulative}, \texttt{cw} is
consistently able to solve more instances with any given time limit than any
other encoding, while the order of the remaining encodings from best to worst is
exactly the opposite of that in previous literature (cf. \cref{sec:related}).
Specifically, \texttt{cw} solves the same number of instances in
\SI{8.144}{\second} as \texttt{d02} does in \SI{1000}{\second}, which is the
second-best encoding in our experiments. Furthermore, \cref{tbl:tallies} shows
that \texttt{cw} was the fastest in 935 out of the 980 (i.e.,
\SI{95.4}{\percent}) instances that it managed to solve. Finally, according to
the scatter plots in \cref{fig:scatter}, \texttt{cw} is particularly promising
on Grid networks while losing to \texttt{d02} on some instances of the
Mastermind data set. We conjecture that this is so either because the particular
structure of the problem confuses the heuristics used by ADDMC or because these
instances allow for significant simplifications that are exploited by
\texttt{d02} but not \texttt{cw}.

\begin{table}
  \centering
  \caption{Asymptotic upper bounds on the numbers of variables and clauses/ADDs
    for each encoding.}
  \label{tbl:asymptotes}
  \begin{tabular}{lcc}
    \toprule
    Encoding(s) & Variables & Clauses/ADDs \\
    \midrule
    \texttt{cd05}, \texttt{cd06}, \texttt{sbk05} & $O(nv^{d+1})$ & $O(nv^{d+1})$ \\
    \texttt{cw} & $O(nv)$ & $O(nv^2)$ \\
    \texttt{d02} & $O(nv^{d+1})$ & $O(ndv^{d+1})$ \\
    \bottomrule
  \end{tabular}
\end{table}

\paragraph{Explaining the performance benefits.} Let $n = |\mathcal{V}|$ be the
number of vertices in the Bayesian network, $d = \max_{X \in \mathcal{V}}
|\mathrm{pa}(X)|$ the maximum in-degree (i.e., the number of parents), and $v =
\max_{X \in \mathcal{V}} |\im X|$ the maximum number of values per variable.
\Cref{tbl:asymptotes} shows how \texttt{cw} has fewer variables and fewer
ADDs than any other encoding. However, note that these are upper bounds and most
encodings (including \texttt{cw}) can be smaller in certain situations (e.g.,
with binary random variables or when a CPT has repeating probabilities). We
equate clauses and ADDs (more specifically, factors of the function $\phi$ from
\cref{alg:encoding}) here because ADDMC interprets each clause of any WMC
encoding as a multiplicative factor of the ADD that represents the entire WMC
instance \citep{DBLP:conf/aaai/DudekPV20}. For literal-weight encodings, each
weight is also a factor, but that does not affect the asymptotic bounds in the
table.

\section{Discussion and Conclusion}

WMC was originally motivated by an appeal to the success of \SAT{} solvers in
efficiently tackling an $\NP{}$-complete problem
\citep{DBLP:conf/aaai/SangBK05}. ADDMC does not rely on \SAT{}-based algorithmic
techniques, and our proposed format diverges even more from the DIMACS CNF
format for Boolean formulas. To what extent are \SAT{}-based methods still
applicable? The answer depends significantly on the problem domain. For Bayesian
networks, the rules describing that each random variable can only be associated
with exactly one value were still encoded as clauses. As has been noted
previously \citep{DBLP:conf/sat/ChaviraD06}, rows in CPTs with probabilities
equal to zero or one can be represented as clauses as well. Therefore, our work
can be seen as proposing a middle ground between $\#\SAT{}$ and probabilistic
inference.

While we chose to use ADDMC \citep{DBLP:conf/aaai/DudekPV20} as the WMC
algorithm and Bayesian networks as a canonical example of a probabilistic
inference task, these are only examples meant to illustrate the broader idea
that choosing a more expressive representation of weights can outperform
increasing the size of the problem to keep the weights simple. Indeed, in this
work, we have provided a new theoretical perspective on the expressive power of
WMC and illustrated the empirical benefits of that perspective. The same idea
can be adapted to other inference problem domains such as probabilistic programs
\citep{DBLP:journals/tplp/FierensBRSGTJR15,DBLP:journals/corr/abs-2005-09089} as
well as to search-based solvers such as Cachet \citep{DBLP:conf/sat/SangBBKP04},
although the extension may be less straightforward for other WMC techniques.

\begin{contributions} % will be removed in pdf for initial submission,
                      % so you can already fill it to test with the
                      % ‘accepted’ class option
    Briefly list author contributions.
    This is a nice way of making clear who did what and to give proper credit.

    H.~Q.~Bovik conceived the idea and wrote the paper.
    Coauthor One created the code.
    Coauthor Two created the figures.
\end{contributions}

\begin{acknowledgements} % will be removed in pdf for initial submission,
                         % so you can already fill it to test with the
                         % ‘accepted’ class option
    Briefly acknowledge people and organizations here.

    \emph{All} acknowledgements go in this section.
\end{acknowledgements}

\newpage
\bibliography{paper}

\newpage
\appendix
\section{Proofs}

\measure*
\begin{proof}
  Note that $\mu_\nu(\bot) = 0$ since there are no atoms below $\bot$. Let $a, b
  \in 2^{2^{U}}$ be such that $a \land b = \bot$. By elementary properties of
  Boolean algebras, all atoms below $a \lor b$ are either below $a$ or below
  $b$. Moreover, none of them can be below both $a$ and $b$ because then they
  would have to be below $a \land b = \bot$. Thus
  \begin{align*}
    \mu_\nu(a \lor b) &= \sum_{\{u\} \le a \lor b} \nu(u) = \sum_{\{u\} \le a} \nu(u) + \sum_{\{u\} \le b} \nu(u) \\
                      &= \mu_\nu(a) + \mu_\nu(b)
  \end{align*}
  as required.
\end{proof}

\extendable*
\begin{proof}
  Let $V = U \cup \{ f_m \mid m \in 2^U \}$, and $f = \bigwedge_{m \in 2^U} \{ m
  \} \leftrightarrow f_m$. We define weight function $\nu\colon 2^V \to
  \mathbb{R}_{\ge 0}$ as $\nu = \prod_{v \in V} \nu_v$, where $\nu_v(\{v\}) =
  \mu(\{m\})$ if $v = f_m$ for some $m \in 2^U$ and $\nu_v(x) = 1$ for all other
  $v \in V$ and $x \in 2^{\{v\}}$. Let $\mu'\colon 2^{2^V} \to \mathbb{R}_{\ge
    0}$ be the measure induced by $\nu$. It is enough to show that $\mu$ and $x
  \mapsto \mu'(x \land f)$ agree on the atoms in $2^{2^U}$. For any $\{ a \} \in
  2^{2^U}$,
  \begin{align*}
    \mu'(\{ a \} \land f) &= \sum_{\{ x \} \le \{ a \} \land f} \nu(x) = \nu(a \cup \{ f_a \}) \\
                          &= \nu_{f_a}(\{ f_a \}) = \mu(\{ a \})
  \end{align*}
  as required.
\end{proof}

\cptlemma*
\begin{proof}
  If $X$ is binary, then $\mathrm{CPT}_X$ is a sum of $2\prod_{i=1}^n |\im
  Y_i|$ terms, one for each possible assignment of values to variables $X, Y_1,
  \dots, Y_n$. Exactly one of these terms is nonzero when applied to $T$, and
  it is equal to $\Pr(X = x \mid Y_1 = y_1, \dots, Y_n = y_n)$ by definition.

  If $X$ is not binary, then $\left( \sum_{i=1}^m [\lambda_{X = x_i}]
  \right)(T) = 1$, and $\left( \prod_{i=1}^m \prod_{j=i+1}^m
    (\overline{[\lambda_{X = x_i}]} + \overline{[\lambda_{X = x_j}]})
  \right)(T) = 1$, so $\mathrm{CPT}_X(T) = \Pr(X = x \mid Y_1 = y_1,
  \dots, Y_n = y_n)$ by a similar argument as before.
\end{proof}

\fulldistribution*
\begin{proof}
  If $T = \{ \lambda_{X=v_X} \mid X \in \mathcal{V} \}$ for some $(v_X)_{X
    \in \mathcal{V}} \in \prod_{X \in \mathcal{V}} \im X$, then
  \begin{align*}
    \phi(T) &= \prod_{X \in \mathcal{V}} \Pr \left( X=v_X \;\middle|\; \bigwedge_{Y \in \mathrm{pa}(X)} Y=v_Y \right) \\
            &= \Pr \left( \bigwedge_{X \in \mathcal{V}} X=v_X \right)
  \end{align*}
  by \cref{lemma:cpt} and the definition of a Bayesian network. Otherwise there
  must be some non-binary random variable $X \in \mathcal{V}$ such that
  $|\mathcal{E}(X) \cap T| \ne 1$. If $\mathcal{E}(X) \cap T = \emptyset$, then
  $\left( \sum_{i=1}^m [\lambda_{X = x_i}] \right)(T) = 0$, and so
  $\mathrm{CPT}_X(T) = 0$, and $\phi(T) = 0$. If $|\mathcal{E}(X) \cap T| > 1$,
  then we must have two different values $x_1, x_2 \in \im X$ such that
  $\{\lambda_{X=x_1}, \lambda_{X=x_2} \} \subseteq T$ which means that
  $(\overline{[\lambda_{X=x_1}]} + \overline{[\lambda_{X=x_2}]})(T) = 0$, and
  so, again, $\mathrm{CPT}_X(T) = 0$, and $\phi(T) = 0$.
\end{proof}

\correctness*
\begin{proof}
  Let $\mathcal{V} = \{ X, Y_1, \dots, Y_n \}$. Then
  \begin{align*}
    (\exists_U (\phi \cdot [\lambda_{X=x}]))(\emptyset) &= \sum_{T \in 2^U} (\phi \cdot [\lambda_{X=x}])(T) \\
                                                        &= \sum_{\lambda_{X=x} \in T \in 2^U} \phi(T) \\
                                                        &= \sum_{\lambda_{X=x} \in T \in 2^U} \left( \prod_{Y \in \mathcal{V}} \mathrm{CPT}_Y \right)(T) \\
                                                        &= \sum_{(y_i)_{i=1}^n \in \prod_{i=1}^n \im Y_i} \Pr(x, y_1, \dots, y_n) \\
                                                        &= \Pr(X = x)
  \end{align*}
  by:
  \begin{itemize}
  \item the proof of Theorem~1 by \citet{DBLP:conf/aaai/DudekPV20};
  \item if $\lambda_{X=x} \not\in T \in 2^U$, then $(\phi \cdot
    [\lambda_{X=x}])(T) = \phi(T) \cdot [\lambda_{X=x}](T \cap \{
    \lambda_{X=x} \}) = \phi(T) \cdot 0 = 0$;
  \item \cref{lemma:full_distribution};
  \item marginalisation of a probability distribution.
  \end{itemize}
\end{proof}

\end{document}
