\documentclass{beamer}
\usepackage[utf8]{inputenc}
\usepackage[UKenglish]{babel}
\usepackage[UKenglish]{isodate}
\usepackage{tikz}
\usepackage{minibox}
\usepackage{listings}
\usepackage{complexity}
\usepackage{booktabs}

\beamertemplatenavigationsymbolsempty
\usetheme{Madrid}
\usecolortheme{orchid}

\usetikzlibrary{arrows}
\usetikzlibrary{arrows.meta}
\usetikzlibrary{positioning}
\usetikzlibrary{shapes}

\author[P. Dilkas, V. Belle]{\textbf{Paulius Dilkas} \and Vaishak Belle}
\title[WMC with Conditional Weights for BNs]{Weighted Model Counting with
  Conditional Weights for Bayesian Networks}
\date{UAI 2021}
\institute[University of Edinburgh]{University of Edinburgh, Edinburgh, UK}

\begin{document}

\begin{frame}[noframenumbering,plain]
  \tikz[remember picture,overlay]{
    \node at ([yshift=25pt,xshift=30pt]current page.south)
    {\includegraphics[height=40pt]{../poster/logo_inf.png}};
    \node at ([yshift=25pt,xshift=75pt]current page.south)
    {\includegraphics[height=40pt]{../poster/logo_ecr.png}};
    \node at ([yshift=20pt,xshift=140pt]current page.south)
    {\includegraphics[height=20pt]{../poster/logo_ukri.png}};
  }
  \titlepage
\end{frame}

\begin{frame}[fragile]{The Problem of Computing Probability}
  \vspace{-0.75cm}
  \begin{columns}[t]
    \begin{column}{0.6\textwidth}
      \centering
      \begin{block}{ProbLog}
        \vspace{-0.3cm}
        \begin{lstlisting}[basicstyle=\tiny]
0.001 :: burglary.
0.002 :: earthquake.
0.95  :: alarm     :- burglary, earthquake.
0.94  :: alarm     :- burglary, \+ earthquake.
0.29  :: alarm     :- \+ burglary, earthquake.
0.001 :: alarm     :- \+ burglary, \+ earthquake.
0.9   :: johnCalls :- alarm.
0.05  :: johnCalls :- \+ alarm.
0.7   :: maryCalls :- alarm.
0.01  :: maryCalls :- \+ alarm.
        \end{lstlisting}
        \vspace{-0.2cm}
      \end{block}
      \vspace{-0.25cm}
      \begin{block}{BLOG}
        \vspace{-0.3cm}
        \begin{lstlisting}[escapeinside={(*}{*)},basicstyle=\tiny]
random Boolean Burglary (*$\sim$*) BooleanDistrib(0.001);
random Boolean Earthquake (*$\sim$*) BooleanDistrib(0.002);
random Boolean Alarm (*$\sim$*)
  if Burglary then
    if Earthquake then BooleanDistrib(0.95)
    else BooleanDistrib(0.94)
  else
    if Earthquake then BooleanDistrib(0.29)
    else BooleanDistrib(0.001);
random Boolean JohnCalls (*$\sim$*)
  if Alarm then BooleanDistrib(0.9)
  else BooleanDistrib(0.05);
random Boolean MaryCalls (*$\sim$*)
  if Alarm then BooleanDistrib(0.7)
  else BooleanDistrib(0.01);
        \end{lstlisting}
        \vspace{-0.2cm}
      \end{block}
    \end{column}
    \begin{column}{0.35\textwidth}
      \begin{block}{Bayesian Network}
        \centering
        \begin{tikzpicture}[node distance=2cm,scale=0.5,every node/.style={scale=0.5}]
          \node[draw,ellipse] (alarm) {Alarm};
          \node[draw,ellipse,above left of=alarm] (burglary) {Burglary};
          \node[draw,ellipse,above right of=alarm] (earthquake) {Earthquake};
          \node[draw,ellipse,below left of=alarm] (johnCalls) {JohnCalls};
          \node[draw,ellipse,below right of=alarm] (maryCalls) {MaryCalls};
          \draw[-Latex] (burglary) -- (alarm);
          \draw[-Latex] (earthquake) -- (alarm);
          \draw[-Latex] (alarm) -- (johnCalls);
          \draw[-Latex] (alarm) -- (maryCalls);
        \end{tikzpicture}
      \end{block}
      \vspace{1cm}
      \begin{block}{Markov Random Field}
        \centering
        \begin{tikzpicture}[node distance=2cm,scale=0.5,every node/.style={scale=0.5}]
          \node[draw,ellipse] (alarm) {Alarm};
          \node[draw,ellipse,above left of=alarm] (burglary) {Burglary};
          \node[draw,ellipse,above right of=alarm] (earthquake) {Earthquake};
          \node[draw,ellipse,below left of=alarm] (johnCalls) {JohnCalls};
          \node[draw,ellipse,below right of=alarm] (maryCalls) {MaryCalls};
          \draw (burglary) -- (earthquake);
          \draw (burglary) -- (alarm);
          \draw (earthquake) -- (alarm);
          \draw (alarm) -- (johnCalls);
          \draw (alarm) -- (maryCalls);
        \end{tikzpicture}
      \end{block}
    \end{column}
  \end{columns}
  \onslide<2>{
    \begin{tikzpicture}[remember picture,overlay]
      \node[draw,star,fill=red!10] (wmc) at (current page.center) {WMC};
      \coordinate[xshift=-0.25\linewidth,yshift=-0.25\textheight] (p1) at (current page.center);
      \coordinate[xshift=0.25\linewidth,yshift=-0.25\textheight] (p2) at (current page.center);
      \coordinate[xshift=-0.25\linewidth,yshift=0.25\textheight] (p3) at (current page.center);
      \coordinate[xshift=0.25\linewidth,yshift=0.25\textheight] (p4) at (current page.center);
      \draw[-latex,line width=2pt,color=red!50] (p1) -- (wmc);
      \draw[-latex,line width=2pt,color=red!50] (p2) -- (wmc);
      \draw[-latex,line width=2pt,color=red!50] (p3) -- (wmc);
      \draw[-latex,line width=2pt,color=red!50] (p4) -- (wmc);
    \end{tikzpicture}
  }
\end{frame}

\begin{frame}[fragile]{Weighted Model Counting (WMC)}
  \begin{columns}
    \begin{column}{0.5\textwidth}
      \begin{itemize}
      \item Generalises propositional model counting ($\#\SAT{}$)
      \item Applications:
        \begin{itemize}
        \item graphical models
        \item probabilistic programming
        \item neural-symbolic artificial intelligence
        \end{itemize}
%      \item Main types of algorithms:
%        \begin{itemize}
%        \item using knowledge compilation
%        \item using a \SAT{} solver
%        \item manipulating pseudo-Boolean functions
%        \end{itemize}
      \end{itemize}
    \end{column}
    \begin{column}{0.5\textwidth}
      \begin{example}
      $w(x) = 0.3$, $w(\neg x) = 0.7$, $w(y) = 0.2$, $w(\neg y) = 0.8$
      \vspace{1cm}

      $\mathsf{WMC}(\alert{x \lor y}) = w(x)w(y) + w(x)w(\neg y) + w(\neg x)w(y)
      = 0.44$
      \end{example}
    \end{column}
  \end{columns}
\end{frame}

\begin{frame}{An Alternative Way to Think About WMC}
  \begin{itemize}
  \item Let \structure{$V$} be the set of variables.
  \item Then \structure{$2^{2^V}$} is the Boolean algebra of propositional
    formulas.
  \end{itemize}
  \begin{definition}
    A \alert{measure} is a function \structure{$\mu\colon 2^{2^V} \to
      \mathbb{R}_{\ge 0}$} such that:
    \begin{itemize}
    \item \structure{$\mu(\bot) = 0$};
    \item \structure{$\mu(x \lor y) = \mu(x) + \mu(y)$} whenever \structure{$x
        \land y = \bot$}.
    \end{itemize}
  \end{definition}
  \begin{block}{Observation}
    WMC corresponds to the process of calculating the value of
    \structure{$\mu(x)$} for some \structure{$x \in 2^{2^V}$}.
  \end{block}
\end{frame}

% \begin{frame}{How  to Define a Measure}
%   \begin{itemize}
%   \item A \alert{weight function} is any function \structure{$\nu\colon 2^V \to
%       \mathbb{R}_{\ge 0}$}.
%     \begin{itemize}
%     \item It is \alert{factored} if
%       \[
%         \nu = \prod_{x \in V} \nu_x
%       \]
%       for some functions \structure{$\nu_x\colon 2^{\{x\}} \to \mathbb{R}_{\ge
%           0}$}, \structure{$x \in V$}.
%     \end{itemize}
%   \item We say that \structure{$\nu$} \alert{induces} \structure{$\mu$} if
%     \[
%       \mu(x) = \sum_{\{u\} \le x} \nu(u)
%     \]
%     for all \structure{$x \in 2^{2^V}$}.
%   \item A measure \structure{$\mu$} is \alert{factorable} if there exists a
%     factored weight function \structure{$\nu$} that induces \structure{$\mu$}.
%   \end{itemize}
% \end{frame}

\begin{frame}{The Limitations and Capabilities of WMC}
  \begin{alertblock}{Observation}
    Classical WMC is only able to evaluate \structure{factorable} measures
    (c.f., a collection of mutually independent random variables).
  \end{alertblock}
  \begin{theorem}[Informal Version]
    It is always possible to add more variables to turn a non-factorable measure
    into a factorable measure.
  \end{theorem}
  However, that is not necessarily a good idea!
\end{frame}

\begin{frame}{Encoding Bayesian Networks}
  \begin{itemize}
%  \item \alert{One} variable for every random variable with \alert{two} values.
%  \item \alert{$k$} variables for every random variable with \alert{$k > 2$}
%    values.
  \item Define indicator functions of the form \structure{$[x]\colon 2^{\{x\}}
      \to \{ 0, 1 \}$}.
    \begin{itemize}
    \item \structure{$[x](\emptyset) = 0$};
    \item \structure{$[x](\{ x \}) = 1$}.
    \end{itemize}
  \item Define \structure{$+$}, \structure{$\cdot$}, and scalar multiplication pointwise.
  \item Then a conditional probability table (CPT) can be represented as a function.
  \end{itemize}
  \begin{columns}[t]
    \begin{column}{0.5\textwidth}
      \centering
      \begin{tabular}[t]{ccc}
        \toprule
        $a$ & $b$ & $\Pr(A = a \mid B = b)$ \\
        \midrule
        1 & 1 & 0.6 \\
        1 & 0 & 0.4 \\
        0 & 1 & 0.1 \\
        0 & 0 & 0.9 \\
        \bottomrule
      \end{tabular}
    \end{column}
    \begin{column}{0.5\textwidth}
      \begin{align*}
        \mathsf{CPT_A} &= 0.6[\lambda_{A=1}] \cdot [\lambda_{B=1}] \\
                       &+ 0.4[\lambda_{A=1}] \cdot \overline{[\lambda_{B=1}]} \\
                       &+ 0.1\overline{[\lambda_{A=1}]} \cdot [\lambda_{B=1}] \\
                       &+ 0.9\overline{[\lambda_{A=1}]} \cdot \overline{[\lambda_{B=1}]},
      \end{align*}
    \end{column}
  \end{columns}
\end{frame}

\begin{frame}{Experimental Results}
  \centering
  \input{cumulative}
\end{frame}

% \begin{frame}{Compared Against the Second-Best Encoding}
%   \centering
%   \input{scatter2}
% \end{frame}

\begin{frame}{Comparison With the State of the Art}
  \centering
  \input{scatter1}
\end{frame}

\begin{frame}{Summary and Future Work}
  \begin{itemize}
  \item (Classical) WMC can represent any probability distribution by adding
    more variables.
  \item But this is not the right approach for WMC algorithms that support
    working directly with functions.
  \item Specifically with ADDMC, avoiding redundant variables resulted in
    \alert{$127$} times faster inference.
%  \item Could this idea be successfully applied to other applications of WMC or,
%    perhaps, other WMC algorithms?
  \end{itemize}
  \begin{itemize}
  \item Potential improvements to the encoding:
    \begin{itemize}
    \item Apply ideas from other WMC encodings for Bayesian networks (e.g.,
      prime implicants, $\log$ encoding).
    \item Develop encoding tricks that apply to functions but not to conjunctive
      normal form.
    \item More on this in our SAT~2021 paper \emph{\structure{Weighted Model
          Counting Without Parameter Variables}}.
    \end{itemize}
  \end{itemize}
\end{frame}

\end{document}