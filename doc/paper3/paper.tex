\documentclass[runningheads]{llncs}

\usepackage{graphicx}
% Used for displaying a sample figure. If possible, figure files should
% be included in EPS format.
%
% If you use the hyperref package, please uncomment the following line
% to display URLs in blue roman font according to Springer's eBook style:
% \renewcommand\UrlFont{\color{blue}\rmfamily}

\usepackage{amssymb}
\usepackage{mathtools}
\usepackage[ruled,vlined,linesnumbered]{algorithm2e}
\usepackage[capitalise]{cleveref}
\usepackage{tikz}
\usepackage[binary-units]{siunitx}
\usepackage{booktabs}
\usepackage{capt-of}

\crefname{enumii}{Condition}{Conditions}

\begin{document}

%\title{Contribution Title\thanks{Supported by organization x.}}
\title{Weighted Model Counting Without Parameter Variables}

%\titlerunning{Abbreviated paper title}
% If the paper title is too long for the running head, you can set
% an abbreviated paper title here

%\author{First Author\inst{1}\orcidID{0000-1111-2222-3333} \and
%Second Author\inst{2,3}\orcidID{1111-2222-3333-4444} \and
%Third Author\inst{3}\orcidID{2222--3333-4444-5555}}

%\authorrunning{F. Author et al.}
% First names are abbreviated in the running head.
% If there are more than two authors, 'et al.' is used.

%\institute{Princeton University, Princeton NJ 08544, USA \and
%Springer Heidelberg, Tiergartenstr. 17, 69121 Heidelberg, Germany
%\email{lncs@springer.com}\\
%\url{http://www.springer.com/gp/computer-science/lncs} \and
%ABC Institute, Rupert-Karls-University Heidelberg, Heidelberg, Germany\\
%\email{\{abc,lncs\}@uni-heidelberg.de}}

\maketitle              % typeset the header of the contribution

\begin{abstract}
  The abstract should briefly summarize the contents of the paper in
  150--250 words.

  \keywords{First keyword  \and Second keyword \and Another keyword.}
\end{abstract}

\section{Introduction}

% TODO for later:
% 1) read through v2 of the paper and check if I forgot to mention something
% here
% 2) run this through grammarly

\paragraph{Related Work.}
Many WMC inference algorithms such as
\textsf{Ace}\footnote{\url{http://reasoning.cs.ucla.edu/ace/}}, \textsf{c2d}
\cite{DBLP:conf/ecai/Darwiche04}, and \textsf{miniC2D}
\cite{DBLP:conf/ijcai/OztokD15} work by compilation to tractable representations
such as arithmetic circuits, deterministic, decomposable negation normal form
\cite{DBLP:journals/jancl/Darwiche01}, and sentential decision diagrams (SDDs)
\cite{DBLP:conf/ijcai/Darwiche11}. Some attempts have previously been made to
skip this intermediate stage of representing a Bayesian network as a WMC
instance and compile directly to a more convenient representation.
Specifically, direct compilation from Bayesian networks to SDDs
\cite{DBLP:conf/ecsqaru/ChoiKD13} and from structured Bayesian networks (i.e., a
generalisation of Bayesian networks) to probabilistic SDDs
\cite{shen2020modeling} have been considered. To the best of the authors'
knowledge, neither compilation approach has a publicly available implementation.

Our work is similar in that it introduces a new representation for computational
problems that is based on pseudo-Boolean functions---pseudo-Boolean projection
(PBP). We formally show that every WMC problem instance has a corresponding PBP
instance. We argue that this new format is more appropriate for WMC solvers
whose execution is build on pseudo-Boolean function manipulation, i.e.,
\textsf{ADDMC} \cite{DBLP:conf/aaai/DudekPV20} and \textsf{DPMC}
\cite{DBLP:conf/cp/DudekPV20}. Finally, we show how most methods of encoding
probabilistic inference on Bayesian networks to WMC can be adapted to the PBP
format. This transformation significantly improves inference speed,
outperforming all other considered algorithm-encoding combinations.

%allowing two encodings that were previously incompatible with
%\textsf{DPMC} to be compatible.

\paragraph{Notes.}
\begin{itemize}
\item we also show correctness
\item and introduce a new notation
\end{itemize}

\section{Weighted Model Counting}

TODO: Mention that formulas, clauses, and models are all treated as sets.

\begin{definition}[WMC]
  A \emph{WMC instance} is a tuple $(\phi, X_I, X_P, w)$, where $X_I$ is
  the set of indicator variables, $X_P$ is the set of parameter variables (with
  $X_I \cap X_P = \emptyset$), $\phi$ is a propositional formula in CNF over
  $X_I \cup X_P$, and $w\colon X_I \cup X_P \cup \{\neg x \mid x \in X_I \cup
  X_P\} \to \mathbb{R}$ is the weight function. The \emph{answer} of the
  instance is $\sum_{Y \models \phi} \prod_{Y \models l} w(l)$.
\end{definition}

\begin{definition}[Minimum-Cardinality WMC]
  A \emph{minimum-cardinality WMC} instance has same tuple as a WMC instance,
  but its answer is defined to be $\sum_{Y \models \phi\text{, }|Y| = k}
  \prod_{Y \models l} w(l)$, where $k = \min_{Y \models \phi\text{, }Y \ne
    \emptyset} |Y|$, if $k$ exists, otherwise the answer is zero.
\end{definition}

Our definition of WMC is largely based on the standard definition
\cite{DBLP:journals/ai/ChaviraD08}, but explicitly partitions variables into
indicator and parameter variables. In practice, we identify this partition in
one of two ways. If an encoding is generated by \textsf{Ace}, then variable
types are explicitly identified in the LMAP file generated alongside the
encoding. Otherwise, we define a variable $x$ to be a parameter variable if
$w(x) = w(\neg x) = 1$.

\subsection{Bayesian Network Encodings (mostly done)}

A \emph{Bayesian network} is a directed acyclic graph with random variables as
vertices and edges as conditional dependencies. As is common in related
literature
\cite{DBLP:conf/kr/Darwiche02,DBLP:conf/aaai/SangBK05}, we assume that each
variable has a finite number of values. We call a Bayesian network \emph{binary}
if every variable has two values. If all variables have finite numbers of
values, the probability function associated with each variable $v$ can be
represented as a \emph{conditional probability table} (CPT), i.e., a table
with a row for each combination of values that $v$ and its parent vertices can
take. Each row then also has a \emph{probability}, i.e., a number in $[0, 1]$.

WMC is a well-established technique for Bayesian network inference, particularly
effective on networks where most variables have only a few possible values
\cite{DBLP:conf/kr/Darwiche02}. Many ways of encoding a Bayesian network into a
WMC instance have been proposed. We will refer to them based on initials of the
authors and the year of publication. Darwiche was the first to suggest the
\texttt{d02} \cite{DBLP:conf/kr/Darwiche02} encoding that, in many ways, remains
the foundation behind most other encodings. He also introduced the distinction
between \emph{indicator} and \emph{parameter variables}; the former represent
variable-value pairs in the Bayesian network, while the latter are associated
with probabilities in the CPTs. The encoding \texttt{sbk05}
\cite{DBLP:conf/aaai/SangBK05} is the only encoding that deviates from this
arrangement: for each variable in the Bayesian network, one indicator variable
acts simultaneously as a parameter variable. Chavira and Darwiche propose
\texttt{cd05} \cite{DBLP:conf/ijcai/ChaviraD05} where they shift from WMC to
minimum-cardinality WMC because that allows the encoding to have fewer variables
and clauses. In particular, they propose a way to use the same parameter
variable to represent all probabilities in a CPT that are equal and keep only
clauses that `imply' parameter variables (i.e., omit clauses where a parameter
variable implies indicator variables). In their next encoding, \texttt{cd06}
\cite{DBLP:conf/sat/ChaviraD06}, the same authors optimise the aforementioned
implication clauses, choosing the smallest sufficient selection of indicator
variables. A decade later, Bart et al. present \texttt{bklm16}
\cite{DBLP:conf/ecai/BartKLM16} that improves upon \texttt{cd06} in two ways.
First, they optimise the number of indicator variables used per Bayesian network
variable from a linear to a logarithmic amount. Second, they introduce a scaling
factor that can `absorb' one probability per Bayesian network variable. However,
for this work, we choose to disable the latter improvement since this scaling
factor is often small enough to be indistinguishable from zero without the use
of arbitrary precision arithmetic. Indeed, even a small Bayesian network with
seven mutually independent binary variables, 0.1 and 0.9 probabilities each, is
already big enough for the scaling factor to be exactly equal to zero (as
produced by the \texttt{bklm16}
encoder\footnote{\url{http://www.cril.univ-artois.fr/kc/bn2cnf.html}}).

% TODO: incorporate this somewhere
%This additional condition on model
%cardinality becomes necessary because these encodings eliminate clauses of the
%form $p \Rightarrow i$, where $p \in X_P$ is a parameter variable, and $i \in
%X_I$ is an indicator variable. Nonetheless, our transformation algorithm still
%works on such encodings, although the experimental results are discouraging
%because they use approximately twice as many indicator variables. For
%instance, each binary variable of a Bayesian network is encoded using two
%indicator variables while one would suffice.

\section{Pseudo-Boolean Functions}

\paragraph{Notation.} For any propositional formula $\phi$ over a set of
variables $X$ and $p, q \in \mathbb{R}$, let $[\phi]^p_q\colon 2^X \to
\mathbb{R}$ be a pseudo-Boolean function defined as
\[
  [\phi]^p_q(Y) \coloneqq
  \begin{cases}
    p & \text{if } Y \models \phi \\
    q & \text{otherwise}
  \end{cases}
\]
for any $Y \subseteq X$.

\begin{definition}[Operations] \label{def:operations}
  Let $f, g\colon 2^X \to \mathbb{R}$ be pseudo-Boolean functions, $x, y \in X$,
  $Y = \{y_i\}_{i=1}^n \subseteq X$, and $r \in \mathbb{R}$. Operations such as
  addition and multiplication are defined pointwise as
  \[
    (f+g)(Y) \coloneqq f(Y)+g(Y), \quad \text{and} \quad (f \cdot g)(Y)
    \coloneqq f(Y) \cdot g(Y).
  \]
  Note that this means that binary operations on pseudo-Boolean functions
  inherit properties such as associativity and commutativity. By not
  distinguishing between a real number and a pseudo-Boolean function that always
  returns that number, we can use the same definitions to define \emph{scalar}
  operations as
  \[
    (r+f)(Y) = r+f(Y), \quad \text{and} \quad (r \cdot f)(Y) = r \cdot f(Y).
  \]

  \emph{Restrictions} $f|_{x=0}, f|_{x=1}\colon 2^X \to \mathbb{R}$ of $f$ are
  defined as
  \[
    f|_{x=0}(Y) \coloneqq f(Y \setminus \{x\}), \quad \text{and} \quad
    f|_{x=1}(Y) \coloneqq f(Y \cup \{x\})
  \]
  for all $Y \subseteq X$.

  \emph{Projection} $\exists_x$ is an endomorphism $\exists_x\colon
  \mathbb{R}^{2^X} \to \mathbb{R}^{2^X}$ defined as
  \[
    \exists_xf \coloneqq f|_{x=1} + f|_{x=0}.
  \]
  Since projection is commutative (i.e., $\exists_x\exists_yf =
  \exists_y\exists_xf$) \cite{DBLP:conf/aaai/DudekPV20,DBLP:conf/cp/DudekPV20},
  we can define $\exists_Y\colon \mathbb{R}^{2^X} \to \mathbb{R}^{2^X}$ as
  $\exists_Y \coloneqq \exists_{y_1}\exists_{y_2}\dots\exists_{y_n}$. Throughout
  the paper, projection is assumed to have the lowest precedence (e.g.,
  $\exists_x fg = \exists_x (fg)$).
\end{definition}

Below we list some properties of the operations on pseudo-Boolean functions
discussed in this section that can be conveniently represented using our syntax.
The proofs of all these properties follow directly from the definitions.

\begin{proposition}[Basic Properties] \label{prop:basic}
  For any propositional formulas $\phi$ and $\psi$, and $a, b, c, d \in
  \mathbb{R}$,
  \begin{itemize}
  \item $[\phi]^a_b = [\neg \phi]^b_a$;
  \item $c + [\phi]^a_b = [\phi]^{a+c}_{b+c}$;
  \item $c \cdot [\phi]^a_b = [\phi]^{ac}_{bc}$;
  \item $[\phi]^a_b \cdot [\phi]^c_d = [\phi]^{ac}_{bd}$;
  \item $[\phi]^1_0 \cdot [\psi]_0^1 = [\phi \land \psi]_0^1$.
  \end{itemize}
  And for any pair of pseudo-Boolean functions $f, g \colon 2^X \to \mathbb{R}$
  and $x \in  X$, $(fg)|_{x=i} = f|_{x=i} \cdot g|_{x=i}$ for $i = 0, 1$.
\end{proposition}

\section{PBP as a Computational Problem}

\begin{definition}[PBP Instance] \label{def:new_wmc}
  A PBP instance is a tuple $(F, X, \omega)$, where $X$ is the set of variables,
  $F$ is a set of pseudo-Boolean functions $2^X \to \mathbb{R}$, and $\omega \in
  \mathbb{R}$ is the scaling factor.\footnote{We will further restrict $F$ to
    only contain functions that have exactly two values. While this restriction
    is not necessary from a computational point of view, such functions can be
    conveniently represented both in text files and using the syntax proposed in
    this paper.} The \emph{answer} of the instance is $\omega \cdot
  \left(\exists_X\prod_{f \in F}f\right)(\emptyset)$.
\end{definition}

\begin{remark}
  Adding scaling factor $\omega$ to the definition allows us to remove
  clauses that consist entirely of a single parameter variable. The idea of
  extracting some of the structure of the WMC instance into an external
  multiplicative factor originates in one of the Bayesian network encodings
  \cite{DBLP:conf/ecai/BartKLM16}.
\end{remark}

\subsection{From WMC to PBP}

\paragraph{Notes.}
\begin{itemize}
\item The algorithm is based on several observations that will be made precise
  in \cref{thm:correctness}. First, all weights except for $\{w(p) \mid p \in
  X_P\}$ are redundant as they either duplicate an already-defined weight or are
  equal to one. Second, each clause has at most one parameter variable. Third,
  if the parameter variable is negated, we can ignore the clause (this idea
  first appears in the \texttt{cd05} paper \cite{DBLP:conf/ijcai/ChaviraD05}).
\item We formulate our algorithm as a sequel to the WMC encoding procedure
  because the implementations of Bayesian network WMC encodings are all
  closed-source. However, as all transformations in \cref{alg:transformation}
  are local, the algorithm can be efficiently incorporated into a WMC encoding
  algorithm with no noticeable slowdown.
\item No obvious way to do this for \texttt{sbk05} because the roles of
  indicator and parameter (i.e., `chance') variables overlap
  \cite{DBLP:conf/aaai/SangBK05}.
\item Benefits of having this proof in the paper:
  \begin{itemize}
  \item It puts all encodings on a common ground.
  \item It illustrates the convenience of our notation for reasoning about
    (certain types of) pseudo-Boolean functions.
  \item It's too big and too important to be left for the appendix.
  \end{itemize}
\end{itemize}

%\item The second \textbf{foreach} loop can be performed in constant time by
%  representing $\phi'$ as a list and assuming that the two 'clauses' are
%  adjacent in that list (and incorporating it into the first loop).
%\item The $d$ map is constructed in $\mathcal{O}(|X_P|\log|X_P|)$ time (we want
%  to use a data structure based on binary search trees rather than hashing).
%\item \texttt{rename} can be implemented in $\mathcal{O}(\log |X_P|)$ time.

\begin{algorithm}[t]
  \caption{WMC to PBP transformation}
  \label{alg:transformation}
  \SetKwFunction{rename}{rename}
  \SetKwProg{Fn}{Function}{:}{}
  \KwData{WMC instance $(\phi, X_I, X_P, w)$}
  \KwResult{PBP instance $(F, X, \omega)$}
  $F \gets \emptyset$\;
  $\omega \gets 1$\;
  let $d\colon X_P \to \mathbb{N}$ be defined as $p \mapsto |\{ o \in X_P \mid o
  \le p \}|$\;
  \ForEach{clause $c \in \phi$\label{line:foreach1start}}{
    \uIf{$c \cap X_P = \{ p \}$ for some $p$ \textnormal{\textbf{and}} $w(p) \ne
      1$}{
      \uIf{$|c| = 1$}{$\omega \gets \omega \times w(p)$\;}
      \Else{
        $F \gets F \cup \left\{ \left[ \bigwedge_{l \in c \setminus
              \{p\}} \neg l \right]^{w(p)}_1 \right\}$\;
      }
    }
    \ElseIf{$\{p \mid \neg p \in c\} \cap X_P  = \emptyset$}{
      $F \gets F \cup \{ [c]^1_0 \}$\; \label{line:foreach1end}
    }
  }
  \ForEach{indicator variable $v \in X_I$ \label{line:foreach2start}}{
    \If{$\{[v]_1^p, [\neg v]_1^q\} \subseteq F$ for some $p$ and $q$}{
      $F \gets F \setminus \{ [v]_1^p, [\neg v]_1^q \} \cup \{ [v]_q^p
      \}$\; \label{line:foreach2end}
    }
  }
  replace every variable $v$ in $F$ with $\rename{$v$}$\;
  \Return{$(F, X_I, \omega)$}\;
  \Fn{\rename{$v$}}{
    $S \gets \{u \in X_P \mid u \le v\}$\;
    \lIf{$S = \emptyset$}{\Return{$v$}}
    \Return{$v - d(\max S)$}\;
  }
\end{algorithm}

\subsection{Proof of Correctness}

\paragraph{Notes.}
\begin{itemize}
\item This just says that $p$ is equivalent to a conjunction.
\item The first group of conditions applies to \texttt{d02}, while the second
  group applies to \texttt{bklm16}.
\item For \texttt{cd05} and \texttt{cd06}, condition $2bi$ should be replaced
  with $w(\neg p) = 1$.
\end{itemize}

\paragraph{TODO}
\begin{itemize}
\item check if each condition is actually used. Maybe turn this into a paragraph
  that gives an overview of the proof.
\item condition 1 is necessary in both cases because we're ignoring the weights
  of indicator variables (explicitly acknowledge this)
\item add the \texttt{cd05}/\texttt{cd06} correctness theorem
\end{itemize}

\begin{theorem}[Early Projection
  \cite{DBLP:conf/aaai/DudekPV20,DBLP:conf/cp/DudekPV20},
  verbatim] \label{thm:early}
  Let $X$ and $Y$ be sets of variables. For all functions $f\colon 2^X \to
  \mathbb{R}$ and $g\colon 2^Y \to \mathbb{R}$, if $x \in X \setminus Y$, then
  $\exists_x (f \cdot g) = (\exists_x f) \cdot g$.
\end{theorem}

\begin{lemma} \label{lemma:sum}
  For any pseudo-Boolean function $f\colon 2^X \to \mathbb{R}$,
  \[
    (\exists_X f)(\emptyset) = \sum_{Y \subseteq X} f(Y).
  \]
\end{lemma}
\begin{proof}
  If $X = \{x\}$, then
  \[
    (\exists_xf)(\emptyset) = (f|_{x=1} + f|_{x=0})(\emptyset) =
    f|_{x=1}(\emptyset) + f|_{x=0}(\emptyset) = \sum_{Y \subseteq \{x\}} f(Y).
  \]
  This easily extends to $|X| > 1$ by the definition of projection on sets of
  variables.
\end{proof}

% TODO: mention that this is similar (but not equal) to the DPMC/ADDMC result
\begin{proposition} \label{prop:equivalence}
  Let $(\phi, X_I, X_P, w)$ be a WMC instance. Then
  \begin{equation}
  \left(\left\{[c]_0^1 \;\middle|\; c \in \phi\right\} \cup \left\{[x]_{w(\neg
        x)}^{w(x)} \;\middle|\; x \in X_I \cup X_P\right\}, X_I \cup X_P,
    1\right) \label{eq:new_wmc}
  \end{equation}
  is a PBP instance with the same answer.
\end{proposition}
\begin{proof}
  Let $f = \prod_{c \in \phi} [c]_0^1$, and $g = \prod_{x \in X_I \cup X_P}
  [x]_{w(\neg x)}^{w(x)}$. Then the answer to the WMC instance
  \eqref{eq:new_wmc} is
  \[
    (\exists_{X_I \cup X_P} fg)(\emptyset) = \sum_{Y \subseteq X_I \cup X_P}
    (fg)(Y) = \sum_{Y \subseteq X_I \cup X_P} f(Y)g(Y)
  \]
  by \cref{lemma:sum}. Note that
  \[
    f(Y) =
    \begin{cases}
      1 & \text{if } Y \models \phi, \\
      0 & \text{otherwise},
    \end{cases}
    \quad
    \text{and}
    \quad
    g(Y) = \prod_{Y \models l} w(l),
  \]
  which means that
  \[
    \sum_{Y \subseteq X_I \cup X_P} f(Y)g(Y) = \sum_{Y \models \phi} \prod_{Y
      \models l} w(l)
  \]
  as required.
\end{proof}

\begin{theorem}[Correctness] \label{thm:correctness}
  \Cref{alg:transformation}, when given a WMC instance $(\phi, X_I, X_P,
  w)$, returns PBP instance with the same answer, provided the following
  conditions are satisfied:
  \begin{enumerate}
  \item for all indicator variables $i \in X_I$, $w(i) = w(\neg i) = 1$,
  \item and either
    \begin{enumerate}
    \item for all parameter variables $p \in X_P$, there is a non-empty family
      of literals $(l_i)_{i=1}^n$ such that \label{condition:d02}
      \begin{enumerate}
      \item $w(\neg p) = 1$,
      \item $l_i \in X_I$ or $\neg l_i \in X_I$ for all $i = 1, \dots, n$,
        \label{condition:equivalence1}
      \item and $\{ c \in \phi \mid p \in c \text{ or } \neg p \in c \} =
        \left\{p \lor \bigvee_{i=1}^n \neg l_i \right\} \cup \{ l_i \lor \neg p
        \mid i = 1, \dots, n \}$; \label{condition:equivalence2}
      \end{enumerate}
    \item or for all parameter variables $p \in X_P$, \label{condition:bklm16}
      \begin{enumerate}
      \item $w(p) + w(\neg p) = 1$,
      \item for any clause $c \in \phi$, $|c \cap X_P| \le 1$,
      \item there is no clause $c \in \phi$ such that $\neg p \in c$,
      \item if $\{ p \} \in \phi$, then there is no clause $c \in \phi$ such
        that $c \ne \{ p \}$ and $p \in c$, \label{cond:just_parameter}
        % 'prime implicants are logically disjoint'
      \item and for any $c, d \in \phi$ such that $c \ne d$, $p \in c$ and $p
        \in d$, $\bigwedge_{l \in c \setminus \{ p \}} \neg l \land \bigwedge_{l
          \in d \setminus \{ p \}} \neg l$ is false. \label{cond:disjoint}
      \end{enumerate}
    \end{enumerate}
  \end{enumerate}
\end{theorem}
\begin{proof}
  By \cref{prop:equivalence},
  \begin{equation}
    \left(\left\{[c]_0^1 \;\middle|\; c \in \phi\right\} \cup \left\{[x]_{w(\neg
          x)}^{w(x)} \;\middle|\; x \in X_I \cup X_P\right\}, X_I \cup X_P,
      1\right) \label{eq:new_wmc2}
  \end{equation}
  is a PBP instance with the same answer as the given WMC instance. By
  \cref{def:new_wmc}, its answer is
  \begin{equation} \label{eq:answer}
    \left(\exists_{X_I \cup X_P} \left( \prod_{c \in \phi} [c]_0^1 \right) \prod_{x \in X_I \cup X_P} [x]_{w(\neg x)}^{w(x)} \right)(\emptyset)
  \end{equation}
  Since both \cref{condition:d02,condition:bklm16} ensure that each clause in
  $\phi$ has at most one parameter variable, we can partition $\phi$ into
  $\phi_* \coloneqq \{c \in \phi \mid \mathtt{Vars}(c) \cap X_P = \emptyset \}$
  and $\phi_p \coloneqq \{ c \in \phi \mid \mathtt{Vars}(c) \cap X_P = \{ p \}
  \}$ for all $p \in X_P$. We can then use \cref{thm:early} to reorder
  \eqref{eq:answer} into
  \[
    \left(\exists_{X_I} \left( \prod_{x \in X_I} [x]_{w(\neg x)}^{w(x)} \right)
      \left( \prod_{c \in \phi_*} [c]_0^1 \right) \prod_{p \in X_P} \exists_p
      [p]_{w(\neg p)}^{w(p)} \prod_{c \in \phi_p} [c]_0^1 \right)(\emptyset).
  \]

  Let us first consider how the unfinished WMC instance $(F, X_I, \omega)$ after
  the loop on \crefrange{line:foreach1start}{line:foreach1end} differs from
  \eqref{eq:new_wmc2}. Note that \cref{alg:transformation} leaves each $c \in
  \phi_*$ unchanged, i.e., adds $[c]_0^1$ to $F$. We can then fix an arbitrary
  $p \in X_P$ and let $F_p$ be the set of functions added to $F$ as a
  replacement of $\phi_p$. It is sufficient to show that
  \begin{equation} \label{eq:to_show}
    \omega \prod_{f \in F_p} f = \exists_p [p]_{w(\neg p)}^{w(p)} \prod_{c \in \phi_p} [c]_0^1.
  \end{equation}
  Note that under \cref{condition:d02},
  \[
    \bigwedge_{c \in \phi_p} c \equiv p \Leftrightarrow \bigwedge_{i=1}^n l_i
  \]
  for some family of indicator variable literals $(l_i)_{i=1}^n$. Thus,
  \[
    \exists_p [p]_{w(\neg p)}^{w(p)} \prod_{c \in \phi_p} [c]_0^1 = \exists_p
    [p]_1^{w(p)} \left[ p \Leftrightarrow \bigwedge_{i=1}^n l_i \right]_0^1.
  \]
  If $w(p) = 1$, then
  \begin{equation} \label{eq:bigsums}
    \exists_p [p]_1^{w(p)} \left[ p \Leftrightarrow \bigwedge_{i=1}^n l_i \right]_0^1 = \left.\left[ p \Leftrightarrow \bigwedge_{i=1}^n l_i \right]_0^1\right|_{p=1} + \left.\left[ p \Leftrightarrow \bigwedge_{i=1}^n l_i \right]_0^1\right|_{p=0}.
  \end{equation}
  Since for any input, $\bigwedge_{i=1}^n l_i$ is either true or false, exactly
  one of the two summands in \cref{eq:bigsums} will be equal to one, and the
  other will be equal to zero, and so
  \[
    \left.\left[ p \Leftrightarrow \bigwedge_{i=1}^n l_i
      \right]_0^1\right|_{p=1} + \left.\left[ p \Leftrightarrow \bigwedge_{i=1}^n
        l_i \right]_0^1\right|_{p=0} = 1,
  \]
  where $1$ is a pseudo-Boolean function that always returns one. On the other
  side of \cref{eq:to_show}, since $F_p = \emptyset$, and $\omega$ is unchanged,
  we get $\omega\prod_{f \in F_p} f = 1$, and so \cref{eq:to_show} is satisfied
  under \cref{condition:d02} when $w(p) = 1$.

  If $w(p) \ne 1$, then
  \[
    F_p = \left\{ \left[ \bigwedge_{i = 1}^n l_i \right]_1^{w(p)} \right\},
  \]
  and $\omega = 1$, and so we want to show that
  \[
    \left[ \bigwedge_{i = 1}^n l_i \right]_1^{w(p)} = \exists_p [p]_1^{w(p)}
    \left[ p \Leftrightarrow \bigwedge_{i=1}^n l_i \right]_0^1,
  \]
  and indeed
  \[
    \exists_p [p]_1^{w(p)} \left[ p \Leftrightarrow \bigwedge_{i=1}^n l_i
    \right]_0^1 = w(p) \cdot \left[ \bigwedge_{i=1}^n l_i \right]_0^1 + \left[
      \bigwedge_{i=1}^n l_i \right]_1^0 = \left[ \bigwedge_{i=1}^n l_i
    \right]_1^{w(p)}.
  \]
  This finishes the proof of the correctness of the first `foreach' loop under
  \cref{condition:d02}.

  Now let us assume \cref{condition:bklm16}. We still want to prove
  \cref{eq:to_show}. If $w(p) = 1$, then $F_p = \emptyset$, and $\omega = 1$,
  and so the left-hand side of \cref{eq:to_show} is equal to one. Then the
  right-hand side is
  \[
    \exists_p [p]_0^1 \prod_{c \in \phi_p} [c]_0^1 = \exists_p \left[ p \land
      \bigwedge_{c \in \phi_p} c \right]_0^1 = \exists_p [p]_0^1 = 0 + 1 = 1
  \]
  since $p \in c$ for every clause $c \in \phi_p$.

  If $w(p) \ne 1$, and $\{ p \} \in \phi_p$, then, by
  \cref{cond:just_parameter}, $\phi_p = \{ \{ p \} \}$, and
  \cref{alg:transformation} produces $F_p = \emptyset$ and $\omega = w(p)$, and
  so
  \[
    \exists_p [p]_{w(\neg p)}^{w(p)} [p]_0^1 = \exists_p [p]^{w(p)}_0 = w(p) =
    \omega \prod_{f \in F_p} f.
  \]
  The only remaining case is when $w(p) \ne 1$ and $\{ p \} \not \in \phi_p$.
  Then $\omega = 1$, and
  \[
    F_p = \left\{ \left[\bigwedge_{l \in c \setminus \{ p \}} \neg
        l\right]_1^{w(p)} \;\middle|\; c \in \phi_p \right\},
  \]
  so need to show that
  \[
    \prod_{c \in \phi_p} \left[\bigwedge_{l \in c \setminus \{ p \}} \neg
      l\right]_1^{w(p)} = \exists_p [p]_{1-w(p)}^{w(p)} \prod_{c \in \phi_p}
    [c]_0^1.
  \]
  We can rearrange the right-hand side as % TODO: make it fit into margins
  \begin{align*}
    \exists_p [p]_{1-w(p)}^{w(p)} \prod_{c \in \phi_p} [c]_0^1 &= \exists_p [p]_{1-w(p)}^{w(p)} \left[\bigwedge_{c \in \phi_p} c\right]_0^1 = \exists_p [p]_{1-w(p)}^{w(p)} \left[ p \lor \bigwedge_{c \in \phi_p} c \setminus \{ p \} \right]_0^1 \\
                                                               &= w(p) + (1-w(p)) \left[ \bigwedge_{c \in \phi_p} c \setminus \{ p \} \right]_0^1 = w(p) + \left[ \bigwedge_{c \in \phi_p} c \setminus \{ p \} \right]_0^{1-w(p)} \\
                                                               &= \left[ \bigwedge_{c \in \phi_p} c \setminus \{ p \} \right]_{w(p)}^1 = \left[ \neg \bigwedge_{c \in \phi_p} c \setminus \{ p \} \right]_1^{w(p)} = \left[ \bigvee_{c \in \phi_p} \neg(c \setminus \{ p \}) \right]_1^{w(p)} \\
                                                               &= \left[ \bigvee_{c \in \phi_p} \neg \bigvee_{l \in c \setminus \{ p \}} l \right]_1^{w(p)} = \left[ \bigvee_{c \in \phi_p} \bigwedge_{l \in c \setminus \{ p \}} \neg l \right]_1^{w(p)}.
  \end{align*}
   By \cref{cond:disjoint}, $\bigwedge_{l \in c \setminus \{ p \}} \neg l$ can
   be true for at most one $c \in \phi_p$, and so
   \[
     \left[ \bigvee_{c \in \phi_p} \bigwedge_{l \in c \setminus \{ p \}} \neg l
     \right]_1^{w(p)} = \prod_{c \in \phi_p} \left[ \bigwedge_{l \in c \setminus
         \{ p \}} \neg l \right]_1^{w(p)}
   \]
   which is exactly what we needed to show. This ends the proof that the
   first loop of \cref{alg:transformation} preserves the answer under both
   \cref{condition:d02} and \cref{condition:bklm16}. Finally, the loop on
   \crefrange{line:foreach2start}{line:foreach2end} of \cref{alg:transformation}
   replaces $[v]_1^p[\neg v]_1^q$ with $[v]_q^p$ (for some $v \in X_I$ and $p, q
   \in \mathbb{R}$), but, of course,
   \[
     [v]_1^p[\neg v]_1^q = [v]_1^p[v]_q^1 = [v]_q^p,
   \]
   i.e., the answer is unchanged.
\end{proof}

\section{Experimental Evaluation (mostly done)}
% TODO: what are the encodings compiled with?

We run a set of experiments, comparing all five original Bayesian network
encodings (\texttt{bklm16}, \texttt{cd05}, \texttt{cd06}, \texttt{d02}
\texttt{sbk05}) as well as the first four with \cref{alg:transformation} applied
afterwards.\footnote{Recall that \texttt{cd05} and \texttt{cd06} are
  incompatible with \textsf{DPMC}.} For each encoding \texttt{e}, we will write
\texttt{e++} to denote the combination of encoding a Bayesian network as a WMC
instance using \texttt{e} and transforming it into a PBP instance using
\cref{alg:transformation}. Along with
\textsf{DPMC}\footnote{\url{https://github.com/vardigroup/DPMC}}, we also
include WMC algorithms used in the papers that introduce each encoding:
\textsf{Ace} for \texttt{cd05}, \texttt{cd06}, and \texttt{d02};
\textsf{Cachet}\footnote{\url{https://cs.rochester.edu/u/kautz/Cachet/}}
\cite{DBLP:conf/sat/SangBBKP04} for \texttt{sbk05}; and
\textsf{c2d}\footnote{\url{http://reasoning.cs.ucla.edu/c2d/}}
\cite{DBLP:conf/ecai/Darwiche04} with
\textsf{query-dnnf}\footnote{\url{http://www.cril.univ-artois.fr/kc/d-DNNF-reasoner.html}}
for \texttt{bklm16}. We focus on the following questions:
\begin{itemize}
\item Can parameter variable elimination improve inference speed?
\item How does DPMC combined with encodings without (and with) parameter
  variables compare with other WMC algorithms and other encodings?
\item Which instances is our approach particularly successful on (compared to
  other algorithms and encodings and to the same encoding before our
  transformation)?
\item What proportion of variables is typically eliminated?
\item Do some encodings benefit from this transformation more than others?
\end{itemize}

\subsection{Setup}

\textsf{DPMC} is run with tree decomposition based planning and algebraic
decision diagram based execution---the best-performing combination in the
original set of experiments \cite{DBLP:conf/cp/DudekPV20}. We use a single
iteration of \textsf{htd} \cite{DBLP:conf/cpaior/AbseherMW17} to generate
approximately optimal tree decompositions---we found that this configuration
is efficient enough to handle huge instances, and yet the width of the
returned decomposition is unlikely to differ from optimal by more than one or
two. We also enabled \textsf{DPMC}'s greedy mode. This mode (which was not part
of the original paper \cite{DBLP:conf/cp/DudekPV20}) optimises the order in
which pseudo-Boolean functions are multiplied by prioritising functions with
small representations.

For experimental data, we use Bayesian networks available with \textsf{Ace} and
\textsf{Cachet}. We split them into the following groups:
\begin{itemize}
\item DQMR (390 instances) and
\item Grid networks (450 instances) as described by Sang et al.
  \cite{DBLP:conf/aaai/SangBK05};
\item Mastermind (144 instances) and
\item Random Blocks (256 instances) by Chavira et al.
  \cite{DBLP:journals/ijar/ChaviraDJ06};
\item other binary Bayesian networks (50 instances) including Plan Recognition
  \cite{DBLP:conf/aaai/SangBK05}, Friends and Smokers, Students and Professors
  \cite{DBLP:journals/ijar/ChaviraDJ06}, and \texttt{tcc4f};
\item non-binary classic networks (176 instances): \texttt{alarm},
  \texttt{diabetes}, \texttt{hailfinder}, \texttt{mildew},
  \texttt{munin1}--\texttt{4}, \texttt{pathfinder}, \texttt{pigs}, and
  \texttt{water}.
\end{itemize}

To perform Bayesian network inference with \textsf{DPMC} (or with any
other WMC algorithm not based on compilation such as \textsf{Cachet}), one
needs to select a probability to compute
\cite{DBLP:conf/cp/DudekPV20,DBLP:conf/sat/SangBBKP04}. If a network comes
with an evidence file, we compute the probability of this evidence. Otherwise,
let $X$ be the variable last mentioned in the Bayesian network file. If
\textsf{true} is one of the values of $X$, then we compute $\Pr(X =
\textsf{true})$, otherwise we choose the first-mentioned value of $X$.

The experiments were run on a computing cluster with Intel Xeon E5-2630,
Intel Xeon E7-4820, and Intel Xeon Gold 6138 processors with a
\SI{1000}{\second} timeout separately on both encoding and inference, and a
\SI{32}{\gibi\byte} memory limit.\footnote{Each instance was run on the same
  processor across all algorithms and encodings.}

\subsection{Results}

\begin{figure}
  \centering
  \input{cumulative.tex}%
  \caption{Cumulative numbers of instances solved by each algorithm-encoding
    pair over time. The dotted line denotes the total number of instances used.}
  \label{fig:cumulative}
\end{figure}

\begin{figure}
  \centering
  \input{scatter.tex}%
  \caption{An instance-by-instance comparison between $\textsf{DPMC} +
    \texttt{bklm16++}$ (the best combination according to \cref{fig:cumulative})
  and the second and third best performing combinations: $\textsf{Ace} +
  \texttt{cd06}$ and $\textsf{DPMC} + \texttt{bklm16}$.}
  \label{fig:scatter}
\end{figure}

\begin{figure}
  \centering
  \begin{minipage}{0.49\textwidth}
    \centering
    \input{box.tex}%
    \caption{Box plots of the numbers of variables in each encoding across all
      benchmark instances before and after applying \cref{alg:transformation}.
      Outliers and the top parts of some whiskers are
      omitted.}\label{fig:box}
  \end{minipage}\hfill
  \begin{minipage}{0.49\textwidth}
    \centering
    \captionof{table}{The numbers of instances (out of 1466) that each algorithm and
      encoding combination solved faster than any other combination and in
      total.}\label{tbl:performance}
    \begin{tabular}{lrr}
      \toprule
      Combination & Fastest & Solved \\
      \midrule
      $\textsf{Ace} + \texttt{cd05}$ & 27 & 1247 \\
      $\textsf{Ace} + \texttt{cd06}$ & 135 & 1340 \\
      $\textsf{Ace} + \texttt{d02}$ & 56 & 1060 \\
      $\textsf{DPMC} + \texttt{bklm16}$ & 241 & 1327 \\
      $\textsf{DPMC} + \texttt{bklm16++}$ & \textbf{992} & \textbf{1435} \\
      $\textsf{DPMC} + \texttt{cd05++}$ & \textcolor{gray}{0} & 867 \\
      $\textsf{DPMC} + \texttt{cd06++}$ & \textcolor{gray}{0} & 932 \\
      $\textsf{DPMC} + \texttt{d02}$ & 1 & 1267 \\
      $\textsf{DPMC} + \texttt{d02++}$ & 7 & 1272 \\
      $\textsf{DPMC} + \texttt{sbk05}$ & 31 & 1308 \\
      $\textsf{c2d} + \texttt{bklm16}$ & \textcolor{gray}{0} & 997 \\
      $\textsf{Cachet} + \texttt{sbk05}$ & 49 & 983 \\
      \bottomrule
    \end{tabular}
  \end{minipage}
\end{figure}

\Cref{fig:cumulative} shows $\textsf{DPMC}+\texttt{bklm16++}$ to be the
best-performing combination across all time limits up to \SI{1000}{\second} with
$\textsf{Ace} + \texttt{cd06}$ and $\textsf{DPMC}+\texttt{bklm16}$ not far
behind. Overall, $\textsf{DPMC}+\texttt{bklm16++}$ is 3.35 times faster than
$\textsf{DPMC}+\texttt{bklm16}$ and 2.96 times faster than
$\textsf{Ace}+\texttt{cd06}$. \Cref{tbl:performance} further shows that
$\textsf{DPMC}+\texttt{bklm16++}$ solves almost a hundred more instances than
any other combination, and is the fastest in \SI{69.1}{\percent} of them.

The scatter plots in \cref{fig:scatter} show that how $\textsf{DPMC} +
\texttt{bklm16++}$ (and perhaps \textsf{DPMC} more generally) compares to
$\textsf{Ace} + \texttt{cd06}$ depends significantly on the data set: the former
is a clear winner on DQMR and Grid instances, while the latter performs well on
Mastermind and Random Blocks. Perhaps because the underlying WMC algorithm
remains the same, the difference between $\textsf{DPMC} + \texttt{bklm16}$ with
and without applying \cref{alg:transformation} is quite noisy, i.e, with most
instances scattered around the line of equality. However, our transformation
does enable \textsf{DPMC} to solve many instances that were previously beyond
its reach.

We also record numbers of variables in each encoding before and after applying
\cref{alg:transformation}. \Cref{fig:box} shows a significant reduction in the
number of variables. For instance, the median number of variables in instances
encoded with \texttt{bklm16} was reduced four times: from 1499 to 376. While
\texttt{bklm16++} results in the overall lowest number of variables, the
difference between \texttt{bklm16++} and \texttt{d02++} seems small. Indeed, the
numbers of variables in these two encodings are equal for binary Bayesian
networks (i.e., most of our data). Nonetheless, \texttt{bklm16++} is still much
faster than \texttt{d02++} when run with \textsf{DPMC}.

Overall, transforming WMC instances to the PBP format allows us to significantly
simplify each instance. This transformation is particularly effective on
\texttt{bklm16}, allowing it to surpass \texttt{cd06} and become the new state
of the art. While there is a similarly significant reduction in the number of
variables for \texttt{d02}, the performance of $\textsf{DPMC}+\texttt{d02}$ is
virtually unaffected. Finally, while our transformation makes it possible to use
\texttt{cd05} and \texttt{cd06} with \textsf{DPMC}, the two combinations remain
inefficient.

\section{Conclusions}

\paragraph{Notes.}
\begin{itemize}
\item Benefits of my approach:
  \begin{itemize}
  \item smaller primal graph, so easier to perform tree decomposition
  \item Variable order is less likely to be obstructed by all the unnecessary
    'parameter' variables.
  \item There are others, but they're not that important.
  \end{itemize}
\item Future work: do other WMC encodings also have this variable partition
  property?
\item Do I need to formally consider extending a pseudo-Boolean function to a
  bigger domain?
\end{itemize}

\bibliographystyle{splncs04}
\bibliography{paper}

\end{document}
